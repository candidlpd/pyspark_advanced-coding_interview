{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H:\\\\pyspark_advanced-coding_interview'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"H:\\pyspark_advanced-coding_interview\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference between Count(*), Count(1), Count(colname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+------+\n",
      "|EmployeeID|EmployeeName|Department |Salary|\n",
      "+----------+------------+-----------+------+\n",
      "|1         |Alice       |HR         |70000 |\n",
      "|2         |Bob         |Engineering|85000 |\n",
      "|3         |Charlie     |Engineering|90000 |\n",
      "|4         |David       |HR         |80000 |\n",
      "|5         |Eve         |Marketing  |null  |\n",
      "|6         |null        |Engineering|75000 |\n",
      "|7         |Grace       |null       |60000 |\n",
      "|8         |Heidi       |HR         |null  |\n",
      "|9         |Ivan        |Marketing  |88000 |\n",
      "|10        |Judy        |null       |null  |\n",
      "|11        |Kevin       |Engineering|99000 |\n",
      "|12        |Laura       |Sales      |72000 |\n",
      "|13        |null        |Sales      |65000 |\n",
      "|14        |Mallory     |IT         |77000 |\n",
      "|15        |Oscar       |IT         |81000 |\n",
      "+----------+------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"CountDifference\").getOrCreate()\n",
    "\n",
    "# Define schema using StructType and StructField\n",
    "schema = StructType([\n",
    "    StructField(\"EmployeeID\", IntegerType(), True),\n",
    "    StructField(\"EmployeeName\", StringType(), True),\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"Salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Sample data with some null values\n",
    "data = [\n",
    "    (1, \"Alice\", \"HR\", 70000),\n",
    "    (2, \"Bob\", \"Engineering\", 85000),\n",
    "    (3, \"Charlie\", \"Engineering\", 90000),\n",
    "    (4, \"David\", \"HR\", 80000),\n",
    "    (5, \"Eve\", \"Marketing\", None),\n",
    "    (6, None, \"Engineering\", 75000),\n",
    "    (7, \"Grace\", None, 60000),\n",
    "    (8, \"Heidi\", \"HR\", None),\n",
    "    (9, \"Ivan\", \"Marketing\", 88000),\n",
    "    (10, \"Judy\", None, None),\n",
    "    (11, \"Kevin\", \"Engineering\", 99000),\n",
    "    (12, \"Laura\", \"Sales\", 72000),\n",
    "    (13, None, \"Sales\", 65000),\n",
    "    (14, \"Mallory\", \"IT\", 77000),\n",
    "    (15, \"Oscar\", \"IT\", 81000)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `1` cannot be resolved. Did you mean one of the following? [`EmployeeID`, `EmployeeName`, `Department`, `Salary`].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# COUNT(1): Similar to COUNT(*), it counts all rows.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m count_all_rows_1 \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_number_rows\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m count_all_rows_1\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\dataframe.py:3507\u001b[0m, in \u001b[0;36mDataFrame.agg\u001b[1;34m(self, *exprs)\u001b[0m\n\u001b[0;32m   3471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magg\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mexprs: Union[Column, Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   3472\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Aggregate on the entire :class:`DataFrame` without groups\u001b[39;00m\n\u001b[0;32m   3473\u001b[0m \u001b[38;5;124;03m    (shorthand for ``df.groupBy().agg()``).\u001b[39;00m\n\u001b[0;32m   3474\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3505\u001b[0m \u001b[38;5;124;03m    +--------+\u001b[39;00m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexprs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\group.py:170\u001b[0m, in \u001b[0;36mGroupedData.agg\u001b[1;34m(self, *exprs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m exprs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexprs should not be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(exprs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exprs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 170\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jgd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;66;03m# Columns\u001b[39;00m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m exprs), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall exprs should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\lpdda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\errors\\exceptions\\captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `1` cannot be resolved. Did you mean one of the following? [`EmployeeID`, `EmployeeName`, `Department`, `Salary`]."
     ]
    }
   ],
   "source": [
    "# COUNT(1): Similar to COUNT(*), it counts all rows.\n",
    "count_all_rows_1 = df.agg({\"1\": \"count\"}).alias(\"total_number_rows\")\n",
    "count_all_rows_1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|count(EmployeeName)|\n",
      "+-------------------+\n",
      "|                 13|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COUNT(EmployeeName): Counts non-NULL values in the EmployeeName column.\n",
    "count_non_null = df.agg({\"EmployeeName\": \"count\"}).alias(\"NonNullEmployeeNames\")\n",
    "count_non_null.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      15|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COUNT(*): Counts all rows, including those with NULL values in any or all columns.\n",
    "count_all_rows = df.agg({\"*\": \"count\"}).alias(\"TotalRows\")\n",
    "count_all_rows.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------+--------------------+\n",
      "|Department |TotalRows|CountID|NonNullEmployeeNames|\n",
      "+-----------+---------+-------+--------------------+\n",
      "|HR         |3        |3      |3                   |\n",
      "|Engineering|4        |4      |3                   |\n",
      "|Marketing  |2        |2      |2                   |\n",
      "|null       |2        |2      |2                   |\n",
      "|Sales      |2        |2      |1                   |\n",
      "|IT         |2        |2      |2                   |\n",
      "+-----------+---------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Group by Department and use different COUNT functions\n",
    "df_grouped_count = df.groupBy(\"Department\").agg(\n",
    "    count(\"*\").alias(\"TotalRows\"),         # Equivalent to COUNT(*)\n",
    "    count(\"EmployeeID\").alias(\"CountID\"),  # Counts non-NULL EmployeeID\n",
    "    count(\"EmployeeName\").alias(\"NonNullEmployeeNames\")  # Counts non-NULL EmployeeName\n",
    ")\n",
    "\n",
    "df_grouped_count.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|TotalRows|\n",
      "+---------+\n",
      "|       15|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Register the DataFrame as a temporary table\n",
    "df.createOrReplaceTempView(\"Employees\")\n",
    "\n",
    "\n",
    "# SQL Query using COUNT(*)\n",
    "sql_count_star = \"\"\"\n",
    "SELECT COUNT(*) AS TotalRows\n",
    "FROM Employees\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "result_count_star = spark.sql(sql_count_star)\n",
    "result_count_star.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|TotalRows|\n",
      "+---------+\n",
      "|       15|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL Query using COUNT(1)\n",
    "sql_count_one = \"\"\"\n",
    "SELECT COUNT(1) AS TotalRows\n",
    "FROM Employees\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "result_count_one = spark.sql(sql_count_one)\n",
    "result_count_one.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|TotalEmployeeNames|\n",
      "+------------------+\n",
      "|                13|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL Query using COUNT(EmployeeName)\n",
    "sql_count_col = \"\"\"\n",
    "SELECT COUNT(EmployeeName) AS TotalEmployeeNames\n",
    "FROM Employees\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "result_count_col = spark.sql(sql_count_col)\n",
    "result_count_col.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------------------+---------------+\n",
      "| Department|TotalRows|NonNullEmployeeNames|NonNullSalaries|\n",
      "+-----------+---------+--------------------+---------------+\n",
      "|         HR|        3|                   3|              2|\n",
      "|Engineering|        4|                   3|              4|\n",
      "|  Marketing|        2|                   2|              1|\n",
      "|       null|        2|                   2|              1|\n",
      "|      Sales|        2|                   1|              2|\n",
      "|         IT|        2|                   2|              2|\n",
      "+-----------+---------+--------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL Query using COUNT(*) and COUNT(EmployeeName) together\n",
    "sql_count_group_by = \"\"\"\n",
    "SELECT Department, \n",
    "       COUNT(*) AS TotalRows, \n",
    "       COUNT(EmployeeName) AS NonNullEmployeeNames,\n",
    "       COUNT(Salary) AS NonNullSalaries\n",
    "FROM Employees\n",
    "GROUP BY Department\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "result_count_group_by = spark.sql(sql_count_group_by)\n",
    "result_count_group_by.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows (COUNT(*)): 15\n",
      "Total Rows (COUNT(1)): 15\n",
      "Total Non-NULL Employee Names (COUNT(EmployeeName)): 13\n",
      "Total Non-NULL Salaries (COUNT(Salary)): 12\n",
      "\n",
      "Grouped Count:\n",
      "             TotalRows  NonNullEmployeeNames  NonNullSalaries\n",
      "Department                                                   \n",
      "Engineering          4                     3                4\n",
      "HR                   3                     3                2\n",
      "IT                   2                     2                2\n",
      "Marketing            2                     2                1\n",
      "Sales                2                     1                2\n",
      "NULL Employee Names: 2\n",
      "NULL Salaries: 3\n"
     ]
    }
   ],
   "source": [
    "# Full example code showing all methods\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (1, \"Alice\", \"HR\", 70000),\n",
    "    (2, \"Bob\", \"Engineering\", 85000),\n",
    "    (3, \"Charlie\", \"Engineering\", 90000),\n",
    "    (4, \"David\", \"HR\", 80000),\n",
    "    (5, \"Eve\", \"Marketing\", None),\n",
    "    (6, None, \"Engineering\", 75000),\n",
    "    (7, \"Grace\", None, 60000),\n",
    "    (8, \"Heidi\", \"HR\", None),\n",
    "    (9, \"Ivan\", \"Marketing\", 88000),\n",
    "    (10, \"Judy\", None, None),\n",
    "    (11, \"Kevin\", \"Engineering\", 99000),\n",
    "    (12, \"Laura\", \"Sales\", 72000),\n",
    "    (13, None, \"Sales\", 65000),\n",
    "    (14, \"Mallory\", \"IT\", 77000),\n",
    "    (15, \"Oscar\", \"IT\", 81000)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"EmployeeID\", \"EmployeeName\", \"Department\", \"Salary\"])\n",
    "\n",
    "# Method 1: Count all rows (COUNT(*))\n",
    "count_all = len(df)\n",
    "print(\"Total Rows (COUNT(*)):\", count_all)\n",
    "\n",
    "# Method 2: Count all rows (COUNT(1))\n",
    "count_one = len(df)\n",
    "print(\"Total Rows (COUNT(1)):\", count_one)\n",
    "\n",
    "# Method 3: Count non-NULL values in columns (COUNT(column_name))\n",
    "count_employee_names = df[\"EmployeeName\"].count()\n",
    "count_salaries = df[\"Salary\"].count()\n",
    "print(\"Total Non-NULL Employee Names (COUNT(EmployeeName)):\", count_employee_names)\n",
    "print(\"Total Non-NULL Salaries (COUNT(Salary)):\", count_salaries)\n",
    "\n",
    "# Using groupby and agg\n",
    "grouped_count = df.groupby(\"Department\").agg(\n",
    "    TotalRows=pd.NamedAgg(column=\"Department\", aggfunc=\"size\"),\n",
    "    NonNullEmployeeNames=pd.NamedAgg(column=\"EmployeeName\", aggfunc=\"count\"),\n",
    "    NonNullSalaries=pd.NamedAgg(column=\"Salary\", aggfunc=\"count\")\n",
    ")\n",
    "print(\"\\nGrouped Count:\")\n",
    "print(grouped_count)\n",
    "\n",
    "# Count NULL values\n",
    "null_employee_names = df[\"EmployeeName\"].isnull().sum()\n",
    "null_salaries = df[\"Salary\"].isnull().sum()\n",
    "print(\"NULL Employee Names:\", null_employee_names)\n",
    "print(\"NULL Salaries:\", null_salaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
