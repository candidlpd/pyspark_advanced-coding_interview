{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H:\\\\pyspark_advanced-coding_interview'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"H:\\pyspark_advanced-coding_interview\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to sort alphanumeric data | alphabets and numbers in correct order | PATINDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "|id |alphanumeric|\n",
      "+---+------------+\n",
      "|1  |A1          |\n",
      "|2  |A10         |\n",
      "|3  |A2          |\n",
      "|4  |B1          |\n",
      "|5  |B10         |\n",
      "|6  |B2          |\n",
      "|7  |C3          |\n",
      "|8  |C1          |\n",
      "|9  |C20         |\n",
      "|10 |D100        |\n",
      "|11 |D10         |\n",
      "|12 |D2          |\n",
      "|13 |E25         |\n",
      "|14 |E5          |\n",
      "|15 |F7          |\n",
      "|16 |Z1          |\n",
      "|17 |Z10         |\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"SortAlphanumericData\").getOrCreate()\n",
    "\n",
    "# Sample Data: Alphanumeric strings\n",
    "data = [\n",
    "    (1, \"A1\"),\n",
    "    (2, \"A10\"),\n",
    "    (3, \"A2\"),\n",
    "    (4, \"B1\"),\n",
    "    (5, \"B10\"),\n",
    "    (6, \"B2\"),\n",
    "    (7, \"C3\"),\n",
    "    (8, \"C1\"),\n",
    "    (9, \"C20\"),\n",
    "    (10, \"D100\"),\n",
    "    (11, \"D10\"),\n",
    "    (12, \"D2\"),\n",
    "    (13, \"E25\"),\n",
    "    (14, \"E5\"),\n",
    "    (15, \"F7\"),\n",
    "    (16, \"Z1\"),\n",
    "    (17, \"Z10\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"id\", \"alphanumeric\"])\n",
    "\n",
    "# Create a Temporary View for Spark SQL\n",
    "df.createOrReplaceTempView(\"alphanumeric_data\")\n",
    "\n",
    "# Show the original DataFrame\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-------+-------+\n",
      "| id|alphanumeric|letters|numbers|\n",
      "+---+------------+-------+-------+\n",
      "|  1|          A1|      A|      1|\n",
      "|  3|          A2|      A|      2|\n",
      "|  2|         A10|      A|     10|\n",
      "|  4|          B1|      B|      1|\n",
      "|  6|          B2|      B|      2|\n",
      "|  5|         B10|      B|     10|\n",
      "|  8|          C1|      C|      1|\n",
      "|  7|          C3|      C|      3|\n",
      "|  9|         C20|      C|     20|\n",
      "| 12|          D2|      D|      2|\n",
      "| 11|         D10|      D|     10|\n",
      "| 10|        D100|      D|    100|\n",
      "| 14|          E5|      E|      5|\n",
      "| 13|         E25|      E|     25|\n",
      "| 15|          F7|      F|      7|\n",
      "| 16|          Z1|      Z|      1|\n",
      "| 17|         Z10|      Z|     10|\n",
      "+---+------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = spark.sql(\"\"\" \n",
    "                \n",
    "  SELECT *,\n",
    "       regexp_extract(alphanumeric, '^[A-Za-z]+', 0) AS letters,\n",
    "       CAST(regexp_extract(alphanumeric, '[0-9]+$', 0) AS INT) AS numbers\n",
    "FROM alphanumeric_data\n",
    "ORDER BY letters ASC, numbers ASC;\n",
    "              \n",
    "                \"\"\")\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-------+-------+\n",
      "|id |alphanumeric|letters|numbers|\n",
      "+---+------------+-------+-------+\n",
      "|1  |A1          |A      |1      |\n",
      "|3  |A2          |A      |2      |\n",
      "|2  |A10         |A      |10     |\n",
      "|4  |B1          |B      |1      |\n",
      "|6  |B2          |B      |2      |\n",
      "|5  |B10         |B      |10     |\n",
      "|8  |C1          |C      |1      |\n",
      "|7  |C3          |C      |3      |\n",
      "|9  |C20         |C      |20     |\n",
      "|12 |D2          |D      |2      |\n",
      "|11 |D10         |D      |10     |\n",
      "|10 |D100        |D      |100    |\n",
      "|14 |E5          |E      |5      |\n",
      "|13 |E25         |E      |25     |\n",
      "|15 |F7          |F      |7      |\n",
      "|16 |Z1          |Z      |1      |\n",
      "|17 |Z10         |Z      |10     |\n",
      "+---+------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "# Extract letters and numbers separately\n",
    "df_sorted = df.withColumn(\"letters\", regexp_extract(\"alphanumeric\", \"^[A-Za-z]+\", 0)) \\\n",
    "              .withColumn(\"numbers\", regexp_extract(\"alphanumeric\", \"[0-9]+$\", 0).cast(\"int\")) \\\n",
    "              .orderBy(col(\"letters\").asc(), col(\"numbers\").asc())\n",
    "\n",
    "# Show the sorted DataFrame\n",
    "df_sorted.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-------+-------+\n",
      "|id |alphanumeric|letters|numbers|\n",
      "+---+------------+-------+-------+\n",
      "|1  |A1          |A      |1      |\n",
      "|3  |A2          |A      |2      |\n",
      "|2  |A10         |A      |10     |\n",
      "|4  |B1          |B      |1      |\n",
      "|6  |B2          |B      |2      |\n",
      "|5  |B10         |B      |10     |\n",
      "|8  |C1          |C      |1      |\n",
      "|7  |C3          |C      |3      |\n",
      "|9  |C20         |C      |20     |\n",
      "|12 |D2          |D      |2      |\n",
      "|11 |D10         |D      |10     |\n",
      "|10 |D100        |D      |100    |\n",
      "|14 |E5          |E      |5      |\n",
      "|13 |E25         |E      |25     |\n",
      "|15 |F7          |F      |7      |\n",
      "|16 |Z1          |Z      |1      |\n",
      "|17 |Z10         |Z      |10     |\n",
      "+---+------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "# UDF to extract letters\n",
    "def extract_letters(s):\n",
    "    import re\n",
    "    return re.findall(r'[A-Za-z]+', s)[0] if re.findall(r'[A-Za-z]+', s) else None\n",
    "\n",
    "# UDF to extract numbers\n",
    "def extract_numbers(s):\n",
    "    import re\n",
    "    return int(re.findall(r'\\d+', s)[0]) if re.findall(r'\\d+', s) else None\n",
    "\n",
    "# Register UDFs\n",
    "extract_letters_udf = udf(extract_letters, StringType())\n",
    "extract_numbers_udf = udf(extract_numbers, IntegerType())\n",
    "\n",
    "# Apply UDFs to extract and sort\n",
    "df_sorted_udf = df.withColumn(\"letters\", extract_letters_udf(\"alphanumeric\")) \\\n",
    "                  .withColumn(\"numbers\", extract_numbers_udf(\"alphanumeric\")) \\\n",
    "                  .orderBy(\"letters\", \"numbers\")\n",
    "\n",
    "df_sorted_udf.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+\n",
      "| id|alphanumeric|custom_order|\n",
      "+---+------------+------------+\n",
      "|  1|          A1|           1|\n",
      "|  2|         A10|           1|\n",
      "|  3|          A2|           1|\n",
      "|  4|          B1|           2|\n",
      "|  5|         B10|           2|\n",
      "|  6|          B2|           2|\n",
      "|  8|          C1|           3|\n",
      "|  9|         C20|           3|\n",
      "|  7|          C3|           3|\n",
      "| 11|         D10|           3|\n",
      "| 10|        D100|           3|\n",
      "| 12|          D2|           3|\n",
      "| 13|         E25|           3|\n",
      "| 14|          E5|           3|\n",
      "| 15|          F7|           3|\n",
      "| 16|          Z1|           3|\n",
      "| 17|         Z10|           3|\n",
      "+---+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res1 = spark.sql(\"\"\" \n",
    "                \n",
    "SELECT *,\n",
    "       CASE WHEN alphanumeric LIKE 'A%' THEN 1\n",
    "            WHEN alphanumeric LIKE 'B%' THEN 2\n",
    "            ELSE 3\n",
    "       END AS custom_order\n",
    "FROM alphanumeric_data\n",
    "ORDER BY custom_order, alphanumeric;\n",
    "\n",
    "              \n",
    "                \"\"\")\n",
    "res1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
