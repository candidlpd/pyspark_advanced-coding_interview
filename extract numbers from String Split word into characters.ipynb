{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H:\\\\pyspark_advanced-coding_interview'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"H:\\pyspark_advanced-coding_interview\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract numbers from String | Split word into characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n",
      "|id |mixed_string |\n",
      "+---+-------------+\n",
      "|1  |Order123     |\n",
      "|2  |Item4567     |\n",
      "|3  |Prod98       |\n",
      "|4  |Ref2001      |\n",
      "|5  |Code33x      |\n",
      "|6  |Alpha1234beta|\n",
      "|7  |XYZ000       |\n",
      "|8  |num42value   |\n",
      "|9  |Num100number |\n",
      "|10 |Val56        |\n",
      "|11 |AlphaBeta123 |\n",
      "|12 |Box789       |\n",
      "|13 |CodeX11      |\n",
      "|14 |SampleText2  |\n",
      "|15 |DataValue500 |\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ExtractNumbersSplitCharacters\").getOrCreate()\n",
    "\n",
    "# Sample Data: Strings with mixed content\n",
    "data = [\n",
    "    (1, \"Order123\"),\n",
    "    (2, \"Item4567\"),\n",
    "    (3, \"Prod98\"),\n",
    "    (4, \"Ref2001\"),\n",
    "    (5, \"Code33x\"),\n",
    "    (6, \"Alpha1234beta\"),\n",
    "    (7, \"XYZ000\"),\n",
    "    (8, \"num42value\"),\n",
    "    (9, \"Num100number\"),\n",
    "    (10, \"Val56\"),\n",
    "    (11, \"AlphaBeta123\"),\n",
    "    (12, \"Box789\"),\n",
    "    (13, \"CodeX11\"),\n",
    "    (14, \"SampleText2\"),\n",
    "    (15, \"DataValue500\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"id\", \"mixed_string\"])\n",
    "\n",
    "# Create a Temporary View for Spark SQL\n",
    "df.createOrReplaceTempView(\"mixed_data\")\n",
    "df.cache()\n",
    "# Show the original DataFrame\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----------------+\n",
      "| id| mixed_string|extracted_number|\n",
      "+---+-------------+----------------+\n",
      "|  1|     Order123|               d|\n",
      "|  2|     Item4567|                |\n",
      "|  3|       Prod98|               d|\n",
      "|  4|      Ref2001|                |\n",
      "|  5|      Code33x|               d|\n",
      "|  6|Alpha1234beta|                |\n",
      "|  7|       XYZ000|                |\n",
      "|  8|   num42value|                |\n",
      "|  9| Num100number|                |\n",
      "| 10|        Val56|                |\n",
      "| 11| AlphaBeta123|                |\n",
      "| 12|       Box789|                |\n",
      "| 13|      CodeX11|               d|\n",
      "| 14|  SampleText2|                |\n",
      "| 15| DataValue500|                |\n",
      "+---+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = spark.sql(\"\"\"  \n",
    "                \n",
    "   SELECT id, mixed_string, \n",
    "       regexp_extract(mixed_string, '\\\\d+', 0) AS extracted_number\n",
    "FROM mixed_data;\n",
    "             \n",
    "                \"\"\")\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+\n",
      "| id| mixed_string|only_numbers|\n",
      "+---+-------------+------------+\n",
      "|  1|     Order123|         123|\n",
      "|  2|     Item4567|        4567|\n",
      "|  3|       Prod98|          98|\n",
      "|  4|      Ref2001|        2001|\n",
      "|  5|      Code33x|          33|\n",
      "|  6|Alpha1234beta|        1234|\n",
      "|  7|       XYZ000|         000|\n",
      "|  8|   num42value|          42|\n",
      "|  9| Num100number|         100|\n",
      "| 10|        Val56|          56|\n",
      "| 11| AlphaBeta123|         123|\n",
      "| 12|       Box789|         789|\n",
      "| 13|      CodeX11|          11|\n",
      "| 14|  SampleText2|           2|\n",
      "| 15| DataValue500|         500|\n",
      "+---+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res1 = spark.sql(\"\"\"  \n",
    "                \n",
    "SELECT id, mixed_string, \n",
    "       regexp_replace(mixed_string, '[^0-9]', '') AS only_numbers\n",
    "FROM mixed_data;\n",
    "\n",
    "             \n",
    "                \"\"\")\n",
    "\n",
    "res1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----------------+\n",
      "|id |mixed_string |extracted_number|\n",
      "+---+-------------+----------------+\n",
      "|1  |Order123     |123             |\n",
      "|2  |Item4567     |4567            |\n",
      "|3  |Prod98       |98              |\n",
      "|4  |Ref2001      |2001            |\n",
      "|5  |Code33x      |33              |\n",
      "|6  |Alpha1234beta|1234            |\n",
      "|7  |XYZ000       |000             |\n",
      "|8  |num42value   |42              |\n",
      "|9  |Num100number |100             |\n",
      "|10 |Val56        |56              |\n",
      "|11 |AlphaBeta123 |123             |\n",
      "|12 |Box789       |789             |\n",
      "|13 |CodeX11      |11              |\n",
      "|14 |SampleText2  |2               |\n",
      "|15 |DataValue500 |500             |\n",
      "+---+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "# Extract numbers from the string using regexp_extract\n",
    "df_numbers = df.withColumn(\"extracted_number\", regexp_extract(\"mixed_string\", \"\\\\d+\", 0))\n",
    "\n",
    "# Show the result\n",
    "df_numbers.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+\n",
      "|id |mixed_string |only_numbers|\n",
      "+---+-------------+------------+\n",
      "|1  |Order123     |123         |\n",
      "|2  |Item4567     |4567        |\n",
      "|3  |Prod98       |98          |\n",
      "|4  |Ref2001      |2001        |\n",
      "|5  |Code33x      |33          |\n",
      "|6  |Alpha1234beta|1234        |\n",
      "|7  |XYZ000       |000         |\n",
      "|8  |num42value   |42          |\n",
      "|9  |Num100number |100         |\n",
      "|10 |Val56        |56          |\n",
      "|11 |AlphaBeta123 |123         |\n",
      "|12 |Box789       |789         |\n",
      "|13 |CodeX11      |11          |\n",
      "|14 |SampleText2  |2           |\n",
      "|15 |DataValue500 |500         |\n",
      "+---+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "# Remove non-numeric characters to extract only numbers\n",
    "df_only_numbers = df.withColumn(\"only_numbers\", regexp_replace(\"mixed_string\", \"[^0-9]\", \"\"))\n",
    "\n",
    "# Show the result\n",
    "df_only_numbers.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---------+\n",
      "| id|mixed_string|character|\n",
      "+---+------------+---------+\n",
      "|  1|    Order123|        O|\n",
      "|  1|    Order123|        r|\n",
      "|  1|    Order123|        d|\n",
      "|  1|    Order123|        e|\n",
      "|  1|    Order123|        r|\n",
      "|  1|    Order123|        1|\n",
      "|  1|    Order123|        2|\n",
      "|  1|    Order123|        3|\n",
      "|  2|    Item4567|        I|\n",
      "|  2|    Item4567|        t|\n",
      "|  2|    Item4567|        e|\n",
      "|  2|    Item4567|        m|\n",
      "|  2|    Item4567|        4|\n",
      "|  2|    Item4567|        5|\n",
      "|  2|    Item4567|        6|\n",
      "|  2|    Item4567|        7|\n",
      "|  3|      Prod98|        P|\n",
      "|  3|      Prod98|        r|\n",
      "|  3|      Prod98|        o|\n",
      "|  3|      Prod98|        d|\n",
      "+---+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res2 = spark.sql(\"\"\"  \n",
    "                \n",
    "SELECT id, mixed_string, \n",
    "       explode(split(mixed_string, '')) AS character\n",
    "FROM mixed_data;\n",
    "\n",
    "             \n",
    "                \"\"\")\n",
    "\n",
    "res2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---------+\n",
      "|id |mixed_string|character|\n",
      "+---+------------+---------+\n",
      "|1  |Order123    |O        |\n",
      "|1  |Order123    |r        |\n",
      "|1  |Order123    |d        |\n",
      "|1  |Order123    |e        |\n",
      "|1  |Order123    |r        |\n",
      "|1  |Order123    |1        |\n",
      "|1  |Order123    |2        |\n",
      "|1  |Order123    |3        |\n",
      "|2  |Item4567    |I        |\n",
      "|2  |Item4567    |t        |\n",
      "|2  |Item4567    |e        |\n",
      "|2  |Item4567    |m        |\n",
      "|2  |Item4567    |4        |\n",
      "|2  |Item4567    |5        |\n",
      "|2  |Item4567    |6        |\n",
      "|2  |Item4567    |7        |\n",
      "|3  |Prod98      |P        |\n",
      "|3  |Prod98      |r        |\n",
      "|3  |Prod98      |o        |\n",
      "|3  |Prod98      |d        |\n",
      "+---+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "\n",
    "# Split string into characters and explode them into separate rows\n",
    "df_chars = df.withColumn(\"character\", explode(split(\"mixed_string\", \"\")))\n",
    "\n",
    "# Show the result\n",
    "df_chars.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---------+\n",
      "|id |mixed_string|character|\n",
      "+---+------------+---------+\n",
      "|1  |Order123    |O        |\n",
      "|1  |Order123    |r        |\n",
      "|1  |Order123    |d        |\n",
      "|1  |Order123    |e        |\n",
      "|1  |Order123    |r        |\n",
      "|1  |Order123    |1        |\n",
      "|1  |Order123    |2        |\n",
      "|1  |Order123    |3        |\n",
      "|2  |Item4567    |I        |\n",
      "|2  |Item4567    |t        |\n",
      "|2  |Item4567    |e        |\n",
      "|2  |Item4567    |m        |\n",
      "|2  |Item4567    |4        |\n",
      "|2  |Item4567    |5        |\n",
      "|2  |Item4567    |6        |\n",
      "|2  |Item4567    |7        |\n",
      "|3  |Prod98      |P        |\n",
      "|3  |Prod98      |r        |\n",
      "|3  |Prod98      |o        |\n",
      "|3  |Prod98      |d        |\n",
      "+---+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# Define UDF to split string into characters\n",
    "def split_to_chars(s):\n",
    "    return list(s)\n",
    "\n",
    "# Register UDF\n",
    "split_to_chars_udf = udf(split_to_chars, ArrayType(StringType()))\n",
    "\n",
    "# Use UDF to create a new column\n",
    "df_udf = df.withColumn(\"characters\", split_to_chars_udf(\"mixed_string\"))\n",
    "\n",
    "# Explode characters into rows\n",
    "df_chars_udf = df_udf.select(\"id\", \"mixed_string\", explode(\"characters\").alias(\"character\"))\n",
    "df_chars_udf.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
