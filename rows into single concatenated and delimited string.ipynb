{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H:\\\\pyspark_advanced-coding_interview'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"H:\\pyspark_advanced-coding_interview\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert data from rows into single concatenated and delimited string | STRING_AGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+\n",
      "|id |name   |department|\n",
      "+---+-------+----------+\n",
      "|1  |Alice  |HR        |\n",
      "|2  |Bob    |Finance   |\n",
      "|3  |Charlie|IT        |\n",
      "|4  |David  |HR        |\n",
      "|5  |Eva    |Finance   |\n",
      "|6  |Frank  |IT        |\n",
      "|7  |Grace  |HR        |\n",
      "|8  |Henry  |Finance   |\n",
      "|9  |Ivy    |IT        |\n",
      "|10 |Jack   |Marketing |\n",
      "+---+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"StringAggregation\").getOrCreate()\n",
    "\n",
    "# Sample Data: Names of employees from different departments\n",
    "data = [\n",
    "    (1, \"Alice\", \"HR\"),\n",
    "    (2, \"Bob\", \"Finance\"),\n",
    "    (3, \"Charlie\", \"IT\"),\n",
    "    (4, \"David\", \"HR\"),\n",
    "    (5, \"Eva\", \"Finance\"),\n",
    "    (6, \"Frank\", \"IT\"),\n",
    "    (7, \"Grace\", \"HR\"),\n",
    "    (8, \"Henry\", \"Finance\"),\n",
    "    (9, \"Ivy\", \"IT\"),\n",
    "    (10, \"Jack\", \"Marketing\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"department\"])\n",
    "\n",
    "# Create a Temporary View for Spark SQL\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "# Show the Original DataFrame\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|department|          employees|\n",
      "+----------+-------------------+\n",
      "|        HR|Alice, David, Grace|\n",
      "|   Finance|    Bob, Eva, Henry|\n",
      "|        IT|Charlie, Frank, Ivy|\n",
      "| Marketing|               Jack|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = spark.sql(\"\"\" \n",
    "                \n",
    "SELECT department, \n",
    "       concat_ws(', ', collect_list(name)) AS employees\n",
    "FROM employees\n",
    "GROUP BY department;\n",
    "                         \n",
    "                \n",
    "                \"\"\")\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|department|   unique_employees|\n",
      "+----------+-------------------+\n",
      "|        HR|Grace, Alice, David|\n",
      "|   Finance|    Eva, Henry, Bob|\n",
      "|        IT|Ivy, Frank, Charlie|\n",
      "| Marketing|               Jack|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res1 = spark.sql(\"\"\" \n",
    "                \n",
    "SELECT department, \n",
    "       concat_ws(', ', collect_set(name)) AS unique_employees\n",
    "FROM employees\n",
    "GROUP BY department;\n",
    "                         \n",
    "                \n",
    "                \"\"\")\n",
    "res1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|department|employees          |\n",
      "+----------+-------------------+\n",
      "|HR        |Alice, David, Grace|\n",
      "|Finance   |Bob, Eva, Henry    |\n",
      "|IT        |Charlie, Frank, Ivy|\n",
      "|Marketing |Jack               |\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list, concat_ws\n",
    "\n",
    "# Aggregate names into a single concatenated string by department\n",
    "df_concat = df.groupBy(\"department\").agg(\n",
    "    concat_ws(\", \", collect_list(\"name\")).alias(\"employees\")\n",
    ")\n",
    "\n",
    "# Show the Result\n",
    "df_concat.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|department|unique_employees   |\n",
      "+----------+-------------------+\n",
      "|HR        |Grace, Alice, David|\n",
      "|Finance   |Eva, Henry, Bob    |\n",
      "|IT        |Ivy, Frank, Charlie|\n",
      "|Marketing |Jack               |\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set\n",
    "\n",
    "# Aggregate names into a unique concatenated string by department\n",
    "df_unique_concat = df.groupBy(\"department\").agg(\n",
    "    concat_ws(\", \", collect_set(\"name\")).alias(\"unique_employees\")\n",
    ")\n",
    "\n",
    "# Show the Result\n",
    "df_unique_concat.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|       all_employees|\n",
      "+--------------------+\n",
      "|Alice, Bob, Charl...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res3 = spark.sql(\"\"\" \n",
    " \n",
    " SELECT concat_ws(', ', collect_list(name)) AS all_employees\n",
    "FROM employees;                \n",
    "             \n",
    "                 \"\"\")\n",
    "\n",
    "res3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------+\n",
      "|all_employees                                                  |\n",
      "+---------------------------------------------------------------+\n",
      "|Alice, Bob, Charlie, David, Eva, Frank, Grace, Henry, Ivy, Jack|\n",
      "+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_all_concat = df.agg(\n",
    "    concat_ws(\", \", collect_list(\"name\")).alias(\"all_employees\")\n",
    ")\n",
    "\n",
    "df_all_concat.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|department|employees          |\n",
      "+----------+-------------------+\n",
      "|HR        |Alice, David, Grace|\n",
      "|Finance   |Bob, Eva, Henry    |\n",
      "|IT        |Charlie, Frank, Ivy|\n",
      "|Marketing |Jack               |\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define UDF to concatenate names\n",
    "def concatenate_names(names):\n",
    "    return \", \".join(names)\n",
    "\n",
    "# Register UDF\n",
    "concat_udf = udf(concatenate_names, StringType())\n",
    "\n",
    "# Use UDF to aggregate names\n",
    "df_udf_concat = df.groupBy(\"department\").agg(\n",
    "    concat_udf(collect_list(\"name\")).alias(\"employees\")\n",
    ")\n",
    "\n",
    "# Show the Result\n",
    "df_udf_concat.show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
