{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H:\\\\pyspark_advanced-coding_interview'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"H:\\pyspark_advanced-coding_interview\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------+--------+------+------+\n",
      "|ProductID|ProductName|January|February| March| April|\n",
      "+---------+-----------+-------+--------+------+------+\n",
      "|        1|  Product A| 1000.0|  1100.0|1200.0|1300.0|\n",
      "|        2|  Product B| 1500.0|  1600.0|1700.0|1800.0|\n",
      "|        3|  Product C| 2000.0|  2100.0|2200.0|2300.0|\n",
      "|        4|  Product D| 2500.0|  2600.0|2700.0|2800.0|\n",
      "|        5|  Product E| 3000.0|  3100.0|3200.0|3300.0|\n",
      "|        6|  Product F| 3500.0|  3600.0|3700.0|3800.0|\n",
      "|        7|  Product G| 4000.0|  4100.0|4200.0|4300.0|\n",
      "|        8|  Product H| 4500.0|  4600.0|4700.0|4800.0|\n",
      "|        9|  Product I| 5000.0|  5100.0|5200.0|5300.0|\n",
      "|       10|  Product J| 5500.0|  5600.0|5700.0|5800.0|\n",
      "|       11|  Product K| 6000.0|  6100.0|6200.0|6300.0|\n",
      "|       12|  Product L| 6500.0|  6600.0|6700.0|6800.0|\n",
      "|       13|  Product M| 7000.0|  7100.0|7200.0|7300.0|\n",
      "|       14|  Product N| 7500.0|  7600.0|7700.0|7800.0|\n",
      "|       15|  Product O| 8000.0|  8100.0|8200.0|8300.0|\n",
      "+---------+-----------+-------+--------+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 55637)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\lpdda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"c:\\Users\\lpdda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"c:\\Users\\lpdda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"c:\\Users\\lpdda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"C:\\spark\\python\\pyspark\\accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"C:\\spark\\python\\pyspark\\accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "       ^^^^^^\n",
      "  File \"C:\\spark\\python\\pyspark\\accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\lpdda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"UnpivotExample\").getOrCreate()\n",
    "\n",
    "# Define schema using StructType and StructField\n",
    "schema = StructType([\n",
    "    StructField(\"ProductID\", IntegerType(), True),\n",
    "    StructField(\"ProductName\", StringType(), True),\n",
    "    StructField(\"January\", DoubleType(), True),\n",
    "    StructField(\"February\", DoubleType(), True),\n",
    "    StructField(\"March\", DoubleType(), True),\n",
    "    StructField(\"April\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Sample data (ProductID, ProductName, January, February, March, April)\n",
    "data = [\n",
    "    (1, \"Product A\", 1000.0, 1100.0, 1200.0, 1300.0),\n",
    "    (2, \"Product B\", 1500.0, 1600.0, 1700.0, 1800.0),\n",
    "    (3, \"Product C\", 2000.0, 2100.0, 2200.0, 2300.0),\n",
    "    (4, \"Product D\", 2500.0, 2600.0, 2700.0, 2800.0),\n",
    "    (5, \"Product E\", 3000.0, 3100.0, 3200.0, 3300.0),\n",
    "    (6, \"Product F\", 3500.0, 3600.0, 3700.0, 3800.0),\n",
    "    (7, \"Product G\", 4000.0, 4100.0, 4200.0, 4300.0),\n",
    "    (8, \"Product H\", 4500.0, 4600.0, 4700.0, 4800.0),\n",
    "    (9, \"Product I\", 5000.0, 5100.0, 5200.0, 5300.0),\n",
    "    (10, \"Product J\", 5500.0, 5600.0, 5700.0, 5800.0),\n",
    "    (11, \"Product K\", 6000.0, 6100.0, 6200.0, 6300.0),\n",
    "    (12, \"Product L\", 6500.0, 6600.0, 6700.0, 6800.0),\n",
    "    (13, \"Product M\", 7000.0, 7100.0, 7200.0, 7300.0),\n",
    "    (14, \"Product N\", 7500.0, 7600.0, 7700.0, 7800.0),\n",
    "    (15, \"Product O\", 8000.0, 8100.0, 8200.0, 8300.0)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.cache()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------+------+\n",
      "|ProductID|ProductName|   Month| Sales|\n",
      "+---------+-----------+--------+------+\n",
      "|        1|  Product A| January|1000.0|\n",
      "|        2|  Product B| January|1500.0|\n",
      "|        3|  Product C| January|2000.0|\n",
      "|        4|  Product D| January|2500.0|\n",
      "|        5|  Product E| January|3000.0|\n",
      "|        6|  Product F| January|3500.0|\n",
      "|        7|  Product G| January|4000.0|\n",
      "|        8|  Product H| January|4500.0|\n",
      "|        9|  Product I| January|5000.0|\n",
      "|       10|  Product J| January|5500.0|\n",
      "|       11|  Product K| January|6000.0|\n",
      "|       12|  Product L| January|6500.0|\n",
      "|       13|  Product M| January|7000.0|\n",
      "|       14|  Product N| January|7500.0|\n",
      "|       15|  Product O| January|8000.0|\n",
      "|        1|  Product A|February|1100.0|\n",
      "|        2|  Product B|February|1600.0|\n",
      "|        3|  Product C|February|2100.0|\n",
      "|        4|  Product D|February|2600.0|\n",
      "|        5|  Product E|February|3100.0|\n",
      "+---------+-----------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query3 = spark.sql(\"\"\"\n",
    "SELECT ProductID, ProductName, 'January' AS Month, January AS Sales FROM SalesData\n",
    "UNION ALL\n",
    "SELECT ProductID, ProductName, 'February' AS Month, February AS Sales FROM SalesData\n",
    "UNION ALL\n",
    "SELECT ProductID, ProductName, 'March' AS Month, March AS Sales FROM SalesData\n",
    "UNION ALL\n",
    "SELECT ProductID, ProductName, 'April' AS Month, April AS Sales FROM SalesData\n",
    "\n",
    "                   \"\"\")\n",
    "query3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------+------+\n",
      "|ProductID|ProductName|   Month| Sales|\n",
      "+---------+-----------+--------+------+\n",
      "|        1|  Product A| January|1000.0|\n",
      "|        1|  Product A|February|1100.0|\n",
      "|        1|  Product A|   March|1200.0|\n",
      "|        1|  Product A|   April|1300.0|\n",
      "|        2|  Product B| January|1500.0|\n",
      "|        2|  Product B|February|1600.0|\n",
      "|        2|  Product B|   March|1700.0|\n",
      "|        2|  Product B|   April|1800.0|\n",
      "|        3|  Product C| January|2000.0|\n",
      "|        3|  Product C|February|2100.0|\n",
      "|        3|  Product C|   March|2200.0|\n",
      "|        3|  Product C|   April|2300.0|\n",
      "|        4|  Product D| January|2500.0|\n",
      "|        4|  Product D|February|2600.0|\n",
      "|        4|  Product D|   March|2700.0|\n",
      "|        4|  Product D|   April|2800.0|\n",
      "|        5|  Product E| January|3000.0|\n",
      "|        5|  Product E|February|3100.0|\n",
      "|        5|  Product E|   March|3200.0|\n",
      "|        5|  Product E|   April|3300.0|\n",
      "+---------+-----------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a temporary table\n",
    "df.createOrReplaceTempView(\"SalesData\")\n",
    "\n",
    "query = spark.sql(\"\"\"  \n",
    "SELECT ProductID, ProductName, \n",
    "       stack(4, 'January', January, 'February', February, 'March', March, 'April', April) AS (Month, Sales)\n",
    "FROM SalesData\n",
    " \"\"\")\n",
    "\n",
    "query.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------+------+\n",
      "|ProductID|ProductName|   Month| Sales|\n",
      "+---------+-----------+--------+------+\n",
      "|        1|  Product A| January|1000.0|\n",
      "|        1|  Product A|February|1100.0|\n",
      "|        1|  Product A|   March|1200.0|\n",
      "|        1|  Product A|   April|1300.0|\n",
      "|        2|  Product B| January|1500.0|\n",
      "|        2|  Product B|February|1600.0|\n",
      "|        2|  Product B|   March|1700.0|\n",
      "|        2|  Product B|   April|1800.0|\n",
      "|        3|  Product C| January|2000.0|\n",
      "|        3|  Product C|February|2100.0|\n",
      "|        3|  Product C|   March|2200.0|\n",
      "|        3|  Product C|   April|2300.0|\n",
      "|        4|  Product D| January|2500.0|\n",
      "|        4|  Product D|February|2600.0|\n",
      "|        4|  Product D|   March|2700.0|\n",
      "|        4|  Product D|   April|2800.0|\n",
      "|        5|  Product E| January|3000.0|\n",
      "|        5|  Product E|February|3100.0|\n",
      "|        5|  Product E|   March|3200.0|\n",
      "|        5|  Product E|   April|3300.0|\n",
      "+---------+-----------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query5 = spark.sql(\"\"\" \n",
    "SELECT ProductID, ProductName, exploded.Month, exploded.Sales\n",
    "FROM SalesData\n",
    "LATERAL VIEW EXPLODE(\n",
    "    ARRAY(\n",
    "        STRUCT('January' AS Month, January AS Sales),\n",
    "        STRUCT('February' AS Month, February AS Sales),\n",
    "        STRUCT('March' AS Month, March AS Sales),\n",
    "        STRUCT('April' AS Month, April AS Sales)\n",
    "    )\n",
    ") AS exploded\n",
    "\n",
    "\n",
    "    \"\"\")\n",
    "query5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------+------+\n",
      "|ProductID|ProductName|   Month| Sales|\n",
      "+---------+-----------+--------+------+\n",
      "|        1|  Product A| January|1000.0|\n",
      "|        2|  Product B| January|1500.0|\n",
      "|        3|  Product C| January|2000.0|\n",
      "|        4|  Product D| January|2500.0|\n",
      "|        5|  Product E| January|3000.0|\n",
      "|        6|  Product F| January|3500.0|\n",
      "|        7|  Product G| January|4000.0|\n",
      "|        8|  Product H| January|4500.0|\n",
      "|        9|  Product I| January|5000.0|\n",
      "|       10|  Product J| January|5500.0|\n",
      "|       11|  Product K| January|6000.0|\n",
      "|       12|  Product L| January|6500.0|\n",
      "|       13|  Product M| January|7000.0|\n",
      "|       14|  Product N| January|7500.0|\n",
      "|       15|  Product O| January|8000.0|\n",
      "|        1|  Product A|February|1100.0|\n",
      "|        2|  Product B|February|1600.0|\n",
      "|        3|  Product C|February|2100.0|\n",
      "|        4|  Product D|February|2600.0|\n",
      "|        5|  Product E|February|3100.0|\n",
      "+---------+-----------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query7 = spark.sql(\"\"\"       \n",
    "SELECT ProductID, ProductName, 'January' AS Month, CASE WHEN January IS NOT NULL THEN January ELSE NULL END AS Sales FROM SalesData\n",
    "UNION ALL\n",
    "SELECT ProductID, ProductName, 'February' AS Month, CASE WHEN February IS NOT NULL THEN February ELSE NULL END AS Sales FROM SalesData\n",
    "UNION ALL\n",
    "SELECT ProductID, ProductName, 'March' AS Month, CASE WHEN March IS NOT NULL THEN March ELSE NULL END AS Sales FROM SalesData\n",
    "UNION ALL\n",
    "SELECT ProductID, ProductName, 'April' AS Month, CASE WHEN April IS NOT NULL THEN April ELSE NULL END AS Sales FROM SalesData\n",
    "                \n",
    "                   \"\"\")\n",
    "query7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------+--------+\n",
      "|ProductID|ProductName| Sales|   month|\n",
      "+---------+-----------+------+--------+\n",
      "|        1|  Product A|1000.0| January|\n",
      "|        1|  Product A|1100.0|February|\n",
      "|        1|  Product A|1200.0|   March|\n",
      "|        1|  Product A|1300.0|   April|\n",
      "|        2|  Product B|1500.0| January|\n",
      "|        2|  Product B|1600.0|February|\n",
      "|        2|  Product B|1700.0|   March|\n",
      "|        2|  Product B|1800.0|   April|\n",
      "|        3|  Product C|2000.0| January|\n",
      "|        3|  Product C|2100.0|February|\n",
      "|        3|  Product C|2200.0|   March|\n",
      "|        3|  Product C|2300.0|   April|\n",
      "|        4|  Product D|2500.0| January|\n",
      "|        4|  Product D|2600.0|February|\n",
      "|        4|  Product D|2700.0|   March|\n",
      "|        4|  Product D|2800.0|   April|\n",
      "|        5|  Product E|3000.0| January|\n",
      "|        5|  Product E|3100.0|February|\n",
      "|        5|  Product E|3200.0|   March|\n",
      "|        5|  Product E|3300.0|   April|\n",
      "+---------+-----------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Unpivot |Convert data from columns into Rows\n",
    "\n",
    "query1 = spark.sql(\"\"\"\n",
    "SELECT ProductID, ProductName, \n",
    "       CASE WHEN month = 'January' THEN January\n",
    "            WHEN month = 'February' THEN February\n",
    "            WHEN month = 'March' THEN March\n",
    "            WHEN month = 'April' THEN April END AS Sales,\n",
    "       month\n",
    "FROM SalesData \n",
    "LATERAL VIEW EXPLODE(array('January', 'February', 'March', 'April')) AS month\n",
    "\"\"\")\n",
    "\n",
    "query1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------+------+\n",
      "|ProductID|ProductName|   Month| Sales|\n",
      "+---------+-----------+--------+------+\n",
      "|        1|  Product A| January|1000.0|\n",
      "|        1|  Product A|February|1100.0|\n",
      "|        1|  Product A|   March|1200.0|\n",
      "|        1|  Product A|   April|1300.0|\n",
      "|        2|  Product B| January|1500.0|\n",
      "|        2|  Product B|February|1600.0|\n",
      "|        2|  Product B|   March|1700.0|\n",
      "|        2|  Product B|   April|1800.0|\n",
      "|        3|  Product C| January|2000.0|\n",
      "|        3|  Product C|February|2100.0|\n",
      "|        3|  Product C|   March|2200.0|\n",
      "|        3|  Product C|   April|2300.0|\n",
      "|        4|  Product D| January|2500.0|\n",
      "|        4|  Product D|February|2600.0|\n",
      "|        4|  Product D|   March|2700.0|\n",
      "|        4|  Product D|   April|2800.0|\n",
      "|        5|  Product E| January|3000.0|\n",
      "|        5|  Product E|February|3100.0|\n",
      "|        5|  Product E|   March|3200.0|\n",
      "|        5|  Product E|   April|3300.0|\n",
      "+---------+-----------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unpivot the DataFrame using selectExpr and stack\n",
    "unpivot_df = df.selectExpr(\"ProductID\", \"ProductName\", \n",
    "                           \"stack(4, 'January', January, 'February', February, 'March', March, 'April', April) as (Month, Sales)\")\n",
    "\n",
    "# Show the unpivoted data\n",
    "unpivot_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------+------+\n",
      "|ProductID|ProductName|   Month| Sales|\n",
      "+---------+-----------+--------+------+\n",
      "|        1|  Product A| January|1000.0|\n",
      "|        2|  Product B| January|1500.0|\n",
      "|        3|  Product C| January|2000.0|\n",
      "|        4|  Product D| January|2500.0|\n",
      "|        5|  Product E| January|3000.0|\n",
      "|        6|  Product F| January|3500.0|\n",
      "|        7|  Product G| January|4000.0|\n",
      "|        8|  Product H| January|4500.0|\n",
      "|        9|  Product I| January|5000.0|\n",
      "|       10|  Product J| January|5500.0|\n",
      "|       11|  Product K| January|6000.0|\n",
      "|       12|  Product L| January|6500.0|\n",
      "|       13|  Product M| January|7000.0|\n",
      "|       14|  Product N| January|7500.0|\n",
      "|       15|  Product O| January|8000.0|\n",
      "|        1|  Product A|February|1100.0|\n",
      "|        2|  Product B|February|1600.0|\n",
      "|        3|  Product C|February|2100.0|\n",
      "|        4|  Product D|February|2600.0|\n",
      "|        5|  Product E|February|3100.0|\n",
      "+---------+-----------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit,col\n",
    "\n",
    "\n",
    "# Convert columns to rows by unioning individual selects\n",
    "january = df.select(\"ProductID\", \"ProductName\", lit(\"January\").alias(\"Month\"), col(\"January\").alias(\"Sales\"))\n",
    "february = df.select(\"ProductID\", \"ProductName\", lit(\"February\").alias(\"Month\"), col(\"February\").alias(\"Sales\"))\n",
    "march = df.select(\"ProductID\", \"ProductName\", lit(\"March\").alias(\"Month\"), col(\"March\").alias(\"Sales\"))\n",
    "april = df.select(\"ProductID\", \"ProductName\", lit(\"April\").alias(\"Month\"), col(\"April\").alias(\"Sales\"))\n",
    "\n",
    "# Combine all DataFrames using union\n",
    "unpivot_union_df = january.union(february).union(march).union(april)\n",
    "unpivot_union_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------+------+\n",
      "|ProductID|ProductName|   Month| Sales|\n",
      "+---------+-----------+--------+------+\n",
      "|        1|  Product A| January|1000.0|\n",
      "|        1|  Product A|February|1100.0|\n",
      "|        1|  Product A|   March|1200.0|\n",
      "|        1|  Product A|   April|1300.0|\n",
      "|        2|  Product B| January|1500.0|\n",
      "|        2|  Product B|February|1600.0|\n",
      "|        2|  Product B|   March|1700.0|\n",
      "|        2|  Product B|   April|1800.0|\n",
      "|        3|  Product C| January|2000.0|\n",
      "|        3|  Product C|February|2100.0|\n",
      "|        3|  Product C|   March|2200.0|\n",
      "|        3|  Product C|   April|2300.0|\n",
      "|        4|  Product D| January|2500.0|\n",
      "|        4|  Product D|February|2600.0|\n",
      "|        4|  Product D|   March|2700.0|\n",
      "|        4|  Product D|   April|2800.0|\n",
      "|        5|  Product E| January|3000.0|\n",
      "|        5|  Product E|February|3100.0|\n",
      "|        5|  Product E|   March|3200.0|\n",
      "|        5|  Product E|   April|3300.0|\n",
      "+---------+-----------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array, struct, explode\n",
    "\n",
    "# Create an array of structs where each struct represents a (month, sales) pair\n",
    "df_with_array = df.withColumn(\"Months\", \n",
    "                              array(struct(lit(\"January\").alias(\"Month\"), col(\"January\").alias(\"Sales\")),\n",
    "                                    struct(lit(\"February\").alias(\"Month\"), col(\"February\").alias(\"Sales\")),\n",
    "                                    struct(lit(\"March\").alias(\"Month\"), col(\"March\").alias(\"Sales\")),\n",
    "                                    struct(lit(\"April\").alias(\"Month\"), col(\"April\").alias(\"Sales\"))\n",
    "                                   ))\n",
    "\n",
    "# Explode the array to convert columns into rows\n",
    "unpivot_explode_df = df_with_array.select(\"ProductID\", \"ProductName\", explode(\"Months\").alias(\"MonthSales\"))\n",
    "unpivot_explode_df = unpivot_explode_df.select(\"ProductID\", \"ProductName\", col(\"MonthSales.Month\"), col(\"MonthSales.Sales\"))\n",
    "\n",
    "# Show the unpivoted data\n",
    "unpivot_explode_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
