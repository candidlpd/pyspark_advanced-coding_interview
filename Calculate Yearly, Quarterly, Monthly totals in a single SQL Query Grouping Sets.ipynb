{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H:\\pyspark_advanced-coding_interview\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"H:\\pyspark_advanced-coding_interview\")\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session with optimized settings\n",
    "spark = (\n",
    "    SparkSession.builder \n",
    "    .appName(\"OptimizedLocalSpark\") \n",
    "    .config(\"spark.driver.memory\", \"8g\")        \n",
    "    .config(\"spark.executor.memory\", \"8g\")    \n",
    "    .config(\"spark.executor.cores\", \"4\")       \n",
    "    .config(\"spark.cores.max\", \"12\")           \n",
    "    .config(\"spark.sql.shuffle.partitions\", \"28\")  \n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Yearly, Quarterly, Monthly totals in a single SQL Query |Grouping Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+------+\n",
      "|sale_id|sale_date |product  |amount|\n",
      "+-------+----------+---------+------+\n",
      "|1      |2023-01-15|Product A|1500  |\n",
      "|2      |2023-01-20|Product B|2000  |\n",
      "|3      |2023-02-11|Product A|3000  |\n",
      "|4      |2023-02-28|Product C|2500  |\n",
      "|5      |2023-03-05|Product B|3200  |\n",
      "|6      |2023-03-20|Product C|2700  |\n",
      "|7      |2023-04-05|Product A|4000  |\n",
      "|8      |2023-05-15|Product B|2300  |\n",
      "|9      |2023-06-10|Product C|2900  |\n",
      "|10     |2023-07-22|Product A|1500  |\n",
      "|11     |2023-08-18|Product B|3300  |\n",
      "|12     |2023-09-25|Product C|3400  |\n",
      "|13     |2023-10-10|Product A|1900  |\n",
      "|14     |2023-11-21|Product B|2500  |\n",
      "|15     |2023-12-15|Product C|2700  |\n",
      "+-------+----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, quarter\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ComplexAggregationExample\").getOrCreate()\n",
    "\n",
    "# Sample Data: Sales records with date, product, and sales amount\n",
    "data = [\n",
    "    (1, \"2023-01-15\", \"Product A\", 1500),\n",
    "    (2, \"2023-01-20\", \"Product B\", 2000),\n",
    "    (3, \"2023-02-11\", \"Product A\", 3000),\n",
    "    (4, \"2023-02-28\", \"Product C\", 2500),\n",
    "    (5, \"2023-03-05\", \"Product B\", 3200),\n",
    "    (6, \"2023-03-20\", \"Product C\", 2700),\n",
    "    (7, \"2023-04-05\", \"Product A\", 4000),\n",
    "    (8, \"2023-05-15\", \"Product B\", 2300),\n",
    "    (9, \"2023-06-10\", \"Product C\", 2900),\n",
    "    (10, \"2023-07-22\", \"Product A\", 1500),\n",
    "    (11, \"2023-08-18\", \"Product B\", 3300),\n",
    "    (12, \"2023-09-25\", \"Product C\", 3400),\n",
    "    (13, \"2023-10-10\", \"Product A\", 1900),\n",
    "    (14, \"2023-11-21\", \"Product B\", 2500),\n",
    "    (15, \"2023-12-15\", \"Product C\", 2700),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"sale_id\", \"sale_date\", \"product\", \"amount\"])\n",
    "\n",
    "# Create a Temporary View for Spark SQL\n",
    "df.createOrReplaceTempView(\"sales_data\")\n",
    "\n",
    "# Show the Original DataFrame\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+-----------+\n",
      "|year|quarter|month|total_sales|\n",
      "+----+-------+-----+-----------+\n",
      "|2023|   null| null|      39400|\n",
      "|2023|   null|    1|       3500|\n",
      "|2023|   null|    2|       5500|\n",
      "|2023|   null|    3|       5900|\n",
      "|2023|   null|    4|       4000|\n",
      "|2023|   null|    5|       2300|\n",
      "|2023|   null|    6|       2900|\n",
      "|2023|   null|    7|       1500|\n",
      "|2023|   null|    8|       3300|\n",
      "|2023|   null|    9|       3400|\n",
      "|2023|   null|   10|       1900|\n",
      "|2023|   null|   11|       2500|\n",
      "|2023|   null|   12|       2700|\n",
      "|2023|      1| null|      14900|\n",
      "|2023|      2| null|       9200|\n",
      "|2023|      3| null|       8200|\n",
      "|2023|      4| null|       7100|\n",
      "+----+-------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = spark.sql(\"\"\" \n",
    "                \n",
    "SELECT \n",
    "    year(sale_date) AS year, \n",
    "    quarter(sale_date) AS quarter, \n",
    "    month(sale_date) AS month, \n",
    "    SUM(amount) AS total_sales\n",
    "FROM sales_data\n",
    "GROUP BY GROUPING SETS (\n",
    "    (year(sale_date)),                 \n",
    "    (year(sale_date), quarter(sale_date)), \n",
    "    (year(sale_date), month(sale_date))    \n",
    ")\n",
    "ORDER BY year, quarter, month;\n",
    "\n",
    "\n",
    "\n",
    "              \n",
    "                \"\"\")\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+-----------+\n",
      "|year|quarter|month|total_sales|\n",
      "+----+-------+-----+-----------+\n",
      "|null|null   |null |39400      |\n",
      "|2023|null   |null |39400      |\n",
      "|2023|1      |null |14900      |\n",
      "|2023|1      |1    |3500       |\n",
      "|2023|1      |2    |5500       |\n",
      "|2023|1      |3    |5900       |\n",
      "|2023|2      |null |9200       |\n",
      "|2023|2      |4    |4000       |\n",
      "|2023|2      |5    |2300       |\n",
      "|2023|2      |6    |2900       |\n",
      "|2023|3      |null |8200       |\n",
      "|2023|3      |7    |1500       |\n",
      "|2023|3      |8    |3300       |\n",
      "|2023|3      |9    |3400       |\n",
      "|2023|4      |null |7100       |\n",
      "|2023|4      |10   |1900       |\n",
      "|2023|4      |11   |2500       |\n",
      "|2023|4      |12   |2700       |\n",
      "+----+-------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#rollup(\"year\", \"quarter\", \"month\") will group by year, then by year and quarter, and finally by year, quarter, and month.\n",
    "\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "# Add columns for year, quarter, and month\n",
    "df_with_periods = df.withColumn(\"year\", year(\"sale_date\")) \\\n",
    "                    .withColumn(\"quarter\", quarter(\"sale_date\")) \\\n",
    "                    .withColumn(\"month\", month(\"sale_date\"))\n",
    "\n",
    "# Use rollup to simulate GROUPING SETS and calculate totals\n",
    "df_grouped = df_with_periods.rollup(\"year\", \"quarter\", \"month\").agg(_sum(\"amount\").alias(\"total_sales\")).orderBy(\"year\", \"quarter\", \"month\")\n",
    "\n",
    "# Show the result\n",
    "df_grouped.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+-----------+\n",
      "|year|quarter|month|total_sales|\n",
      "+----+-------+-----+-----------+\n",
      "|null|null   |null |39400      |\n",
      "|null|null   |1    |3500       |\n",
      "|null|null   |2    |5500       |\n",
      "|null|null   |3    |5900       |\n",
      "|null|null   |4    |4000       |\n",
      "|null|null   |5    |2300       |\n",
      "|null|null   |6    |2900       |\n",
      "|null|null   |7    |1500       |\n",
      "|null|null   |8    |3300       |\n",
      "|null|null   |9    |3400       |\n",
      "|null|null   |10   |1900       |\n",
      "|null|null   |11   |2500       |\n",
      "|null|null   |12   |2700       |\n",
      "|null|1      |null |14900      |\n",
      "|null|1      |1    |3500       |\n",
      "|null|1      |2    |5500       |\n",
      "|null|1      |3    |5900       |\n",
      "|null|2      |null |9200       |\n",
      "|null|2      |4    |4000       |\n",
      "|null|2      |5    |2300       |\n",
      "+----+-------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Produces all possible combinations of the group-by keys. It can be more comprehensive than rollup if you need deeper insights.\n",
    "\n",
    "# Use cube to get comprehensive combinations for year, quarter, and month\n",
    "df_cubed = df_with_periods.cube(\"year\", \"quarter\", \"month\").agg(_sum(\"amount\").alias(\"total_sales\")).orderBy(\"year\", \"quarter\", \"month\")\n",
    "\n",
    "# Show the result\n",
    "df_cubed.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-------+-----+\n",
      "|year|total_sales|quarter|month|\n",
      "+----+-----------+-------+-----+\n",
      "|2023|39400      |null   |null |\n",
      "|2023|7          |1500   |null |\n",
      "|2023|10         |1900   |null |\n",
      "|2023|5          |2300   |null |\n",
      "|2023|11         |2500   |null |\n",
      "|2023|12         |2700   |null |\n",
      "|2023|6          |2900   |null |\n",
      "|2023|8          |3300   |null |\n",
      "|2023|9          |3400   |null |\n",
      "|2023|1          |3500   |null |\n",
      "|2023|4          |4000   |null |\n",
      "|2023|2          |5500   |null |\n",
      "|2023|3          |5900   |null |\n",
      "|2023|4          |7100   |null |\n",
      "|2023|3          |8200   |null |\n",
      "|2023|2          |9200   |null |\n",
      "|2023|1          |14900  |null |\n",
      "+----+-----------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate totals separately and combine using union\n",
    "from pyspark.sql.functions import sum as _sum, lit\n",
    "yearly_totals = df_with_periods.groupBy(\"year\").agg(_sum(\"amount\").alias(\"total_sales\")).withColumn(\"quarter\", col(\"year\") * lit(None)).withColumn(\"month\", col(\"year\") * lit(None))\n",
    "quarterly_totals = df_with_periods.groupBy(\"year\", \"quarter\").agg(_sum(\"amount\").alias(\"total_sales\")).withColumn(\"month\", col(\"year\") * lit(None))\n",
    "monthly_totals = df_with_periods.groupBy(\"year\", \"month\").agg(_sum(\"amount\").alias(\"total_sales\")).withColumn(\"quarter\", col(\"year\") * lit(None))\n",
    "\n",
    "# Combine all totals\n",
    "combined_totals = yearly_totals.union(quarterly_totals).union(monthly_totals).orderBy(\"year\", \"quarter\", \"month\")\n",
    "\n",
    "# Show the result\n",
    "combined_totals.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
