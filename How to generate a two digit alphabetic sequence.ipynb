{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H:\\\\pyspark_advanced-coding_interview'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"H:\\pyspark_advanced-coding_interview\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|TwoDigitAlpha|\n",
      "+-------------+\n",
      "|           AA|\n",
      "|           AB|\n",
      "|           AC|\n",
      "|           AD|\n",
      "|           AE|\n",
      "|           AF|\n",
      "|           AG|\n",
      "|           AH|\n",
      "|           AI|\n",
      "|           AJ|\n",
      "|           AK|\n",
      "|           AL|\n",
      "|           AM|\n",
      "|           AN|\n",
      "|           AO|\n",
      "|           AP|\n",
      "|           AQ|\n",
      "|           AR|\n",
      "|           AS|\n",
      "|           AT|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+\n",
      "|TwoDigitAlpha|\n",
      "+-------------+\n",
      "|           AA|\n",
      "|           AB|\n",
      "|           AC|\n",
      "|           AD|\n",
      "|           AE|\n",
      "|           AF|\n",
      "|           AG|\n",
      "|           AH|\n",
      "|           AI|\n",
      "|           AJ|\n",
      "+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark Session (if not already created)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"AlphabeticSequence\").getOrCreate()\n",
    "\n",
    "# Generate the first and second letter sequences using ASCII values\n",
    "query = spark.sql( \"\"\"\n",
    "WITH Alphabet AS (\n",
    "    SELECT CHAR(SEQUENCE.ID) AS Letter\n",
    "    FROM (SELECT EXPLODE(SEQUENCE(65, 90)) AS ID) AS SEQUENCE\n",
    ")\n",
    "SELECT CONCAT(A.Letter, B.Letter) AS TwoDigitAlpha\n",
    "FROM Alphabet A\n",
    "CROSS JOIN Alphabet B\n",
    "ORDER BY TwoDigitAlpha\n",
    "\"\"\"\n",
    ")\n",
    "query.show()\n",
    "\n",
    "\n",
    "query.show(10)  # Show the first 10 results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|TwoDigitAlphabetic|\n",
      "+------------------+\n",
      "|                AA|\n",
      "|                AB|\n",
      "|                AC|\n",
      "|                AD|\n",
      "|                AE|\n",
      "|                AF|\n",
      "|                AG|\n",
      "|                AH|\n",
      "|                AI|\n",
      "|                AJ|\n",
      "|                AK|\n",
      "|                AL|\n",
      "|                AM|\n",
      "|                AN|\n",
      "|                AO|\n",
      "|                AP|\n",
      "|                AQ|\n",
      "|                AR|\n",
      "|                AS|\n",
      "|                AT|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query2 = spark.sql(\"\"\"WITH Letters AS (\n",
    "    SELECT char(ascii('A') + n) AS Letter\n",
    "    FROM (\n",
    "        SELECT explode(array(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25)) AS n\n",
    "    )\n",
    ")\n",
    "SELECT CONCAT(l1.Letter, l2.Letter) AS TwoDigitAlphabetic\n",
    "FROM Letters l1\n",
    "CROSS JOIN Letters l2\n",
    "ORDER BY TwoDigitAlphabetic;\n",
    "\"\"\")\n",
    "query2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|TwoDigitAlphabetic|\n",
      "+------------------+\n",
      "|                AA|\n",
      "|                AB|\n",
      "|                AC|\n",
      "|                AD|\n",
      "|                AE|\n",
      "|                AF|\n",
      "|                AG|\n",
      "|                AH|\n",
      "|                AI|\n",
      "|                AJ|\n",
      "|                AK|\n",
      "|                AL|\n",
      "|                AM|\n",
      "|                AN|\n",
      "|                AO|\n",
      "|                AP|\n",
      "|                AQ|\n",
      "|                AR|\n",
      "|                AS|\n",
      "|                AT|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query3 = spark.sql(\"\"\" \n",
    "    WITH Alphabet AS (\n",
    "    SELECT char(ASCII('A') + idx) AS Letter\n",
    "    FROM (\n",
    "        SELECT posexplode(array_repeat(0, 26)) AS (idx, _)\n",
    "    )\n",
    ")\n",
    "SELECT CONCAT(A.Letter, B.Letter) AS TwoDigitAlphabetic\n",
    "FROM Alphabet A\n",
    "CROSS JOIN Alphabet B\n",
    "ORDER BY TwoDigitAlphabetic;\n",
    "               \n",
    "                   \"\"\")\n",
    "\n",
    "query3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|TwoDigitAlphabetic|\n",
      "+------------------+\n",
      "|                AA|\n",
      "|                AB|\n",
      "|                AC|\n",
      "|                AD|\n",
      "|                AE|\n",
      "|                AF|\n",
      "|                AG|\n",
      "|                AH|\n",
      "|                AI|\n",
      "|                AJ|\n",
      "|                AK|\n",
      "|                AL|\n",
      "|                AM|\n",
      "|                AN|\n",
      "|                AO|\n",
      "|                AP|\n",
      "|                AQ|\n",
      "|                AR|\n",
      "|                AS|\n",
      "|                AT|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query4 = spark.sql(\"\"\" \n",
    "WITH Alphabet AS (\n",
    "    SELECT char(ASCII('A') + idx) AS Letter\n",
    "    FROM (\n",
    "        SELECT posexplode(array_repeat(0, 26)) AS (idx, _)\n",
    "    )\n",
    ")\n",
    "SELECT CONCAT(A.Letter, B.Letter) AS TwoDigitAlphabetic\n",
    "FROM Alphabet A\n",
    "CROSS JOIN Alphabet B\n",
    "ORDER BY TwoDigitAlphabetic;\n",
    "\n",
    "\n",
    "              \n",
    "                   \"\"\")\n",
    "\n",
    "query4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|TwoDigitAlphabetic|\n",
      "+------------------+\n",
      "|                AA|\n",
      "|                AB|\n",
      "|                AC|\n",
      "|                AD|\n",
      "|                AE|\n",
      "|                AF|\n",
      "|                AG|\n",
      "|                AH|\n",
      "|                AI|\n",
      "|                AJ|\n",
      "|                AK|\n",
      "|                AL|\n",
      "|                AM|\n",
      "|                AN|\n",
      "|                AO|\n",
      "|                AP|\n",
      "|                AQ|\n",
      "|                AR|\n",
      "|                AS|\n",
      "|                AT|\n",
      "|                AU|\n",
      "|                AV|\n",
      "|                AW|\n",
      "|                AX|\n",
      "|                AY|\n",
      "|                AZ|\n",
      "|                BA|\n",
      "|                BB|\n",
      "|                BC|\n",
      "|                BD|\n",
      "|                BE|\n",
      "|                BF|\n",
      "|                BG|\n",
      "|                BH|\n",
      "|                BI|\n",
      "|                BJ|\n",
      "|                BK|\n",
      "|                BL|\n",
      "|                BM|\n",
      "|                BN|\n",
      "|                BO|\n",
      "|                BP|\n",
      "|                BQ|\n",
      "|                BR|\n",
      "|                BS|\n",
      "|                BT|\n",
      "|                BU|\n",
      "|                BV|\n",
      "|                BW|\n",
      "|                BX|\n",
      "|                BY|\n",
      "|                BZ|\n",
      "+------------------+\n",
      "only showing top 52 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, ascii, col\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"AlphabetSequence\").getOrCreate()\n",
    "\n",
    "# Generate DataFrame for letters A-Z using ASCII values\n",
    "letters = spark.sql(\"SELECT char(ascii('A') + n) AS Letter FROM (SELECT explode(sequence(0, 25)) AS n)\")\n",
    "\n",
    "# Use Cartesian product to create combinations\n",
    "two_letter_combinations = letters.alias(\"l1\").crossJoin(letters.alias(\"l2\")) \\\n",
    "    .select(expr(\"concat(l1.Letter, l2.Letter) AS TwoDigitAlphabetic\")) \\\n",
    "    .orderBy(\"TwoDigitAlphabetic\")\n",
    "\n",
    "two_letter_combinations.show(52)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+\n",
      "|Letter1|Letter2|TwoLetterCode|\n",
      "+-------+-------+-------------+\n",
      "|      A|      A|         null|\n",
      "|      A|      B|         null|\n",
      "|      A|      C|         null|\n",
      "|      A|      D|         null|\n",
      "|      A|      E|         null|\n",
      "+-------+-------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------+\n",
      "|TwoLetterCode|\n",
      "+-------------+\n",
      "|           AA|\n",
      "|           AB|\n",
      "|           AC|\n",
      "|           AD|\n",
      "|           AE|\n",
      "|           AF|\n",
      "|           AG|\n",
      "|           AH|\n",
      "|           AI|\n",
      "|           AJ|\n",
      "|           AK|\n",
      "|           AL|\n",
      "|           AM|\n",
      "|           AN|\n",
      "|           AO|\n",
      "|           AP|\n",
      "|           AQ|\n",
      "|           AR|\n",
      "|           AS|\n",
      "|           AT|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"TwoDigitAlphabeticSequence\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame with letters A to Z\n",
    "letters = [chr(i) for i in range(ord('A'), ord('Z') + 1)]\n",
    "df1 = spark.createDataFrame([(l,) for l in letters], [\"Letter1\"])\n",
    "df2 = spark.createDataFrame([(l,) for l in letters], [\"Letter2\"])\n",
    "\n",
    "# Perform a cartesian join to generate two-letter sequences\n",
    "two_letter_sequence = df1.crossJoin(df2).select(col(\"Letter1\"), col(\"Letter2\"))\n",
    "\n",
    "# Combine the two letters to form a two-letter code\n",
    "result_df = two_letter_sequence.withColumn(\"TwoLetterCode\", col(\"Letter1\") + col(\"Letter2\"))\n",
    "result_df.show(5)  # Show first 5 results for brevity\n",
    "\n",
    "\n",
    "\n",
    "df1.createOrReplaceTempView(\"Letters1\")\n",
    "df2.createOrReplaceTempView(\"Letters2\")\n",
    "\n",
    "query6 = spark.sql(\"\"\"\n",
    "SELECT CONCAT(Letter1, Letter2) AS TwoLetterCode\n",
    "FROM Letters1 CROSS JOIN Letters2\n",
    "\"\"\")\n",
    "\n",
    "query6.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|TwoLetterCode|\n",
      "+-------------+\n",
      "|           AA|\n",
      "|           AB|\n",
      "|           AC|\n",
      "|           AD|\n",
      "|           AE|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Create a DataFrame for alphabet letters using ASCII values\n",
    "alphabet_df = spark.createDataFrame([(chr(i),) for i in range(ord('A'), ord('Z') + 1)], [\"Letter\"])\n",
    "\n",
    "# Generate the sequence using two loops and a list comprehension\n",
    "sequence_data = [(chr(i) + chr(j),) for i in range(ord('A'), ord('Z') + 1) for j in range(ord('A'), ord('Z') + 1)]\n",
    "\n",
    "# Convert to DataFrame\n",
    "sequence_df = spark.createDataFrame(sequence_data, [\"TwoLetterCode\"])\n",
    "sequence_df.show(5)  # Show first 5 results for brevity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|TwoLetterCode|\n",
      "+-------------+\n",
      "|           AA|\n",
      "|           AB|\n",
      "|           AC|\n",
      "|           AD|\n",
      "|           AE|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Define a UDF to generate two-letter sequences\n",
    "def generate_two_letter_code():\n",
    "    return [chr(i) + chr(j) for i in range(ord('A'), ord('Z') + 1) for j in range(ord('A'), ord('Z') + 1)]\n",
    "\n",
    "# Convert the list to a PySpark DataFrame\n",
    "two_letter_code_df = spark.createDataFrame(generate_two_letter_code(), StringType()).toDF(\"TwoLetterCode\")\n",
    "\n",
    "# Show the DataFrame\n",
    "two_letter_code_df.show(5)  # Show first 5 results for brevity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
