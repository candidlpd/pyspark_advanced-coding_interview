{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H:\\\\pyspark_advanced-coding_interview'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"H:\\pyspark_advanced-coding_interview\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+\n",
      "|EmployeeID|EmployeeName|Department|\n",
      "+----------+------------+----------+\n",
      "|         1|       Alice|        HR|\n",
      "|         2|         Bob|        HR|\n",
      "|         3|     Charlie|        IT|\n",
      "|         4|       David|        IT|\n",
      "|         5|         Eve|        IT|\n",
      "|         6|       Frank|   Finance|\n",
      "|         7|       Grace|   Finance|\n",
      "|         8|       Heidi|        HR|\n",
      "|         9|        Ivan|     Sales|\n",
      "|        10|        Judy|     Sales|\n",
      "|        11|       Kevin|     Sales|\n",
      "|        12|       Laura|        IT|\n",
      "|        13|     Mallory| Marketing|\n",
      "|        14|        Niaj| Marketing|\n",
      "|        15|       Oscar| Marketing|\n",
      "+----------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"RowToCommaSeparated\").getOrCreate()\n",
    "\n",
    "# Define schema using StructType and StructField\n",
    "schema = StructType([\n",
    "    StructField(\"EmployeeID\", IntegerType(), True),\n",
    "    StructField(\"EmployeeName\", StringType(), True),\n",
    "    StructField(\"Department\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (1, \"Alice\", \"HR\"),\n",
    "    (2, \"Bob\", \"HR\"),\n",
    "    (3, \"Charlie\", \"IT\"),\n",
    "    (4, \"David\", \"IT\"),\n",
    "    (5, \"Eve\", \"IT\"),\n",
    "    (6, \"Frank\", \"Finance\"),\n",
    "    (7, \"Grace\", \"Finance\"),\n",
    "    (8, \"Heidi\", \"HR\"),\n",
    "    (9, \"Ivan\", \"Sales\"),\n",
    "    (10, \"Judy\", \"Sales\"),\n",
    "    (11, \"Kevin\", \"Sales\"),\n",
    "    (12, \"Laura\", \"IT\"),\n",
    "    (13, \"Mallory\", \"Marketing\"),\n",
    "    (14, \"Niaj\", \"Marketing\"),\n",
    "    (15, \"Oscar\", \"Marketing\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.cache()\n",
    "# Register the DataFrame as a temporary table\n",
    "df.createOrReplaceTempView(\"Employees\")\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saprk SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|Department|       EmployeeNames|\n",
      "+----------+--------------------+\n",
      "|        HR|   Alice, Bob, Heidi|\n",
      "|        IT|Charlie, David, E...|\n",
      "|   Finance|        Frank, Grace|\n",
      "|     Sales|   Ivan, Judy, Kevin|\n",
      "| Marketing|Mallory, Niaj, Oscar|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How to convert data from rows into comma separated single column | FOR XML PATH | STUFF\n",
    "\n",
    "query = spark.sql(\"\"\" \n",
    "                   \n",
    "SELECT Department, CONCAT_WS(', ', COLLECT_LIST(EmployeeName)) AS EmployeeNames\n",
    "FROM Employees\n",
    "GROUP BY Department\n",
    "\n",
    "                  \"\"\")\n",
    "query.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|Department|       EmployeeNames|\n",
      "+----------+--------------------+\n",
      "|     Sales|   Ivan, Judy, Kevin|\n",
      "|        HR|   Alice, Bob, Heidi|\n",
      "|   Finance|        Frank, Grace|\n",
      "| Marketing|Mallory, Niaj, Oscar|\n",
      "|        IT|Charlie, David, E...|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query3 = spark.sql(\"\"\" \n",
    "                   \n",
    "SELECT Department, CONCAT_WS(', ', COLLECT_LIST(EmployeeName)) AS EmployeeNames\n",
    "FROM (\n",
    "    SELECT Department, EmployeeName\n",
    "    FROM Employees\n",
    "    ORDER BY Department, EmployeeID\n",
    ") AS OrderedEmployees\n",
    "GROUP BY Department\n",
    "\n",
    "                  \"\"\")\n",
    "query3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------------+\n",
      "|Department|EmployeeNames             |\n",
      "+----------+--------------------------+\n",
      "|HR        |Alice, Bob, Heidi         |\n",
      "|IT        |Charlie, David, Eve, Laura|\n",
      "|Finance   |Frank, Grace              |\n",
      "|Sales     |Ivan, Judy, Kevin         |\n",
      "|Marketing |Mallory, Niaj, Oscar      |\n",
      "+----------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list, concat_ws\n",
    "\n",
    "# Group by Department and concatenate employee names into a single comma-separated string\n",
    "result_df = df.groupBy(\"Department\") \\\n",
    "    .agg(concat_ws(\", \", collect_list(\"EmployeeName\")).alias(\"EmployeeNames\"))\n",
    "\n",
    "result_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------------+\n",
      "|Department|DistinctEmployeeNames     |\n",
      "+----------+--------------------------+\n",
      "|HR        |Heidi, Bob, Alice         |\n",
      "|IT        |Laura, Eve, David, Charlie|\n",
      "|Finance   |Frank, Grace              |\n",
      "|Sales     |Kevin, Judy, Ivan         |\n",
      "|Marketing |Mallory, Oscar, Niaj      |\n",
      "+----------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set, concat_ws\n",
    "# Use collect_set to gather unique employee names into a single string\n",
    "distinct_agg_df = df.groupBy(\"Department\") \\\n",
    "    .agg(concat_ws(\", \", collect_set(\"EmployeeName\")).alias(\"DistinctEmployeeNames\"))\n",
    "\n",
    "distinct_agg_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|Department|EmployeeNames|\n",
      "+----------+-------------+\n",
      "|HR        |Alice        |\n",
      "|HR        |Bob          |\n",
      "|IT        |Charlie      |\n",
      "|IT        |David        |\n",
      "|IT        |Eve          |\n",
      "|Finance   |Frank        |\n",
      "|Finance   |Grace        |\n",
      "|HR        |Heidi        |\n",
      "|Sales     |Ivan         |\n",
      "|Sales     |Judy         |\n",
      "|Sales     |Kevin        |\n",
      "|IT        |Laura        |\n",
      "|Marketing |Mallory      |\n",
      "|Marketing |Niaj         |\n",
      "|Marketing |Oscar        |\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define custom function to concatenate names within each partition\n",
    "def aggregate_names(partition):\n",
    "    from itertools import groupby\n",
    "    from operator import itemgetter\n",
    "\n",
    "    sorted_partition = sorted(partition, key=itemgetter(2))  # Sort by Department\n",
    "    for key, group in groupby(sorted_partition, key=itemgetter(2)):\n",
    "        employees = [item[1] for item in group]\n",
    "        yield (key, \", \".join(employees))\n",
    "\n",
    "# Convert to RDD, apply custom aggregation, and convert back to DataFrame\n",
    "rdd = df.rdd.mapPartitions(aggregate_names)\n",
    "custom_agg_df = rdd.toDF([\"Department\", \"EmployeeNames\"])\n",
    "custom_agg_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
