{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H:\\\\pyspark_advanced-coding_interview'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"H:\\pyspark_advanced-coding_interview\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session with optimized settings\n",
    "spark = (\n",
    "    SparkSession.builder \n",
    "    .appName(\"OptimizedLocalSpark\") \n",
    "    .config(\"spark.driver.memory\", \"8g\")        \n",
    "    .config(\"spark.executor.memory\", \"8g\")    \n",
    "    .config(\"spark.executor.cores\", \"4\")       \n",
    "    .config(\"spark.cores.max\", \"12\")           \n",
    "    .config(\"spark.sql.shuffle.partitions\", \"28\")  \n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+----------+\n",
      "|RecordID|TaskName| StartDate|   EndDate|\n",
      "+--------+--------+----------+----------+\n",
      "|       1|  Task A|2024-01-01|2024-01-15|\n",
      "|       2|  Task B|2024-02-03|2024-02-17|\n",
      "|       3|  Task C|2024-01-10|2024-01-25|\n",
      "|       4|  Task D|2024-03-05|2024-03-19|\n",
      "|       5|  Task E|2024-03-21|2024-04-04|\n",
      "|       6|  Task F|2024-04-08|2024-04-22|\n",
      "|       7|  Task G|2024-05-12|2024-05-26|\n",
      "|       8|  Task H|2024-06-02|2024-06-16|\n",
      "|       9|  Task I|2024-07-01|2024-07-15|\n",
      "|      10|  Task J|2024-07-20|2024-08-03|\n",
      "|      11|  Task K|2024-08-09|2024-08-23|\n",
      "|      12|  Task L|2024-09-01|2024-09-15|\n",
      "|      13|  Task M|2024-10-01|2024-10-15|\n",
      "|      14|  Task N|2024-10-20|2024-11-03|\n",
      "|      15|  Task O|2024-12-05|2024-12-19|\n",
      "+--------+--------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"BiweeklyFridays\").getOrCreate()\n",
    "\n",
    "# Define schema using StructType and StructField\n",
    "schema = StructType([\n",
    "    StructField(\"RecordID\", IntegerType(), True),\n",
    "    StructField(\"TaskName\", StringType(), True),\n",
    "    StructField(\"StartDate\", DateType(), True),\n",
    "    StructField(\"EndDate\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Sample data (RecordID, TaskName, StartDate, EndDate)\n",
    "data = [\n",
    "    (1, \"Task A\", datetime.strptime(\"2024-01-01\", \"%Y-%m-%d\").date(), datetime.strptime(\"2024-01-15\", \"%Y-%m-%d\").date()),\n",
    "    (2, \"Task B\", datetime.strptime(\"2024-02-03\", \"%Y-%m-%d\").date(), datetime.strptime(\"2024-02-17\", \"%Y-%m-%d\").date()),\n",
    "    (3, \"Task C\", datetime.strptime(\"2024-01-10\", \"%Y-%m-%d\").date(), datetime.strptime(\"2024-01-25\", \"%Y-%m-%d\").date()),\n",
    "    (4, \"Task D\", datetime.strptime(\"2024-03-05\", \"%Y-%m-%d\").date(), datetime.strptime(\"2024-03-19\", \"%Y-%m-%d\").date()),\n",
    "    (5, \"Task E\", datetime.strptime(\"2024-03-21\", \"%Y-%m-%d\").date(), datetime.strptime(\"2024-04-04\", \"%Y-%m-%d\").date()),\n",
    "    (6, \"Task F\", datetime.strptime(\"2024-04-08\", \"%Y-%m-%d\").date(), datetime.strptime(\"2024-04-22\", \"%Y-%m-%d\").date()),\n",
    "    (7, \"Task G\", datetime.strptime(\"2024-05-12\", \"%Y-%m-%d\").date(), datetime.strptime(\"2024-05-26\", \"%Y-%m-%d\").date()),\n",
    "    (8, \"Task H\", datetime.strptime(\"2024-06-02\", \"%Y-%m-%d\").date(), datetime.strptime(\"2024-06-16\", \"%Y-%m-%d\").date()),\n",
    "    (9, \"Task I\", datetime.strptime(\"2024-07-01\", \"%Y-%m-%d\").date(), datetime.strptime(\"2024-07-15\", \"%Y-%m-%d\").date()),\n",
    "    (10, \"Task J\", datetime.strptime(\"2024-07-20\", \"%Y-%m-%d\").date(), datetime.strptime(\"2024-08-03\", \"%Y-%m-%d\").date()),\n",
    "    (11, \"Task K\", datetime.strptime(\"2024-08-09\", \"%Y-%m-%d\").date(), datetime.strptime(\"2024-08-23\", \"%Y-%m-%d\").date()),\n",
    "    (12, \"Task L\", datetime.strptime(\"2024-09-01\", \"%Y-%m-%d\").date(), datetime.strptime(\"2024-09-15\", \"%Y-%m-%d\").date()),\n",
    "    (13, \"Task M\", datetime.strptime(\"2024-10-01\", \"%Y-%m-%d\").date(), datetime.strptime(\"2024-10-15\", \"%Y-%m-%d\").date()),\n",
    "    (14, \"Task N\", datetime.strptime(\"2024-10-20\", \"%Y-%m-%d\").date(), datetime.strptime(\"2024-11-03\", \"%Y-%m-%d\").date()),\n",
    "    (15, \"Task O\", datetime.strptime(\"2024-12-05\", \"%Y-%m-%d\").date(), datetime.strptime(\"2024-12-19\", \"%Y-%m-%d\").date())\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"Tasks\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|BiweeklyFriday|\n",
      "+--------------+\n",
      "|    2024-01-05|\n",
      "|    2024-01-19|\n",
      "|    2024-02-02|\n",
      "|    2024-02-16|\n",
      "|    2024-03-01|\n",
      "|    2024-03-15|\n",
      "|    2024-03-29|\n",
      "|    2024-04-12|\n",
      "|    2024-04-26|\n",
      "|    2024-05-10|\n",
      "|    2024-05-24|\n",
      "|    2024-06-07|\n",
      "|    2024-06-21|\n",
      "|    2024-07-05|\n",
      "|    2024-07-19|\n",
      "|    2024-08-02|\n",
      "|    2024-08-16|\n",
      "|    2024-08-30|\n",
      "|    2024-09-13|\n",
      "|    2024-09-27|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate Biweekly Friday dates in an Year\n",
    "# Create DataFrame for the first Friday of the year\n",
    "first_friday = spark.sql(\"SELECT to_date('2024-01-05') AS BiweeklyFriday\")\n",
    "\n",
    "# Generate Biweekly Fridays Using a Loop in Spark SQL\n",
    "query = \"\"\"\n",
    "SELECT explode(array(\n",
    "    to_date('2024-01-05'),\n",
    "    date_add('2024-01-05', 14 * 1),\n",
    "    date_add('2024-01-05', 14 * 2),\n",
    "    date_add('2024-01-05', 14 * 3),\n",
    "    date_add('2024-01-05', 14 * 4),\n",
    "    date_add('2024-01-05', 14 * 5),\n",
    "    date_add('2024-01-05', 14 * 6),\n",
    "    date_add('2024-01-05', 14 * 7),\n",
    "    date_add('2024-01-05', 14 * 8),\n",
    "    date_add('2024-01-05', 14 * 9),\n",
    "    date_add('2024-01-05', 14 * 10),\n",
    "    date_add('2024-01-05', 14 * 11),\n",
    "    date_add('2024-01-05', 14 * 12),\n",
    "    date_add('2024-01-05', 14 * 13),\n",
    "    date_add('2024-01-05', 14 * 14),\n",
    "    date_add('2024-01-05', 14 * 15),\n",
    "    date_add('2024-01-05', 14 * 16),\n",
    "    date_add('2024-01-05', 14 * 17),\n",
    "    date_add('2024-01-05', 14 * 18),\n",
    "    date_add('2024-01-05', 14 * 19),\n",
    "    date_add('2024-01-05', 14 * 20),\n",
    "    date_add('2024-01-05', 14 * 21),\n",
    "    date_add('2024-01-05', 14 * 22),\n",
    "    date_add('2024-01-05', 14 * 23),\n",
    "    date_add('2024-01-05', 14 * 24),\n",
    "    date_add('2024-01-05', 14 * 25)\n",
    ")) AS BiweeklyFriday\n",
    "\"\"\"\n",
    "\n",
    "biweekly_fridays_from_loop = spark.sql(query)\n",
    "biweekly_fridays_from_loop.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|BiweeklyFriday|\n",
      "+--------------+\n",
      "|    2024-01-05|\n",
      "|    2024-01-19|\n",
      "|    2024-02-02|\n",
      "|    2024-02-16|\n",
      "|    2024-03-01|\n",
      "|    2024-03-15|\n",
      "|    2024-03-29|\n",
      "|    2024-04-12|\n",
      "|    2024-04-26|\n",
      "|    2024-05-10|\n",
      "|    2024-05-24|\n",
      "|    2024-06-07|\n",
      "|    2024-06-21|\n",
      "|    2024-07-05|\n",
      "|    2024-07-19|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a temporary SQL view\n",
    "df.createOrReplaceTempView(\"Tasks\")\n",
    "\n",
    "# SQL Query to Generate Biweekly Fridays Starting from First Friday in 2024\n",
    "query = \"\"\"\n",
    "WITH BiweeklyFridays AS (\n",
    "    SELECT date_add('2024-01-05', (row_number() OVER (ORDER BY (SELECT NULL)) - 1) * 14) AS BiweeklyFriday\n",
    "    FROM Tasks\n",
    "    LIMIT 26\n",
    ")\n",
    "SELECT BiweeklyFriday FROM BiweeklyFridays\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "biweekly_fridays_sql = spark.sql(query)\n",
    "biweekly_fridays_sql.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|BiweeklyFriday|\n",
      "+--------------+\n",
      "|    2024-01-05|\n",
      "|    2024-01-19|\n",
      "|    2024-02-02|\n",
      "|    2024-02-16|\n",
      "|    2024-03-01|\n",
      "|    2024-03-15|\n",
      "|    2024-03-29|\n",
      "|    2024-04-12|\n",
      "|    2024-04-26|\n",
      "|    2024-05-10|\n",
      "|    2024-05-24|\n",
      "|    2024-06-07|\n",
      "|    2024-06-21|\n",
      "|    2024-07-05|\n",
      "|    2024-07-19|\n",
      "|    2024-08-02|\n",
      "|    2024-08-16|\n",
      "|    2024-08-30|\n",
      "|    2024-09-13|\n",
      "|    2024-09-27|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, sequence, explode\n",
    "\n",
    "# Step 1: Generate a sequence of biweekly Fridays\n",
    "start_date = datetime.strptime(\"2024-01-05\", \"%Y-%m-%d\").date()\n",
    "end_date = datetime.strptime(\"2024-12-27\", \"%Y-%m-%d\").date()\n",
    "\n",
    "# Step 2: Create a DataFrame using the sequence\n",
    "biweekly_fridays_df = spark.sql(f\"SELECT explode(sequence(to_date('{start_date}'), to_date('{end_date}'), interval 14 days)) AS BiweeklyFriday\")\n",
    "\n",
    "# Show the result\n",
    "biweekly_fridays_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
