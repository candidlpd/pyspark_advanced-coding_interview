{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to dynamically convert rows into columns | Dynamic Pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H:\\\\pyspark_advanced-coding_interview'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"H:\\pyspark_advanced-coding_interview\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----------+\n",
      "|Year|Month    |SalesAmount|\n",
      "+----+---------+-----------+\n",
      "|2023|January  |1000.0     |\n",
      "|2023|February |1500.0     |\n",
      "|2023|March    |1800.0     |\n",
      "|2023|April    |1100.0     |\n",
      "|2023|May      |1300.0     |\n",
      "|2023|June     |950.0      |\n",
      "|2023|July     |1250.0     |\n",
      "|2023|August   |1450.0     |\n",
      "|2023|September|1150.0     |\n",
      "|2023|October  |1500.0     |\n",
      "|2023|November |1400.0     |\n",
      "|2023|December |1600.0     |\n",
      "|2022|January  |900.0      |\n",
      "|2022|February |1400.0     |\n",
      "|2022|March    |1700.0     |\n",
      "+----+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 62038)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\lpdda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"c:\\Users\\lpdda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"c:\\Users\\lpdda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"c:\\Users\\lpdda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"C:\\spark\\python\\pyspark\\accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"C:\\spark\\python\\pyspark\\accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "       ^^^^^^\n",
      "  File \"C:\\spark\\python\\pyspark\\accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\lpdda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Initialize Spark Session (if not already done)\n",
    "spark = SparkSession.builder.appName(\"DynamicPivotExample\").getOrCreate()\n",
    "\n",
    "# Define schema using StructType and StructField\n",
    "schema = StructType([\n",
    "    StructField(\"Year\", IntegerType(), True),\n",
    "    StructField(\"Month\", StringType(), True),\n",
    "    StructField(\"SalesAmount\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Sample sales data\n",
    "data = [\n",
    "    (2023, \"January\", 1000.0),\n",
    "    (2023, \"February\", 1500.0),\n",
    "    (2023, \"March\", 1800.0),\n",
    "    (2023, \"April\", 1100.0),\n",
    "    (2023, \"May\", 1300.0),\n",
    "    (2023, \"June\", 950.0),\n",
    "    (2023, \"July\", 1250.0),\n",
    "    (2023, \"August\", 1450.0),\n",
    "    (2023, \"September\", 1150.0),\n",
    "    (2023, \"October\", 1500.0),\n",
    "    (2023, \"November\", 1400.0),\n",
    "    (2023, \"December\", 1600.0),\n",
    "    (2022, \"January\", 900.0),\n",
    "    (2022, \"February\", 1400.0),\n",
    "    (2022, \"March\", 1700.0)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a temporary table\n",
    "df.createOrReplaceTempView(\"SalesData\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------+------+------+------+-----+------+------+---------+-------+--------+--------+\n",
      "|Year|January|February|March |April |May   |June |July  |August|September|October|November|December|\n",
      "+----+-------+--------+------+------+------+-----+------+------+---------+-------+--------+--------+\n",
      "|2023|1000.0 |1500.0  |1800.0|1100.0|1300.0|950.0|1250.0|1450.0|1150.0   |1500.0 |1400.0  |1600.0  |\n",
      "|2022|900.0  |1400.0  |1700.0|null  |null  |null |null  |null  |null     |null   |null    |null    |\n",
      "+----+-------+--------+------+------+------+-----+------+------+---------+-------+--------+--------+\n",
      "\n",
      "Unique Months: ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n"
     ]
    }
   ],
   "source": [
    "# Perform a static pivot using DataFrame API\n",
    "static_pivot_df = df.groupBy(\"Year\").pivot(\"Month\", [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \n",
    "                                                     \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]).sum(\"SalesAmount\")\n",
    "static_pivot_df.show(truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Collect unique months dynamically\n",
    "unique_months = [row[\"Month\"] for row in df.select(\"Month\").distinct().collect()]\n",
    "\n",
    "print(\"Unique Months:\", unique_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------+------+------+------+-----+------+------+---------+-------+--------+--------+\n",
      "|Year|January|February|March |April |May   |June |July  |August|September|October|November|December|\n",
      "+----+-------+--------+------+------+------+-----+------+------+---------+-------+--------+--------+\n",
      "|2023|1000.0 |1500.0  |1800.0|1100.0|1300.0|950.0|1250.0|1450.0|1150.0   |1500.0 |1400.0  |1600.0  |\n",
      "|2022|900.0  |1400.0  |1700.0|null  |null  |null |null  |null  |null     |null   |null    |null    |\n",
      "+----+-------+--------+------+------+------+-----+------+------+---------+-------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create dynamic SQL query\n",
    "pivot_query = f\"\"\"\n",
    "SELECT Year, {', '.join([f\"MAX(CASE WHEN Month = '{month}' THEN SalesAmount ELSE NULL END) AS `{month}`\" for month in unique_months])}\n",
    "FROM SalesData\n",
    "GROUP BY Year\n",
    "\"\"\"\n",
    "\n",
    "# Execute the dynamic pivot query\n",
    "dynamic_pivot_df = spark.sql(pivot_query)\n",
    "dynamic_pivot_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+--------+--------+-------+------+-----+------+------+--------+-------+---------+\n",
      "|Year|April |August|December|February|January|July  |June |March |May   |November|October|September|\n",
      "+----+------+------+--------+--------+-------+------+-----+------+------+--------+-------+---------+\n",
      "|2023|1100.0|1450.0|1600.0  |1500.0  |1000.0 |1250.0|950.0|1800.0|1300.0|1400.0  |1500.0 |1150.0   |\n",
      "|2022|null  |null  |null    |1400.0  |900.0  |null  |null |1700.0|null  |null    |null   |null     |\n",
      "+----+------+------+--------+--------+-------+------+-----+------+------+--------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "# Collect unique months dynamically\n",
    "months_list = df.select(\"Month\").distinct().orderBy(\"Month\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Perform dynamic pivot\n",
    "dynamic_pivot_df_api = df.groupBy(\"Year\").pivot(\"Month\", months_list).sum(\"SalesAmount\")\n",
    "dynamic_pivot_df_api.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Months: ['April', 'August', 'December', 'February', 'January', 'July', 'June', 'March', 'May', 'November', 'October', 'September']\n",
      "+----+------+------+--------+--------+-------+------+-----+------+------+--------+-------+---------+\n",
      "|Year|April |August|December|February|January|July  |June |March |May   |November|October|September|\n",
      "+----+------+------+--------+--------+-------+------+-----+------+------+--------+-------+---------+\n",
      "|2023|1100.0|1450.0|1600.0  |1500.0  |1000.0 |1250.0|950.0|1800.0|1300.0|1400.0  |1500.0 |1150.0   |\n",
      "|2022|null  |null  |null    |1400.0  |900.0  |null  |null |1700.0|null  |null    |null   |null     |\n",
      "+----+------+------+--------+--------+-------+------+-----+------+------+--------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Collect unique month names dynamically\n",
    "months = df.select(\"Month\").distinct().orderBy(\"Month\").rdd.flatMap(lambda x: x).collect()\n",
    "print(\"Unique Months:\", months)\n",
    "\n",
    "# Construct aggregation expressions dynamically\n",
    "aggregations = {month: expr(f\"MAX(CASE WHEN Month = '{month}' THEN SalesAmount ELSE NULL END)\").alias(month) for month in months}\n",
    "\n",
    "# Create a pivot-like DataFrame by aggregating each month dynamically\n",
    "df_dynamic_agg = df.groupBy(\"Year\").agg(*aggregations.values())\n",
    "df_dynamic_agg.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+--------+--------+-------+------+-----+------+------+--------+-------+---------+\n",
      "|Year|April |August|December|February|January|July  |June |March |May   |November|October|September|\n",
      "+----+------+------+--------+--------+-------+------+-----+------+------+--------+-------+---------+\n",
      "|2023|1100.0|1450.0|1600.0  |1500.0  |1000.0 |1250.0|950.0|1800.0|1300.0|1400.0  |1500.0 |1150.0   |\n",
      "|2022|null  |null  |null    |1400.0  |900.0  |null  |null |1700.0|null  |null    |null   |null     |\n",
      "+----+------+------+--------+--------+-------+------+-----+------+------+--------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Construct aggregation expressions dynamically\n",
    "aggregations = {month: expr(f\"MAX(CASE WHEN Month = '{month}' THEN SalesAmount ELSE NULL END)\").alias(month) for month in months}\n",
    "\n",
    "# Create a pivot-like DataFrame by aggregating each month dynamically\n",
    "df_dynamic_agg = df.groupBy(\"Year\").agg(*aggregations.values())\n",
    "df_dynamic_agg.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to RDD for manual pivot operation\n",
    "rdd_pivot = df.rdd.map(lambda x: (x.Year, (x.Month, x.SalesAmount))) \\\n",
    "    .groupByKey() \\\n",
    "    .map(lambda x: (x[0], dict(x[1])))\n",
    "\n",
    "# Transform back to DataFrame with dynamic columns\n",
    "pivoted_df = spark.createDataFrame(rdd_pivot.map(lambda x: (x[0], *[x[1].get(month, None) for month in months])),\n",
    "                                   schema=[\"Year\"] + months)\n",
    "\n",
    "pivoted_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+--------+--------+-------+------+-----+------+------+--------+-------+---------+\n",
      "|Year|April |August|December|February|January|July  |June |March |May   |November|October|September|\n",
      "+----+------+------+--------+--------+-------+------+-----+------+------+--------+-------+---------+\n",
      "|2023|1100.0|1450.0|1600.0  |1500.0  |1000.0 |1250.0|950.0|1800.0|1300.0|1400.0  |1500.0 |1150.0   |\n",
      "|2022|null  |null  |null    |1400.0  |900.0  |null  |null |1700.0|null  |null    |null   |null     |\n",
      "+----+------+------+--------+--------+-------+------+-----+------+------+--------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "months = df.select(\"Month\").distinct().orderBy(\"Month\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Create dynamic SQL query using the collected unique months\n",
    "pivot_query = f\"\"\"\n",
    "SELECT Year, {', '.join([f\"MAX(CASE WHEN Month = '{month}' THEN SalesAmount ELSE NULL END) AS `{month}`\" for month in months])}\n",
    "FROM SalesData\n",
    "GROUP BY Year\n",
    "\"\"\"\n",
    "\n",
    "# Execute the dynamic pivot query using Spark SQL\n",
    "df.createOrReplaceTempView(\"SalesData\")\n",
    "dynamic_sql_pivot = spark.sql(pivot_query)\n",
    "dynamic_sql_pivot.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
