{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H:\\\\pyspark_advanced-coding_interview'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"H:\\pyspark_advanced-coding_interview\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamically Pass Values to IN clause | Parametrize Values | String_Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------+---------+----------+---------+-----+----------------+--------+\n",
      "|id |name   |department|salary|country  |hire_date |role     |grade|years_experience|status  |\n",
      "+---+-------+----------+------+---------+----------+---------+-----+----------------+--------+\n",
      "|1  |Alice  |HR        |5000  |USA      |2024-10-01|Manager  |A    |5               |Active  |\n",
      "|2  |Bob    |Finance   |6000  |Canada   |2024-10-03|Analyst  |B    |3               |Active  |\n",
      "|3  |Charlie|IT        |7000  |UK       |2024-10-05|Developer|A    |7               |Inactive|\n",
      "|4  |David  |HR        |5500  |USA      |2024-10-07|Recruiter|B    |2               |Active  |\n",
      "|5  |Eva    |Marketing |6500  |Germany  |2024-10-09|Executive|A    |4               |Inactive|\n",
      "|6  |Frank  |IT        |8000  |France   |2024-10-11|Developer|A    |6               |Active  |\n",
      "|7  |Grace  |Finance   |7500  |USA      |2024-10-13|Analyst  |B    |5               |Inactive|\n",
      "|8  |Henry  |IT        |7200  |India    |2024-10-15|Tester   |A    |3               |Active  |\n",
      "|9  |Ivy    |Marketing |5800  |Japan    |2024-10-17|Executive|A    |4               |Active  |\n",
      "|10 |Jack   |HR        |9000  |Australia|2024-10-19|Manager  |B    |6               |Inactive|\n",
      "|11 |Kim    |Finance   |6100  |USA      |2024-10-21|Analyst  |B    |3               |Active  |\n",
      "|12 |Liam   |IT        |8100  |Canada   |2024-10-23|Developer|A    |7               |Inactive|\n",
      "|13 |Mia    |Marketing |6700  |UK       |2024-10-25|Executive|B    |4               |Active  |\n",
      "|14 |Noah   |HR        |5600  |France   |2024-10-27|Recruiter|A    |2               |Inactive|\n",
      "|15 |Olivia |Finance   |5800  |Germany  |2024-10-29|Analyst  |A    |3               |Active  |\n",
      "+---+-------+----------+------+---------+----------+---------+-----+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"DynamicINClause\").getOrCreate()\n",
    "\n",
    "# Sample Data: Data with 10 columns and 15 rows\n",
    "data = [\n",
    "    (1, \"Alice\", \"HR\", 5000, \"USA\", \"2024-10-01\", \"Manager\", \"A\", 5, \"Active\"),\n",
    "    (2, \"Bob\", \"Finance\", 6000, \"Canada\", \"2024-10-03\", \"Analyst\", \"B\", 3, \"Active\"),\n",
    "    (3, \"Charlie\", \"IT\", 7000, \"UK\", \"2024-10-05\", \"Developer\", \"A\", 7, \"Inactive\"),\n",
    "    (4, \"David\", \"HR\", 5500, \"USA\", \"2024-10-07\", \"Recruiter\", \"B\", 2, \"Active\"),\n",
    "    (5, \"Eva\", \"Marketing\", 6500, \"Germany\", \"2024-10-09\", \"Executive\", \"A\", 4, \"Inactive\"),\n",
    "    (6, \"Frank\", \"IT\", 8000, \"France\", \"2024-10-11\", \"Developer\", \"A\", 6, \"Active\"),\n",
    "    (7, \"Grace\", \"Finance\", 7500, \"USA\", \"2024-10-13\", \"Analyst\", \"B\", 5, \"Inactive\"),\n",
    "    (8, \"Henry\", \"IT\", 7200, \"India\", \"2024-10-15\", \"Tester\", \"A\", 3, \"Active\"),\n",
    "    (9, \"Ivy\", \"Marketing\", 5800, \"Japan\", \"2024-10-17\", \"Executive\", \"A\", 4, \"Active\"),\n",
    "    (10, \"Jack\", \"HR\", 9000, \"Australia\", \"2024-10-19\", \"Manager\", \"B\", 6, \"Inactive\"),\n",
    "    (11, \"Kim\", \"Finance\", 6100, \"USA\", \"2024-10-21\", \"Analyst\", \"B\", 3, \"Active\"),\n",
    "    (12, \"Liam\", \"IT\", 8100, \"Canada\", \"2024-10-23\", \"Developer\", \"A\", 7, \"Inactive\"),\n",
    "    (13, \"Mia\", \"Marketing\", 6700, \"UK\", \"2024-10-25\", \"Executive\", \"B\", 4, \"Active\"),\n",
    "    (14, \"Noah\", \"HR\", 5600, \"France\", \"2024-10-27\", \"Recruiter\", \"A\", 2, \"Inactive\"),\n",
    "    (15, \"Olivia\", \"Finance\", 5800, \"Germany\", \"2024-10-29\", \"Analyst\", \"A\", 3, \"Active\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"department\", \"salary\", \"country\", \"hire_date\", \"role\", \"grade\", \"years_experience\", \"status\"])\n",
    "df.cache()\n",
    "# Create a Temporary View for Spark SQL\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "# Show the original DataFrame\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+------+-------+----------+---------+-----+----------------+--------+\n",
      "|id |name  |department|salary|country|hire_date |role     |grade|years_experience|status  |\n",
      "+---+------+----------+------+-------+----------+---------+-----+----------------+--------+\n",
      "|1  |Alice |HR        |5000  |USA    |2024-10-01|Manager  |A    |5               |Active  |\n",
      "|2  |Bob   |Finance   |6000  |Canada |2024-10-03|Analyst  |B    |3               |Active  |\n",
      "|4  |David |HR        |5500  |USA    |2024-10-07|Recruiter|B    |2               |Active  |\n",
      "|5  |Eva   |Marketing |6500  |Germany|2024-10-09|Executive|A    |4               |Inactive|\n",
      "|7  |Grace |Finance   |7500  |USA    |2024-10-13|Analyst  |B    |5               |Inactive|\n",
      "|11 |Kim   |Finance   |6100  |USA    |2024-10-21|Analyst  |B    |3               |Active  |\n",
      "|12 |Liam  |IT        |8100  |Canada |2024-10-23|Developer|A    |7               |Inactive|\n",
      "|15 |Olivia|Finance   |5800  |Germany|2024-10-29|Analyst  |A    |3               |Active  |\n",
      "+---+------+----------+------+-------+----------+---------+-----+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define the dynamic filter values\n",
    "countries_list = [\"USA\", \"Canada\", \"Germany\"]\n",
    "\n",
    "# Use `isin` to filter DataFrame based on the list\n",
    "df_filtered = df.filter(col(\"country\").isin(countries_list))\n",
    "\n",
    "# Show the filtered DataFrame\n",
    "df_filtered.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+------+-------+----------+---------+-----+----------------+--------+\n",
      "|id |name  |department|salary|country|hire_date |role     |grade|years_experience|status  |\n",
      "+---+------+----------+------+-------+----------+---------+-----+----------------+--------+\n",
      "|1  |Alice |HR        |5000  |USA    |2024-10-01|Manager  |A    |5               |Active  |\n",
      "|2  |Bob   |Finance   |6000  |Canada |2024-10-03|Analyst  |B    |3               |Active  |\n",
      "|4  |David |HR        |5500  |USA    |2024-10-07|Recruiter|B    |2               |Active  |\n",
      "|5  |Eva   |Marketing |6500  |Germany|2024-10-09|Executive|A    |4               |Inactive|\n",
      "|7  |Grace |Finance   |7500  |USA    |2024-10-13|Analyst  |B    |5               |Inactive|\n",
      "|11 |Kim   |Finance   |6100  |USA    |2024-10-21|Analyst  |B    |3               |Active  |\n",
      "|12 |Liam  |IT        |8100  |Canada |2024-10-23|Developer|A    |7               |Inactive|\n",
      "|15 |Olivia|Finance   |5800  |Germany|2024-10-29|Analyst  |A    |3               |Active  |\n",
      "+---+------+----------+------+-------+----------+---------+-----+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the countries you want to filter dynamically\n",
    "countries_list = [\"USA\", \"Canada\", \"Germany\"]\n",
    "\n",
    "# Convert list to a comma-separated string\n",
    "countries_str = \"'\" + \"','\".join(countries_list) + \"'\"\n",
    "\n",
    "# Dynamically build the SQL query\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM employees\n",
    "WHERE country IN ({countries_str})\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "result = spark.sql(query)\n",
    "result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+------+-------+----------+---------+-----+----------------+--------+\n",
      "| id|  name|department|salary|country| hire_date|     role|grade|years_experience|  status|\n",
      "+---+------+----------+------+-------+----------+---------+-----+----------------+--------+\n",
      "|  1| Alice|        HR|  5000|    USA|2024-10-01|  Manager|    A|               5|  Active|\n",
      "|  2|   Bob|   Finance|  6000| Canada|2024-10-03|  Analyst|    B|               3|  Active|\n",
      "|  4| David|        HR|  5500|    USA|2024-10-07|Recruiter|    B|               2|  Active|\n",
      "|  5|   Eva| Marketing|  6500|Germany|2024-10-09|Executive|    A|               4|Inactive|\n",
      "|  7| Grace|   Finance|  7500|    USA|2024-10-13|  Analyst|    B|               5|Inactive|\n",
      "| 11|   Kim|   Finance|  6100|    USA|2024-10-21|  Analyst|    B|               3|  Active|\n",
      "| 12|  Liam|        IT|  8100| Canada|2024-10-23|Developer|    A|               7|Inactive|\n",
      "| 15|Olivia|   Finance|  5800|Germany|2024-10-29|  Analyst|    A|               3|  Active|\n",
      "+---+------+----------+------+-------+----------+---------+-----+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res2 = spark.sql(\"\"\" \n",
    "WITH country_list AS (\n",
    "    SELECT explode(split('USA,Canada,Germany', ',')) AS country\n",
    ")\n",
    "SELECT *\n",
    "FROM employees\n",
    "WHERE country IN (SELECT country FROM country_list);\n",
    "               \n",
    "                 \n",
    "                 \"\"\")\n",
    "\n",
    "res2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      country_list|\n",
      "+------------------+\n",
      "|USA,Canada,Germany|\n",
      "+------------------+\n",
      "\n",
      "+---+------+----------+------+-------+----------+---------+-----+----------------+--------+-------+\n",
      "|id |name  |department|salary|country|hire_date |role     |grade|years_experience|status  |country|\n",
      "+---+------+----------+------+-------+----------+---------+-----+----------------+--------+-------+\n",
      "|11 |Kim   |Finance   |6100  |USA    |2024-10-21|Analyst  |B    |3               |Active  |USA    |\n",
      "|7  |Grace |Finance   |7500  |USA    |2024-10-13|Analyst  |B    |5               |Inactive|USA    |\n",
      "|4  |David |HR        |5500  |USA    |2024-10-07|Recruiter|B    |2               |Active  |USA    |\n",
      "|1  |Alice |HR        |5000  |USA    |2024-10-01|Manager  |A    |5               |Active  |USA    |\n",
      "|12 |Liam  |IT        |8100  |Canada |2024-10-23|Developer|A    |7               |Inactive|Canada |\n",
      "|2  |Bob   |Finance   |6000  |Canada |2024-10-03|Analyst  |B    |3               |Active  |Canada |\n",
      "|15 |Olivia|Finance   |5800  |Germany|2024-10-29|Analyst  |A    |3               |Active  |Germany|\n",
      "|5  |Eva   |Marketing |6500  |Germany|2024-10-09|Executive|A    |4               |Inactive|Germany|\n",
      "+---+------+----------+------+-------+----------+---------+-----+----------------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "\n",
    "# Create DataFrame from a comma-separated string\n",
    "countries_str = \"USA,Canada,Germany\"\n",
    "countries_df = spark.createDataFrame([(countries_str,)], [\"country_list\"])\n",
    "\n",
    "countries_df.show()\n",
    "\n",
    "# Use split and explode to convert string to rows\n",
    "split_countries = countries_df.select(explode(split(col(\"country_list\"), \",\")).alias(\"country\"))\n",
    "\n",
    "# Join with the original DataFrame to get matching records\n",
    "df_joined = df.join(split_countries, df.country == split_countries.country, \"inner\")\n",
    "\n",
    "# Show the result\n",
    "df_joined.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
