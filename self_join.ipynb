{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H:\\\\pyspark_advanced-coding_interview'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"H:\\pyspark_advanced-coding_interview\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+------+\n",
      "|EmployeeID|EmployeeName|Department|Salary|\n",
      "+----------+------------+----------+------+\n",
      "|         1|       Alice| Marketing| 60000|\n",
      "|         2|         Bob|     Sales| 50000|\n",
      "|         3|     Charlie| Marketing| 60000|\n",
      "|         4|       David|     Sales| 70000|\n",
      "|         5|         Eve| Marketing| 65000|\n",
      "|         6|       Frank|     Sales| 50000|\n",
      "+----------+------------+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 61068)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\lpdda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"c:\\Users\\lpdda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"c:\\Users\\lpdda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"c:\\Users\\lpdda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"C:\\spark\\python\\pyspark\\accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"C:\\spark\\python\\pyspark\\accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "       ^^^^^^\n",
      "  File \"C:\\spark\\python\\pyspark\\accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\lpdda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"SelfJoinExamples\").getOrCreate()\n",
    "\n",
    "# Sample data for Spark DataFrame\n",
    "data = [\n",
    "    (1, 'Alice', 'Marketing', 60000),\n",
    "    (2, 'Bob', 'Sales', 50000),\n",
    "    (3, 'Charlie', 'Marketing', 60000),\n",
    "    (4, 'David', 'Sales', 70000),\n",
    "    (5, 'Eve', 'Marketing', 65000),\n",
    "    (6, 'Frank', 'Sales', 50000)\n",
    "]\n",
    "\n",
    "columns = [\"EmployeeID\", \"EmployeeName\", \"Department\", \"Salary\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Create a temporary table to use in SQL\n",
    "df.createOrReplaceTempView(\"employee_table\")\n",
    "\n",
    "# Show the table\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Employees Earning the Same Salary Within the Same Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+------+---+\n",
      "|EmployeeID|EmployeeName|Department|Salary|rnk|\n",
      "+----------+------------+----------+------+---+\n",
      "|         1|       Alice| Marketing| 60000|  1|\n",
      "|         2|         Bob|     Sales| 50000|  1|\n",
      "|         3|     Charlie| Marketing| 60000|  1|\n",
      "|         6|       Frank|     Sales| 50000|  1|\n",
      "+----------+------------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find Employees Earning the Same Salary Within the Same Department\n",
    "df.createOrReplaceTempView(\"employee_table\")\n",
    " \n",
    "res = spark.sql(\"\"\"\n",
    "WITH cte_e AS (\n",
    "    SELECT \n",
    "        Department, \n",
    "        dense_rank() OVER (PARTITION BY Department ORDER BY Salary) AS rnk,\n",
    "        EmployeeID \n",
    "    FROM employee_table\n",
    ")\n",
    "SELECT e.*, d.rnk\n",
    "FROM employee_table e\n",
    "INNER JOIN cte_e d \n",
    "ON e.EmployeeID = d.EmployeeID \n",
    "WHERE d.rnk = 1\n",
    "\"\"\")\n",
    "\n",
    "# Show the result\n",
    "res.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------+\n",
      "|Employee1|Employee2|Salary|\n",
      "+---------+---------+------+\n",
      "|    Alice|  Charlie| 60000|\n",
      "|  Charlie|    Alice| 60000|\n",
      "|      Bob|    Frank| 50000|\n",
      "|    Frank|      Bob| 50000|\n",
      "+---------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res2 = spark.sql(\"\"\"\n",
    "\n",
    "  SELECT e1.EmployeeName AS Employee1, e2.EmployeeName AS Employee2, e1.Salary\n",
    "FROM employee_table e1\n",
    "join employee_table e2\n",
    "ON e1.Department = e2.Department AND e1.Salary = e2.Salary AND e1.EmployeeID <> e2.EmployeeID;\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# Show the result\n",
    "res2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------+-----------+------+\n",
      "|Employee1|Department1|Employee2|Department2|Salary|\n",
      "+---------+-----------+---------+-----------+------+\n",
      "|      Bob|      Sales|    Frank|      Sales| 50000|\n",
      "|    Alice|  Marketing|  Charlie|  Marketing| 60000|\n",
      "+---------+-----------+---------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Detect Employees Who Have the Same Salary in Different Departments\n",
    "\n",
    "res7 = spark.sql(\"\"\"                     \n",
    "\n",
    "SELECT e1.EmployeeName AS Employee1, e1.Department AS Department1, \n",
    "       e2.EmployeeName AS Employee2, e2.Department AS Department2, e1.Salary\n",
    "FROM employee_table e1\n",
    "JOIN employee_table e2 \n",
    "ON e1.Salary = e2.Salary AND e1.EmployeeID < e2.EmployeeID;               \n",
    "                 \n",
    "                 \"\"\")\n",
    "res7.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Employees Who Earn More Than Others in the Same Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+----------------+\n",
      "|HigherEarner|LowerEarner|SalaryDifference|\n",
      "+------------+-----------+----------------+\n",
      "|         Eve|      Alice|            5000|\n",
      "|         Eve|    Charlie|            5000|\n",
      "|       David|        Bob|           20000|\n",
      "|       David|      Frank|           20000|\n",
      "+------------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify Employees Who Earn More Than Others in the Same Department\n",
    "res3 = spark.sql(\"\"\"\n",
    "SELECT e1.EmployeeName AS HigherEarner, e2.EmployeeName AS LowerEarner, e1.Salary - e2.Salary AS SalaryDifference\n",
    "FROM employee_table e1\n",
    "JOIN employee_table e2\n",
    "ON e1.Department = e2.Department AND e1.Salary > e2.Salary;\n",
    "\"\"\")\n",
    "res3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+------+---------+\n",
      "|EmployeeID|EmployeeName| Department|Salary|ManagerID|\n",
      "+----------+------------+-----------+------+---------+\n",
      "|         1|       Alice|      Sales| 50000|        3|\n",
      "|         2|         Bob|      Sales| 60000|        3|\n",
      "|         3|     Charlie|      Sales| 90000|     null|\n",
      "|         4|       David|Engineering| 75000|        6|\n",
      "|         5|         Eve|Engineering| 80000|        6|\n",
      "|         6|       Frank|Engineering|100000|     null|\n",
      "+----------+------------+-----------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"RealWorldSelfJoinExamples\").getOrCreate()\n",
    "\n",
    "# Sample data for Spark DataFrame\n",
    "data1 = [\n",
    "    (1, 'Alice', 'Sales', 50000, 3),\n",
    "    (2, 'Bob', 'Sales', 60000, 3),\n",
    "    (3, 'Charlie', 'Sales', 90000, None), # Charlie is the manager\n",
    "    (4, 'David', 'Engineering', 75000, 6),\n",
    "    (5, 'Eve', 'Engineering', 80000, 6),\n",
    "    (6, 'Frank', 'Engineering', 100000, None), # Frank is the manager\n",
    "]\n",
    "\n",
    "columns1 = [\"EmployeeID\", \"EmployeeName\", \"Department\", \"Salary\", \"ManagerID\"]\n",
    "\n",
    "# Create DataFrame and Table\n",
    "df1 = spark.createDataFrame(data1, columns1)\n",
    "df1.createOrReplaceTempView(\"employee_manager\")\n",
    "df1.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Employees and Their Managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|Employee|Manager|\n",
      "+--------+-------+\n",
      "|   Alice|Charlie|\n",
      "|     Bob|Charlie|\n",
      "| Charlie|   null|\n",
      "|   David|  Frank|\n",
      "|     Eve|  Frank|\n",
      "|   Frank|   null|\n",
      "+--------+-------+\n",
      "\n",
      "+--------+-------+\n",
      "|Employee|Manager|\n",
      "+--------+-------+\n",
      "|   Alice|Charlie|\n",
      "|     Bob|Charlie|\n",
      "|   David|  Frank|\n",
      "|     Eve|  Frank|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Identify Employees and Their Managers\n",
    "res5 = spark.sql(\"\"\"                       \n",
    "SELECT e1.EmployeeName AS Employee, e2.EmployeeName AS Manager\n",
    "FROM employee_manager e1\n",
    "LEFT JOIN employee_manager e2 ON e1.ManagerID = e2.EmployeeID;               \n",
    "                 \"\"\")\n",
    "\n",
    "res5.show()\n",
    "\n",
    "res6 = spark.sql(\"\"\"                       \n",
    "SELECT e1.EmployeeName AS Employee, e2.EmployeeName AS Manager\n",
    "FROM employee_manager e1\n",
    "JOIN employee_manager e2 ON e1.ManagerID = e2.EmployeeID;               \n",
    "                 \"\"\")\n",
    "res6.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare and Identify Employees Who Earn More Than Their Managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+-------+-------------+\n",
      "|Employee|EmployeeSalary|Manager|ManagerSalary|\n",
      "+--------+--------------+-------+-------------+\n",
      "+--------+--------------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res11= spark.sql(\"\"\" \n",
    "                 \n",
    " SELECT e1.EmployeeName AS Employee, e1.Salary AS EmployeeSalary, \n",
    "       e2.EmployeeName AS Manager, e2.Salary AS ManagerSalary\n",
    "FROM employee_manager e1\n",
    "JOIN employee_manager e2 \n",
    "ON e1.ManagerID = e2.EmployeeID \n",
    "WHERE e1.Salary > e2.Salary;\n",
    "                \n",
    "                 \n",
    "                 \"\"\")\n",
    "res11.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Consecutive Dates for an Employeeâ€™s Work Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+\n",
      "|EmployeeID|EmployeeName|  WorkDate|\n",
      "+----------+------------+----------+\n",
      "|         1|       Alice|2024-10-20|\n",
      "|         1|       Alice|2024-10-21|\n",
      "|         1|       Alice|2024-10-23|\n",
      "|         2|         Bob|2024-10-20|\n",
      "|         2|         Bob|2024-10-22|\n",
      "|         2|         Bob|2024-10-23|\n",
      "+----------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data for employee work logs\n",
    "data3 = [\n",
    "    (1, 'Alice', '2024-10-20'),\n",
    "    (1, 'Alice', '2024-10-21'),\n",
    "    (1, 'Alice', '2024-10-23'),\n",
    "    (2, 'Bob', '2024-10-20'),\n",
    "    (2, 'Bob', '2024-10-22'),\n",
    "    (2, 'Bob', '2024-10-23'),\n",
    "]\n",
    "\n",
    "columns3 = [\"EmployeeID\", \"EmployeeName\", \"WorkDate\"]\n",
    "df3 = spark.createDataFrame(data3, columns3)\n",
    "df3.createOrReplaceTempView(\"work_logs\")\n",
    "df3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------+\n",
      "|EmployeeName|     Date1|     Date2|\n",
      "+------------+----------+----------+\n",
      "|       Alice|2024-10-20|2024-10-21|\n",
      "|         Bob|2024-10-22|2024-10-23|\n",
      "+------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res10 = spark.sql(\"\"\" \n",
    "   SELECT e1.EmployeeName, e1.WorkDate AS Date1, e2.WorkDate AS Date2\n",
    "FROM work_logs e1\n",
    "JOIN work_logs e2 \n",
    "ON e1.EmployeeID = e2.EmployeeID AND DATE_ADD(e1.WorkDate, 1) = e2.WorkDate;\n",
    "            \n",
    "                  \"\"\")\n",
    "\n",
    "res10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
