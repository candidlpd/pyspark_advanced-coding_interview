{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H:\\pyspark_advanced-coding_interview\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"H:\\pyspark_advanced-coding_interview\")\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session with optimized settings\n",
    "spark = (\n",
    "    SparkSession.builder \n",
    "    .appName(\"OptimizedLocalSpark\") \n",
    "    .config(\"spark.driver.memory\", \"8g\")        \n",
    "    .config(\"spark.executor.memory\", \"8g\")    \n",
    "    .config(\"spark.executor.cores\", \"4\")       \n",
    "    .config(\"spark.cores.max\", \"12\")           \n",
    "    .config(\"spark.sql.shuffle.partitions\", \"28\")  \n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+------+-----------+-------------+----------+--------+-----+\n",
      "|emp_id|name   |department|salary|designation|location     |experience|emp_code|grade|\n",
      "+------+-------+----------+------+-----------+-------------+----------+--------+-----+\n",
      "|1     |Alice  |HR        |4500  |Manager    |New York     |5         |1001    |A    |\n",
      "|2     |Bob    |Finance   |5500  |Analyst    |Los Angeles  |7         |1002    |B    |\n",
      "|3     |Charlie|IT        |7000  |Developer  |San Francisco|4         |1003    |A    |\n",
      "|4     |David  |HR        |4800  |Recruiter  |Seattle      |3         |1004    |C    |\n",
      "|5     |Eva    |Marketing |6000  |Executive  |Austin       |6         |1005    |B    |\n",
      "|6     |Frank  |Finance   |6200  |Consultant |Chicago      |5         |1006    |A    |\n",
      "|7     |Grace  |IT        |7200  |Architect  |Houston      |8         |1007    |A    |\n",
      "|8     |Henry  |HR        |4600  |Manager    |Miami        |5         |1008    |B    |\n",
      "|9     |Ivy    |Marketing |6100  |Executive  |Dallas       |4         |1009    |C    |\n",
      "|10    |Jack   |Finance   |5800  |Analyst    |Phoenix      |7         |1010    |A    |\n",
      "+------+-------+----------+------+-----------+-------------+----------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ComplexDataSwapColumns\").getOrCreate()\n",
    "\n",
    "# Sample Data: Employee information with columns to swap\n",
    "data = [\n",
    "    (1, \"Alice\", \"HR\", 4500, \"Manager\", \"New York\", 5, 1001, \"A\"),\n",
    "    (2, \"Bob\", \"Finance\", 5500, \"Analyst\", \"Los Angeles\", 7, 1002, \"B\"),\n",
    "    (3, \"Charlie\", \"IT\", 7000, \"Developer\", \"San Francisco\", 4, 1003, \"A\"),\n",
    "    (4, \"David\", \"HR\", 4800, \"Recruiter\", \"Seattle\", 3, 1004, \"C\"),\n",
    "    (5, \"Eva\", \"Marketing\", 6000, \"Executive\", \"Austin\", 6, 1005, \"B\"),\n",
    "    (6, \"Frank\", \"Finance\", 6200, \"Consultant\", \"Chicago\", 5, 1006, \"A\"),\n",
    "    (7, \"Grace\", \"IT\", 7200, \"Architect\", \"Houston\", 8, 1007, \"A\"),\n",
    "    (8, \"Henry\", \"HR\", 4600, \"Manager\", \"Miami\", 5, 1008, \"B\"),\n",
    "    (9, \"Ivy\", \"Marketing\", 6100, \"Executive\", \"Dallas\", 4, 1009, \"C\"),\n",
    "    (10, \"Jack\", \"Finance\", 5800, \"Analyst\", \"Phoenix\", 7, 1010, \"A\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"emp_id\", \"name\", \"department\", \"salary\", \"designation\", \"location\", \"experience\", \"emp_code\", \"grade\"])\n",
    "\n",
    "# Create a Temporary View for Spark SQL\n",
    "df.createOrReplaceTempView(\"employee_table\")\n",
    "\n",
    "# Show the Original DataFrame\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Swaps values between the salary and experience columns based on a condition (e.g., when grade is A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------+--------------+-----------+-------------+--------+-----+\n",
      "|emp_id|   name|department|new_salary|new_experience|designation|     location|emp_code|grade|\n",
      "+------+-------+----------+----------+--------------+-----------+-------------+--------+-----+\n",
      "|     1|  Alice|        HR|      4500|             5|    Manager|     New York|    1001|    A|\n",
      "|     2|    Bob|   Finance|         7|          5500|    Analyst|  Los Angeles|    1002|    B|\n",
      "|     3|Charlie|        IT|      7000|             4|  Developer|San Francisco|    1003|    A|\n",
      "|     4|  David|        HR|         3|          4800|  Recruiter|      Seattle|    1004|    C|\n",
      "|     5|    Eva| Marketing|         6|          6000|  Executive|       Austin|    1005|    B|\n",
      "|     6|  Frank|   Finance|      6200|             5| Consultant|      Chicago|    1006|    A|\n",
      "|     7|  Grace|        IT|      7200|             8|  Architect|      Houston|    1007|    A|\n",
      "|     8|  Henry|        HR|         5|          4600|    Manager|        Miami|    1008|    B|\n",
      "|     9|    Ivy| Marketing|         4|          6100|  Executive|       Dallas|    1009|    C|\n",
      "|    10|   Jack|   Finance|      5800|             7|    Analyst|      Phoenix|    1010|    A|\n",
      "+------+-------+----------+----------+--------------+-----------+-------------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = spark.sql(\"\"\" \n",
    "                \n",
    "SELECT emp_id, name, department, \n",
    "       CASE WHEN grade = 'A' THEN salary ELSE experience END AS new_salary,\n",
    "       CASE WHEN grade = 'A' THEN experience ELSE salary END AS new_experience,\n",
    "       designation, location, emp_code, grade\n",
    "FROM employee_table;\n",
    "\n",
    "              \n",
    "                \"\"\")\n",
    "res.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Swap Without Condition    Simply swaps the values of salary and experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------+--------------+-----------+-------------+--------+-----+\n",
      "|emp_id|   name|department|new_salary|new_experience|designation|     location|emp_code|grade|\n",
      "+------+-------+----------+----------+--------------+-----------+-------------+--------+-----+\n",
      "|     1|  Alice|        HR|         5|          4500|    Manager|     New York|    1001|    A|\n",
      "|     2|    Bob|   Finance|         7|          5500|    Analyst|  Los Angeles|    1002|    B|\n",
      "|     3|Charlie|        IT|         4|          7000|  Developer|San Francisco|    1003|    A|\n",
      "|     4|  David|        HR|         3|          4800|  Recruiter|      Seattle|    1004|    C|\n",
      "|     5|    Eva| Marketing|         6|          6000|  Executive|       Austin|    1005|    B|\n",
      "|     6|  Frank|   Finance|         5|          6200| Consultant|      Chicago|    1006|    A|\n",
      "|     7|  Grace|        IT|         8|          7200|  Architect|      Houston|    1007|    A|\n",
      "|     8|  Henry|        HR|         5|          4600|    Manager|        Miami|    1008|    B|\n",
      "|     9|    Ivy| Marketing|         4|          6100|  Executive|       Dallas|    1009|    C|\n",
      "|    10|   Jack|   Finance|         7|          5800|    Analyst|      Phoenix|    1010|    A|\n",
      "+------+-------+----------+----------+--------------+-----------+-------------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res1 = spark.sql(\"\"\" \n",
    "                \n",
    "SELECT emp_id, name, department, \n",
    "       experience AS new_salary,\n",
    "       salary AS new_experience,\n",
    "       designation, location, emp_code, grade\n",
    "FROM employee_table;\n",
    "\n",
    "\n",
    "              \n",
    "                \"\"\")\n",
    "res1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+------+-----------+-------------+----------+--------+-----+----------+--------------+\n",
      "|emp_id|name   |department|salary|designation|location     |experience|emp_code|grade|new_salary|new_experience|\n",
      "+------+-------+----------+------+-----------+-------------+----------+--------+-----+----------+--------------+\n",
      "|1     |Alice  |HR        |4500  |Manager    |New York     |5         |1001    |A    |5         |4500          |\n",
      "|2     |Bob    |Finance   |5500  |Analyst    |Los Angeles  |7         |1002    |B    |5500      |7             |\n",
      "|3     |Charlie|IT        |7000  |Developer  |San Francisco|4         |1003    |A    |4         |7000          |\n",
      "|4     |David  |HR        |4800  |Recruiter  |Seattle      |3         |1004    |C    |4800      |3             |\n",
      "|5     |Eva    |Marketing |6000  |Executive  |Austin       |6         |1005    |B    |6000      |6             |\n",
      "|6     |Frank  |Finance   |6200  |Consultant |Chicago      |5         |1006    |A    |5         |6200          |\n",
      "|7     |Grace  |IT        |7200  |Architect  |Houston      |8         |1007    |A    |8         |7200          |\n",
      "|8     |Henry  |HR        |4600  |Manager    |Miami        |5         |1008    |B    |4600      |5             |\n",
      "|9     |Ivy    |Marketing |6100  |Executive  |Dallas       |4         |1009    |C    |6100      |4             |\n",
      "|10    |Jack   |Finance   |5800  |Analyst    |Phoenix      |7         |1010    |A    |7         |5800          |\n",
      "+------+-------+----------+------+-----------+-------------+----------+--------+-----+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Swap columns based on a condition (e.g., if the grade is 'A')\n",
    "df_swapped = df.withColumn(\"new_salary\", when(col(\"grade\") == \"A\", col(\"experience\")).otherwise(col(\"salary\"))) \\\n",
    "               .withColumn(\"new_experience\", when(col(\"grade\") == \"A\", col(\"salary\")).otherwise(col(\"experience\")))\n",
    "\n",
    "# Show the result\n",
    "df_swapped.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+------+-----------+-------------+----------+--------+-----+----------+--------------+\n",
      "|emp_id|name   |department|salary|designation|location     |experience|emp_code|grade|new_salary|new_experience|\n",
      "+------+-------+----------+------+-----------+-------------+----------+--------+-----+----------+--------------+\n",
      "|1     |Alice  |HR        |4500  |Manager    |New York     |5         |1001    |A    |5         |4500          |\n",
      "|2     |Bob    |Finance   |5500  |Analyst    |Los Angeles  |7         |1002    |B    |7         |5500          |\n",
      "|3     |Charlie|IT        |7000  |Developer  |San Francisco|4         |1003    |A    |4         |7000          |\n",
      "|4     |David  |HR        |4800  |Recruiter  |Seattle      |3         |1004    |C    |3         |4800          |\n",
      "|5     |Eva    |Marketing |6000  |Executive  |Austin       |6         |1005    |B    |6         |6000          |\n",
      "|6     |Frank  |Finance   |6200  |Consultant |Chicago      |5         |1006    |A    |5         |6200          |\n",
      "|7     |Grace  |IT        |7200  |Architect  |Houston      |8         |1007    |A    |8         |7200          |\n",
      "|8     |Henry  |HR        |4600  |Manager    |Miami        |5         |1008    |B    |5         |4600          |\n",
      "|9     |Ivy    |Marketing |6100  |Executive  |Dallas       |4         |1009    |C    |4         |6100          |\n",
      "|10    |Jack   |Finance   |5800  |Analyst    |Phoenix      |7         |1010    |A    |7         |5800          |\n",
      "+------+-------+----------+------+-----------+-------------+----------+--------+-----+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Swap columns directly\n",
    "df_simple_swap = df.withColumn(\"new_salary\", col(\"experience\")) \\\n",
    "                   .withColumn(\"new_experience\", col(\"salary\"))\n",
    "\n",
    "# Show the result\n",
    "df_simple_swap.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Swap with Multiple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------+--------------+---------------+------------+--------+-----+\n",
      "|emp_id|   name|department|new_salary|new_experience|new_designation|new_location|emp_code|grade|\n",
      "+------+-------+----------+----------+--------------+---------------+------------+--------+-----+\n",
      "|     1|  Alice|        HR|         5|          4500|       New York|     Manager|    1001|    A|\n",
      "|     2|    Bob|   Finance|         7|          5500|    Los Angeles|     Analyst|    1002|    B|\n",
      "|     3|Charlie|        IT|         4|          7000|  San Francisco|   Developer|    1003|    A|\n",
      "|     4|  David|        HR|         3|          4800|        Seattle|   Recruiter|    1004|    C|\n",
      "|     5|    Eva| Marketing|         6|          6000|         Austin|   Executive|    1005|    B|\n",
      "|     6|  Frank|   Finance|         5|          6200|        Chicago|  Consultant|    1006|    A|\n",
      "|     7|  Grace|        IT|         8|          7200|        Houston|   Architect|    1007|    A|\n",
      "|     8|  Henry|        HR|         5|          4600|          Miami|     Manager|    1008|    B|\n",
      "|     9|    Ivy| Marketing|         4|          6100|         Dallas|   Executive|    1009|    C|\n",
      "|    10|   Jack|   Finance|         7|          5800|        Phoenix|     Analyst|    1010|    A|\n",
      "+------+-------+----------+----------+--------------+---------------+------------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res2 = spark.sql(\"\"\" \n",
    "                \n",
    "SELECT emp_id, name, department, \n",
    "       experience AS new_salary,\n",
    "       salary AS new_experience,\n",
    "       location AS new_designation,\n",
    "       designation AS new_location,\n",
    "       emp_code, grade\n",
    "FROM employee_table;\n",
    "\n",
    "\n",
    "              \n",
    "                \"\"\")\n",
    "res2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+------+-----------+-------------+----------+--------+-----+----------+--------------+---------------+------------+\n",
      "|emp_id|name   |department|salary|designation|location     |experience|emp_code|grade|new_salary|new_experience|new_designation|new_location|\n",
      "+------+-------+----------+------+-----------+-------------+----------+--------+-----+----------+--------------+---------------+------------+\n",
      "|1     |Alice  |HR        |4500  |Manager    |New York     |5         |1001    |A    |5         |4500          |New York       |Manager     |\n",
      "|2     |Bob    |Finance   |5500  |Analyst    |Los Angeles  |7         |1002    |B    |7         |5500          |Los Angeles    |Analyst     |\n",
      "|3     |Charlie|IT        |7000  |Developer  |San Francisco|4         |1003    |A    |4         |7000          |San Francisco  |Developer   |\n",
      "|4     |David  |HR        |4800  |Recruiter  |Seattle      |3         |1004    |C    |3         |4800          |Seattle        |Recruiter   |\n",
      "|5     |Eva    |Marketing |6000  |Executive  |Austin       |6         |1005    |B    |6         |6000          |Austin         |Executive   |\n",
      "|6     |Frank  |Finance   |6200  |Consultant |Chicago      |5         |1006    |A    |5         |6200          |Chicago        |Consultant  |\n",
      "|7     |Grace  |IT        |7200  |Architect  |Houston      |8         |1007    |A    |8         |7200          |Houston        |Architect   |\n",
      "|8     |Henry  |HR        |4600  |Manager    |Miami        |5         |1008    |B    |5         |4600          |Miami          |Manager     |\n",
      "|9     |Ivy    |Marketing |6100  |Executive  |Dallas       |4         |1009    |C    |4         |6100          |Dallas         |Executive   |\n",
      "|10    |Jack   |Finance   |5800  |Analyst    |Phoenix      |7         |1010    |A    |7         |5800          |Phoenix        |Analyst     |\n",
      "+------+-------+----------+------+-----------+-------------+----------+--------+-----+----------+--------------+---------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Swap multiple column pairs\n",
    "df_multi_swap = df.withColumn(\"new_salary\", col(\"experience\")) \\\n",
    "                  .withColumn(\"new_experience\", col(\"salary\")) \\\n",
    "                  .withColumn(\"new_designation\", col(\"location\")) \\\n",
    "                  .withColumn(\"new_location\", col(\"designation\"))\n",
    "\n",
    "# Show the result\n",
    "df_multi_swap.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------+--------------+-----------+-------------+--------+-----+\n",
      "|emp_id|name   |department|new_salary|new_experience|designation|location     |emp_code|grade|\n",
      "+------+-------+----------+----------+--------------+-----------+-------------+--------+-----+\n",
      "|1     |Alice  |HR        |5         |4500          |Manager    |New York     |1001    |A    |\n",
      "|2     |Bob    |Finance   |7         |5500          |Analyst    |Los Angeles  |1002    |B    |\n",
      "|3     |Charlie|IT        |4         |7000          |Developer  |San Francisco|1003    |A    |\n",
      "|4     |David  |HR        |3         |4800          |Recruiter  |Seattle      |1004    |C    |\n",
      "|5     |Eva    |Marketing |6         |6000          |Executive  |Austin       |1005    |B    |\n",
      "|6     |Frank  |Finance   |5         |6200          |Consultant |Chicago      |1006    |A    |\n",
      "|7     |Grace  |IT        |8         |7200          |Architect  |Houston      |1007    |A    |\n",
      "|8     |Henry  |HR        |5         |4600          |Manager    |Miami        |1008    |B    |\n",
      "|9     |Ivy    |Marketing |4         |6100          |Executive  |Dallas       |1009    |C    |\n",
      "|10    |Jack   |Finance   |7         |5800          |Analyst    |Phoenix      |1010    |A    |\n",
      "+------+-------+----------+----------+--------------+-----------+-------------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a UDF or use a temporary DataFrame for complex swaps\n",
    "def swap_values(row):\n",
    "    new_salary = row.experience\n",
    "    new_experience = row.salary\n",
    "    return (row.emp_id, row.name, row.department, new_salary, new_experience, row.designation, row.location, row.emp_code, row.grade)\n",
    "\n",
    "# Apply swap logic using RDD transformation\n",
    "rdd_swapped = df.rdd.map(lambda row: swap_values(row))\n",
    "df_swapped_final = spark.createDataFrame(rdd_swapped, schema=[\"emp_id\", \"name\", \"department\", \"new_salary\", \"new_experience\", \"designation\", \"location\", \"emp_code\", \"grade\"])\n",
    "\n",
    "# Show the result\n",
    "df_swapped_final.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
