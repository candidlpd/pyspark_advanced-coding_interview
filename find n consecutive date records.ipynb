{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find n consecutive date records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H:\\\\pyspark_advanced-coding_interview'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"H:\\pyspark_advanced-coding_interview\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------+-----------+\n",
      "|SaleID|ProductID|SaleDate  |SalesAmount|\n",
      "+------+---------+----------+-----------+\n",
      "|1     |101      |2024-10-01|200        |\n",
      "|2     |101      |2024-10-02|150        |\n",
      "|3     |101      |2024-10-03|180        |\n",
      "|4     |101      |2024-10-05|220        |\n",
      "|5     |101      |2024-10-06|210        |\n",
      "|6     |101      |2024-10-07|190        |\n",
      "|7     |102      |2024-10-02|300        |\n",
      "|8     |102      |2024-10-03|250        |\n",
      "|9     |102      |2024-10-04|350        |\n",
      "|10    |102      |2024-10-06|400        |\n",
      "|11    |102      |2024-10-08|450        |\n",
      "|12    |103      |2024-10-01|100        |\n",
      "|13    |103      |2024-10-02|120        |\n",
      "|14    |103      |2024-10-03|140        |\n",
      "|15    |103      |2024-10-07|160        |\n",
      "+------+---------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ConsecutiveSalesDays\").getOrCreate()\n",
    "\n",
    "# Define schema using StructType and StructField\n",
    "schema = StructType([\n",
    "    StructField(\"SaleID\", IntegerType(), True),\n",
    "    StructField(\"ProductID\", IntegerType(), True),\n",
    "    StructField(\"SaleDate\", DateType(), True),\n",
    "    StructField(\"SalesAmount\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Sample sales data\n",
    "data = [\n",
    "    (1, 101, datetime(2024, 10, 1), 200),\n",
    "    (2, 101, datetime(2024, 10, 2), 150),\n",
    "    (3, 101, datetime(2024, 10, 3), 180),\n",
    "    (4, 101, datetime(2024, 10, 5), 220),\n",
    "    (5, 101, datetime(2024, 10, 6), 210),\n",
    "    (6, 101, datetime(2024, 10, 7), 190),\n",
    "    (7, 102, datetime(2024, 10, 2), 300),\n",
    "    (8, 102, datetime(2024, 10, 3), 250),\n",
    "    (9, 102, datetime(2024, 10, 4), 350),\n",
    "    (10, 102, datetime(2024, 10, 6), 400),\n",
    "    (11, 102, datetime(2024, 10, 8), 450),\n",
    "    (12, 103, datetime(2024, 10, 1), 100),\n",
    "    (13, 103, datetime(2024, 10, 2), 120),\n",
    "    (14, 103, datetime(2024, 10, 3), 140),\n",
    "    (15, 103, datetime(2024, 10, 7), 160),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a temporary table\n",
    "df.createOrReplaceTempView(\"Sales\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+\n",
      "|ProductID|  SaleDate|SalesAmount|\n",
      "+---------+----------+-----------+\n",
      "|      101|2024-10-02|        150|\n",
      "|      101|2024-10-06|        210|\n",
      "|      102|2024-10-03|        250|\n",
      "|      103|2024-10-02|        120|\n",
      "+---------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = spark.sql(\"\"\"    \n",
    "  WITH ConsecutiveSales AS (\n",
    "    SELECT *,\n",
    "           DATEDIFF(SaleDate, LAG(SaleDate, 1) OVER (PARTITION BY ProductID ORDER BY SaleDate)) AS PrevDiff,\n",
    "           DATEDIFF(LEAD(SaleDate, 1) OVER (PARTITION BY ProductID ORDER BY SaleDate), SaleDate) AS NextDiff\n",
    "    FROM Sales\n",
    ")\n",
    "SELECT ProductID, SaleDate, SalesAmount\n",
    "FROM ConsecutiveSales\n",
    "WHERE PrevDiff = 1 AND NextDiff = 1\n",
    "ORDER BY ProductID, SaleDate;                \n",
    "                  \"\"\")\n",
    "\n",
    "query.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+---------------+\n",
      "|ProductID|StartDate |EndDate   |ConsecutiveDays|\n",
      "+---------+----------+----------+---------------+\n",
      "|101      |2024-10-01|2024-10-03|3              |\n",
      "|101      |2024-10-05|2024-10-07|3              |\n",
      "|102      |2024-10-02|2024-10-04|3              |\n",
      "|103      |2024-10-01|2024-10-03|3              |\n",
      "+---------+----------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute the corrected query\n",
    "query2 = spark.sql(\"\"\" \n",
    "WITH NumberedSales AS (\n",
    "    SELECT ProductID, SaleDate,\n",
    "           ROW_NUMBER() OVER (PARTITION BY ProductID ORDER BY SaleDate) AS RowNum\n",
    "    FROM Sales\n",
    "),\n",
    "DateGroups AS (\n",
    "    SELECT ProductID, SaleDate,\n",
    "           DATE_SUB(SaleDate, RowNum) AS GroupID\n",
    "    FROM NumberedSales\n",
    ")\n",
    "SELECT ProductID, MIN(SaleDate) AS StartDate, MAX(SaleDate) AS EndDate, COUNT(*) AS ConsecutiveDays\n",
    "FROM DateGroups\n",
    "GROUP BY ProductID, GroupID\n",
    "HAVING COUNT(*) >= 3\n",
    "ORDER BY ProductID, StartDate;\n",
    "\"\"\")\n",
    "\n",
    "# Show the results\n",
    "query2.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------+-----------+------------+------------+\n",
      "|SaleID|ProductID|SaleDate  |SalesAmount|PrevDateDiff|NextDateDiff|\n",
      "+------+---------+----------+-----------+------------+------------+\n",
      "|2     |101      |2024-10-02|150        |1           |1           |\n",
      "|5     |101      |2024-10-06|210        |1           |1           |\n",
      "|8     |102      |2024-10-03|250        |1           |1           |\n",
      "|13    |103      |2024-10-02|120        |1           |1           |\n",
      "+------+---------+----------+-----------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, lag, lead, datediff\n",
    "\n",
    "# Define a window specification\n",
    "window_spec = Window.partitionBy(\"ProductID\").orderBy(\"SaleDate\")\n",
    "\n",
    "# Calculate difference with previous and next date\n",
    "df_consecutive = df.withColumn(\"PrevDateDiff\", datediff(col(\"SaleDate\"), lag(\"SaleDate\", 1).over(window_spec))) \\\n",
    "                   .withColumn(\"NextDateDiff\", datediff(lead(\"SaleDate\", 1).over(window_spec), col(\"SaleDate\")))\n",
    "\n",
    "# Filter to find records where both previous and next dates are consecutive\n",
    "df_filtered = df_consecutive.filter((col(\"PrevDateDiff\") == 1) & (col(\"NextDateDiff\") == 1))\n",
    "df_filtered.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------------+----------+\n",
      "|ProductID|GroupID   |ConsecutiveDays|EndDate   |\n",
      "+---------+----------+---------------+----------+\n",
      "|101      |2024-09-30|3              |2024-10-03|\n",
      "|101      |2024-10-01|3              |2024-10-07|\n",
      "|102      |2024-10-01|3              |2024-10-04|\n",
      "|103      |2024-09-30|3              |2024-10-03|\n",
      "+---------+----------+---------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import row_number, date_sub\n",
    "\n",
    "# Create a window with row number\n",
    "window_spec = Window.partitionBy(\"ProductID\").orderBy(\"SaleDate\")\n",
    "\n",
    "# Apply row number and create a group ID\n",
    "df_grouped = df.withColumn(\"RowNum\", row_number().over(window_spec)) \\\n",
    "               .withColumn(\"GroupID\", date_sub(col(\"SaleDate\"), col(\"RowNum\")))\n",
    "\n",
    "# Group by ProductID and GroupID to find consecutive sequences\n",
    "df_consecutive_grouped = df_grouped.groupBy(\"ProductID\", \"GroupID\") \\\n",
    "                                   .agg({\"SaleDate\": \"min\", \"SaleDate\": \"max\", \"SaleID\": \"count\"}) \\\n",
    "                                   .withColumnRenamed(\"min(SaleDate)\", \"StartDate\") \\\n",
    "                                   .withColumnRenamed(\"max(SaleDate)\", \"EndDate\") \\\n",
    "                                   .withColumnRenamed(\"count(SaleID)\", \"ConsecutiveDays\") \\\n",
    "                                   .filter(col(\"ConsecutiveDays\") >= 3)\n",
    "\n",
    "df_consecutive_grouped.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+----------+---------------+\n",
      "|ProductID|DateOffset|StartDate |EndDate   |ConsecutiveDays|\n",
      "+---------+----------+----------+----------+---------------+\n",
      "|101      |2024-09-30|2024-10-01|2024-10-03|3              |\n",
      "|101      |2024-10-01|2024-10-05|2024-10-07|3              |\n",
      "|102      |2024-10-01|2024-10-02|2024-10-04|3              |\n",
      "|103      |2024-09-30|2024-10-01|2024-10-03|3              |\n",
      "+---------+----------+----------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import row_number, datediff, min, max, count, expr\n",
    "\n",
    "# Define a window specification\n",
    "window_spec = Window.partitionBy(\"ProductID\").orderBy(\"SaleDate\")\n",
    "\n",
    "# Assign RowNum for each SaleDate and create a consistent offset\n",
    "df_advanced = df.withColumn(\"RowNum\", row_number().over(window_spec)) \\\n",
    "                .withColumn(\"DateOffset\", expr(\"date_sub(SaleDate, RowNum)\"))\n",
    "\n",
    "# Group by ProductID and DateOffset to identify consecutive dates\n",
    "df_grouped_advanced = df_advanced.groupBy(\"ProductID\", \"DateOffset\") \\\n",
    "                                 .agg(min(\"SaleDate\").alias(\"StartDate\"), \n",
    "                                      max(\"SaleDate\").alias(\"EndDate\"), \n",
    "                                      count(\"SaleID\").alias(\"ConsecutiveDays\")) \\\n",
    "                                 .filter(col(\"ConsecutiveDays\") >= 3) \\\n",
    "                                 .orderBy(\"ProductID\", \"StartDate\")\n",
    "\n",
    "df_grouped_advanced.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
