{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H:\\\\pyspark_advanced-coding_interview'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"H:\\pyspark_advanced-coding_interview\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|id |name   |\n",
      "+---+-------+\n",
      "|1  |Alice  |\n",
      "|2  |Bob    |\n",
      "|3  |Charlie|\n",
      "|4  |David  |\n",
      "|5  |Eva    |\n",
      "|6  |Frank  |\n",
      "|7  |Grace  |\n",
      "|8  |Henry  |\n",
      "|9  |Ivy    |\n",
      "|10 |Jack   |\n",
      "|11 |Kim    |\n",
      "|12 |Liam   |\n",
      "|13 |Mia    |\n",
      "|14 |Noah   |\n",
      "|15 |Olivia |\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"RandomPasswordAndUniqueID\").getOrCreate()\n",
    "\n",
    "# Sample Data: Basic user information (for demonstration)\n",
    "data = [\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (3, \"Charlie\"),\n",
    "    (4, \"David\"),\n",
    "    (5, \"Eva\"),\n",
    "    (6, \"Frank\"),\n",
    "    (7, \"Grace\"),\n",
    "    (8, \"Henry\"),\n",
    "    (9, \"Ivy\"),\n",
    "    (10, \"Jack\"),\n",
    "    (11, \"Kim\"),\n",
    "    (12, \"Liam\"),\n",
    "    (13, \"Mia\"),\n",
    "    (14, \"Noah\"),\n",
    "    (15, \"Olivia\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\"])\n",
    "\n",
    "# Create a Temporary View for Spark SQL\n",
    "df.createOrReplaceTempView(\"users\")\n",
    "\n",
    "# Show the original DataFrame\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Random UUIDs Using uuid() in Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+\n",
      "| id|   name|           unique_id|\n",
      "+---+-------+--------------------+\n",
      "|  1|  Alice|0b4ab8db-d43a-4fc...|\n",
      "|  2|    Bob|2bf478df-9199-4c8...|\n",
      "|  3|Charlie|1063c92f-7f53-450...|\n",
      "|  4|  David|cfd370a3-6bc3-439...|\n",
      "|  5|    Eva|47d1a18f-93a9-42b...|\n",
      "|  6|  Frank|b094114e-8811-4c2...|\n",
      "|  7|  Grace|ba948855-8614-4c7...|\n",
      "|  8|  Henry|8bac6584-bc80-40e...|\n",
      "|  9|    Ivy|b56c2e60-48b0-4c1...|\n",
      "| 10|   Jack|1d64fee6-d3e3-4f6...|\n",
      "| 11|    Kim|84aecd53-12cf-440...|\n",
      "| 12|   Liam|c2e73ffd-3d1b-4ac...|\n",
      "| 13|    Mia|34586c79-96ec-481...|\n",
      "| 14|   Noah|9f0c3b5c-cbc3-424...|\n",
      "| 15| Olivia|9cf04ec6-cf3e-4b4...|\n",
      "+---+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = spark.sql(\"\"\" \n",
    "                \n",
    "SELECT id, name, uuid() AS unique_id\n",
    "FROM users;\n",
    "\n",
    "              \n",
    "                \"\"\")\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------------------------------------+\n",
      "|id |name   |unique_id                           |\n",
      "+---+-------+------------------------------------+\n",
      "|1  |Alice  |d057e16f-0022-4573-8f57-1ccbe6b48d2e|\n",
      "|2  |Bob    |ce25ea13-5938-4ca7-b693-af55e90e8121|\n",
      "|3  |Charlie|61030dcc-90fc-42ad-9433-d5ea4c6fd137|\n",
      "|4  |David  |eca7d25f-d2bd-4a3e-a02d-aa2bdf7dff2e|\n",
      "|5  |Eva    |69569e0d-b852-40d1-8092-f9bf118d5020|\n",
      "|6  |Frank  |9e0bdc37-2a0c-4623-99d1-cb1442144519|\n",
      "|7  |Grace  |d6efeeff-e206-4696-ba0e-fc8f04a38d2e|\n",
      "|8  |Henry  |73fa0135-0b4d-4a86-b40e-0f45f95d1844|\n",
      "|9  |Ivy    |41118319-2f4e-480b-ba73-54cb198b9890|\n",
      "|10 |Jack   |019bf641-941c-436d-a89a-e362040d3020|\n",
      "|11 |Kim    |e0a7df25-8505-4801-aeba-52c61cb1bfa7|\n",
      "|12 |Liam   |ab501d98-e43b-441d-a0f8-8f6d758edaba|\n",
      "|13 |Mia    |132f5c9c-3195-4b49-a022-2f8903c9961a|\n",
      "|14 |Noah   |5b288067-3f0e-4727-9a2e-bd2591334f56|\n",
      "|15 |Olivia |489dfbfa-55b5-4abb-a41e-7cfcbcdf6477|\n",
      "+---+-------+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Generate unique UUIDs similar to NEWID\n",
    "df_with_uuid = df.withColumn(\"unique_id\", expr(\"uuid()\"))\n",
    "\n",
    "# Show the result\n",
    "df_with_uuid.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Random Passwords Using md5 and Random Functions in Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+\n",
      "| id|   name|     random_password|\n",
      "+---+-------+--------------------+\n",
      "|  1|  Alice|7ad62bc8451d62610...|\n",
      "|  2|    Bob|2c9d371dc92406e89...|\n",
      "|  3|Charlie|1b7681ed07dbc9e3c...|\n",
      "|  4|  David|85d0483ce38544531...|\n",
      "|  5|    Eva|32c15bbdb1eddad82...|\n",
      "|  6|  Frank|80498ebdb0849a99d...|\n",
      "|  7|  Grace|8e0dce56aff993c61...|\n",
      "|  8|  Henry|9cfee74e78fc8c376...|\n",
      "|  9|    Ivy|bd28034ffe187bf63...|\n",
      "| 10|   Jack|54858d13744508532...|\n",
      "| 11|    Kim|37372ea52728eef9c...|\n",
      "| 12|   Liam|ce2b8b141285ffc69...|\n",
      "| 13|    Mia|b508acc8f995b354a...|\n",
      "| 14|   Noah|0244440c8d13f2b07...|\n",
      "| 15| Olivia|cebfeeffe69648d14...|\n",
      "+---+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res1 = spark.sql(\"\"\" \n",
    "                \n",
    "SELECT id, name, \n",
    "       md5(cast(rand() * 1000000 AS STRING)) AS random_password\n",
    "FROM users;\n",
    "\n",
    "              \n",
    "                \"\"\")\n",
    "res1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------------------+\n",
      "|id |name   |random_password                 |\n",
      "+---+-------+--------------------------------+\n",
      "|1  |Alice  |2b9606197cde44c390b87f334182cfac|\n",
      "|2  |Bob    |b865d23d9b63f71b5137444c81d50e93|\n",
      "|3  |Charlie|14d7a4cd5cd5b2ee64e506b4f26bfe6f|\n",
      "|4  |David  |cc4afaf8f6aa126174cceddf713fee0c|\n",
      "|5  |Eva    |655719d574b9f3c42bc59899d73f6363|\n",
      "|6  |Frank  |7cae20e9de52a958e96392cd1cf56f49|\n",
      "|7  |Grace  |24c43050ee10b453bf9694b3ee30df7c|\n",
      "|8  |Henry  |814375e7db63c64f2a781cb53a54bfbf|\n",
      "|9  |Ivy    |554c16feefc0aa14fc96fd20c43a8920|\n",
      "|10 |Jack   |c0835e2171fd8f9749236216ee6dccfe|\n",
      "|11 |Kim    |c97b11802f525ee3747e6fe9a9bef766|\n",
      "|12 |Liam   |a65ab0d4e8d5c9c3588c59fead1c1518|\n",
      "|13 |Mia    |e3972868106f31211a6c2f492ce2b1f6|\n",
      "|14 |Noah   |7daefe2baff894873c7118b6da2f5aeb|\n",
      "|15 |Olivia |bfa1fc82d7a75ddd2dcf8b32feea76a0|\n",
      "+---+-------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import md5, rand, concat\n",
    "\n",
    "# Generate random passwords using MD5 hash\n",
    "df_random_passwords = df.withColumn(\"random_password\", md5(concat(rand().cast(\"string\"))))\n",
    "\n",
    "# Show the result\n",
    "df_random_passwords.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------------+\n",
      "|id |name   |random_password|\n",
      "+---+-------+---------------+\n",
      "|1  |Alice  |^'xzC+}U       |\n",
      "|2  |Bob    |5jR1sIQH       |\n",
      "|3  |Charlie|1i\"U{4w0       |\n",
      "|4  |David  |ic~=/*`V       |\n",
      "|5  |Eva    |}\\!M/<j{       |\n",
      "|6  |Frank  |$}]G+Slz       |\n",
      "|7  |Grace  |~F0UT<cq       |\n",
      "|8  |Henry  ||VV$iv%@       |\n",
      "|9  |Ivy    |Y943=7gc       |\n",
      "|10 |Jack   |h<05k=W\"       |\n",
      "|11 |Kim    |EJ9g3hS8       |\n",
      "|12 |Liam   |DrvR>E,_       |\n",
      "|13 |Mia    |J.K<M:UC       |\n",
      "|14 |Noah   |w%dP/Azi       |\n",
      "|15 |Olivia |\"9+3Z$jv       |\n",
      "+---+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define a function to generate random passwords\n",
    "def generate_random_password(length=8):\n",
    "    characters = string.ascii_letters + string.digits + string.punctuation\n",
    "    return ''.join(random.choice(characters) for i in range(length))\n",
    "\n",
    "# Register UDF\n",
    "random_password_udf = udf(generate_random_password, StringType())\n",
    "\n",
    "# Apply UDF to generate random passwords\n",
    "df_with_random_passwords = df.withColumn(\"random_password\", random_password_udf())\n",
    "\n",
    "df_with_random_passwords.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------------------------------------------+\n",
      "|id |name   |unique_id                                    |\n",
      "+---+-------+---------------------------------------------+\n",
      "|1  |Alice  |8589934592-55ac8efc193e16693cbffcf3eb1d538b  |\n",
      "|2  |Bob    |17179869184-42fb221b6a68ef9d742e973f13c652eb |\n",
      "|3  |Charlie|25769803776-9e72920ee9d84f376b9bf8a71449b174 |\n",
      "|4  |David  |42949672960-4191258cce09394513a59a8a16c2f6a4 |\n",
      "|5  |Eva    |51539607552-cffb998dff8d8cb8eb5bef0b7ca39275 |\n",
      "|6  |Frank  |60129542144-ccd4a528a0d9c8f23a04643af3047239 |\n",
      "|7  |Grace  |77309411328-8bc5a009d5f0eac44e42291394f090b9 |\n",
      "|8  |Henry  |85899345920-68e470eefaf2729b28de5ec1997848c4 |\n",
      "|9  |Ivy    |94489280512-65c19b0a55ce2b79aceca9caa6dd9ead |\n",
      "|10 |Jack   |111669149696-70fb57edaed5e1e56438f6795169cef2|\n",
      "|11 |Kim    |120259084288-0d5d330b36c1c4925baa56130cf3e1a6|\n",
      "|12 |Liam   |128849018880-0d3b48a57caf182c5fcc70dbe46f0c24|\n",
      "|13 |Mia    |146028888064-a98dba90ceca03d5f9d97f094f2f7903|\n",
      "|14 |Noah   |154618822656-aa73ef1e342c20d4a6e07a516b6f2a65|\n",
      "|15 |Olivia |163208757248-0476009a4ce1b69dd5d37ec040fe7d90|\n",
      "+---+-------+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, lit\n",
    "\n",
    "# Generate unique IDs using monotonically increasing values and combine with a random number\n",
    "df_unique_id = df.withColumn(\"unique_id\", concat(monotonically_increasing_id(), lit(\"-\"), md5(rand().cast(\"string\"))))\n",
    "\n",
    "df_unique_id.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
