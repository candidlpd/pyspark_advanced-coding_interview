{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find all levels of Employee Manager Hierarchy | Recursion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H:\\\\pyspark_advanced-coding_interview'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"H:\\pyspark_advanced-coding_interview\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---------+\n",
      "|EmployeeID|EmployeeName|ManagerID|\n",
      "+----------+------------+---------+\n",
      "|1         |Alice       |null     |\n",
      "|2         |Bob         |1        |\n",
      "|3         |Charlie     |1        |\n",
      "|4         |David       |2        |\n",
      "|5         |Eve         |2        |\n",
      "|6         |Frank       |3        |\n",
      "|7         |Grace       |3        |\n",
      "|8         |Heidi       |4        |\n",
      "|9         |Ivan        |5        |\n",
      "|10        |Judy        |6        |\n",
      "+----------+------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 64092)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\lpdda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"c:\\Users\\lpdda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"c:\\Users\\lpdda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"c:\\Users\\lpdda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"C:\\spark\\python\\pyspark\\accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"C:\\spark\\python\\pyspark\\accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "       ^^^^^^\n",
      "  File \"C:\\spark\\python\\pyspark\\accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\spark\\python\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\lpdda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Initialize Spark Session (if not already done)\n",
    "spark = SparkSession.builder.appName(\"EmployeeHierarchy\").getOrCreate()\n",
    "\n",
    "# Define schema using StructType and StructField\n",
    "schema = StructType([\n",
    "    StructField(\"EmployeeID\", IntegerType(), True),\n",
    "    StructField(\"EmployeeName\", StringType(), True),\n",
    "    StructField(\"ManagerID\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Sample data representing employees and their managers\n",
    "data = [\n",
    "    (1, \"Alice\", None),    # Alice is the top-level manager\n",
    "    (2, \"Bob\", 1),         # Bob reports to Alice\n",
    "    (3, \"Charlie\", 1),     # Charlie reports to Alice\n",
    "    (4, \"David\", 2),       # David reports to Bob\n",
    "    (5, \"Eve\", 2),         # Eve reports to Bob\n",
    "    (6, \"Frank\", 3),       # Frank reports to Charlie\n",
    "    (7, \"Grace\", 3),       # Grace reports to Charlie\n",
    "    (8, \"Heidi\", 4),       # Heidi reports to David\n",
    "    (9, \"Ivan\", 5),        # Ivan reports to Eve\n",
    "    (10, \"Judy\", 6)        # Judy reports to Frank\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a temporary table\n",
    "df.createOrReplaceTempView(\"Employee\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---------+-----+\n",
      "|EmployeeID|EmployeeName|ManagerID|Level|\n",
      "+----------+------------+---------+-----+\n",
      "|         1|       Alice|     null|    1|\n",
      "+----------+------------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query1 = spark.sql(\"\"\"                    \n",
    " WITH Level1 AS (\n",
    "    SELECT EmployeeID, EmployeeName, ManagerID, 1 AS Level\n",
    "    FROM Employee\n",
    "    WHERE ManagerID IS NULL\n",
    ")\n",
    "SELECT * FROM Level1;                  \n",
    "                   \n",
    "                   \"\"\")\n",
    "\n",
    "query1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---------+-----+\n",
      "|EmployeeID|EmployeeName|ManagerID|Level|\n",
      "+----------+------------+---------+-----+\n",
      "|         1|       Alice|     null|    1|\n",
      "|         2|         Bob|        1|    2|\n",
      "|         3|     Charlie|        1|    2|\n",
      "|         4|       David|        2|    3|\n",
      "|         5|         Eve|        2|    3|\n",
      "|         6|       Frank|        3|    3|\n",
      "|         7|       Grace|        3|    3|\n",
      "+----------+------------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = spark.sql(\"\"\"\n",
    " WITH Level1 AS (\n",
    "    SELECT EmployeeID, EmployeeName, ManagerID, 1 AS Level\n",
    "    FROM Employee\n",
    "    WHERE ManagerID IS NULL\n",
    "),\n",
    "Level2 AS (\n",
    "    SELECT e.EmployeeID, e.EmployeeName, e.ManagerID, l1.Level + 1 AS Level\n",
    "    FROM Employee e\n",
    "    INNER JOIN Level1 l1 ON e.ManagerID = l1.EmployeeID\n",
    "),\n",
    "Level3 AS (\n",
    "    SELECT e.EmployeeID, e.EmployeeName, e.ManagerID, l2.Level + 1 AS Level\n",
    "    FROM Employee e\n",
    "    INNER JOIN Level2 l2 ON e.ManagerID = l2.EmployeeID\n",
    ")\n",
    "SELECT * FROM Level1\n",
    "UNION ALL\n",
    "SELECT * FROM Level2\n",
    "UNION ALL\n",
    "SELECT * FROM Level3;\n",
    "\n",
    "                  \n",
    "                  \"\"\")\n",
    "\n",
    "query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---------+\n",
      "|EmployeeID|EmployeeName|ManagerID|\n",
      "+----------+------------+---------+\n",
      "|         1|       Alice|     null|\n",
      "|         2|         Bob|        1|\n",
      "|         3|     Charlie|        1|\n",
      "|         4|       David|        2|\n",
      "|         5|         Eve|        2|\n",
      "|         6|       Frank|        3|\n",
      "|         7|       Grace|        3|\n",
      "|         8|       Heidi|        4|\n",
      "|         9|        Ivan|        5|\n",
      "|        10|        Judy|        6|\n",
      "+----------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---------+-----+\n",
      "|EmployeeID|EmployeeName|ManagerID|Level|\n",
      "+----------+------------+---------+-----+\n",
      "|1         |Alice       |null     |1    |\n",
      "|2         |Bob         |1        |2    |\n",
      "|3         |Charlie     |1        |2    |\n",
      "|4         |David       |2        |3    |\n",
      "|5         |Eve         |2        |3    |\n",
      "|6         |Frank       |3        |3    |\n",
      "|7         |Grace       |3        |3    |\n",
      "|8         |Heidi       |4        |4    |\n",
      "|9         |Ivan        |5        |4    |\n",
      "|10        |Judy        |6        |4    |\n",
      "+----------+------------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Step 1: Initialize the top-level managers (Level 1)\n",
    "current_level = df.filter(col(\"ManagerID\").isNull()).withColumn(\"Level\", expr(\"1\"))\n",
    "hierarchy = current_level\n",
    "\n",
    "# Step 2: Iterate to expand the hierarchy\n",
    "for level in range(2, 10):  # Assume a maximum of 10 levels\n",
    "    next_level = df.alias(\"e\").join(\n",
    "        current_level.alias(\"c\"),\n",
    "        col(\"e.ManagerID\") == col(\"c.EmployeeID\")\n",
    "    ).select(\n",
    "        col(\"e.EmployeeID\"),\n",
    "        col(\"e.EmployeeName\"),\n",
    "        col(\"e.ManagerID\"),\n",
    "        (col(\"c.Level\") + 1).alias(\"Level\")  # Correctly set the new column\n",
    "    )\n",
    "    \n",
    "    # If no more records found, break the loop\n",
    "    if next_level.count() == 0:\n",
    "        break\n",
    "    \n",
    "    # Add to hierarchy and update the current level\n",
    "    hierarchy = hierarchy.union(next_level)\n",
    "    current_level = next_level\n",
    "\n",
    "# Step 3: Show the complete hierarchy\n",
    "hierarchy.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
