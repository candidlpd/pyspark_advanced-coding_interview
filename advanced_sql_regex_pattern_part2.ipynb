{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H:\\\\pyspark_advanced-coding_interview'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"H:\\pyspark_advanced-coding_interview\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session with optimized settings\n",
    "spark = (\n",
    "    SparkSession.builder \n",
    "    .appName(\"OptimizedLocalSpark\") \n",
    "    .config(\"spark.driver.memory\", \"8g\")        \n",
    "    .config(\"spark.executor.memory\", \"8g\")    \n",
    "    .config(\"spark.executor.cores\", \"4\")       \n",
    "    .config(\"spark.cores.max\", \"12\")           \n",
    "    .config(\"spark.sql.shuffle.partitions\", \"28\")  \n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------+---------------+---------+-------------------+-------------------+----------------+---------------------+-----------------+-------------------------------------+-------------------+------------------------------------+--------------------------+--------------+------------------------------------------+-----------------------------+----------------+----------------+-------------------+-------------------+---------+\n",
      "|id |post                                  |phone          |prefix   |between_chars      |punctuated         |suffix_check    |text_with_ip         |mac_address      |text_with_digits                     |email              |uuid                                |file_path                 |non_ascii     |url_with_params                           |html_content                 |camel_case      |phone_area      |credit_card        |vowel_words        |hex_color|\n",
      "+---+--------------------------------------+---------------+---------+-------------------+-------------------+----------------+---------------------+-----------------+-------------------------------------+-------------------+------------------------------------+--------------------------+--------------+------------------------------------------+-----------------------------+----------------+----------------+-------------------+-------------------+---------+\n",
      "|1  |Loving the new #iPhone13! #Apple #Tech|+1-800-555-1234|USER001  |Hello [world]!     |Let's eat, Grandma!|This is the end.|IP found: 192.168.1.1|00:1A:2B:3C:4D:5E|My credit card is 1234 5678 9012 3456|alice@example.com  |550e8400-e29b-41d4-a716-446655440000|/path/to/file/document.txt|Café au lait  |https://www.example.com?user=alice&lang=en|<p>Hello <b>world</b>!</p>   |userEmail       |+1-123-456-7890 |4111 1111 1111 1111|apple orange banana|#FF5733  |\n",
      "|2  |Just released #PS5! It's amazing!     |(800)555-9876  |ADMIN100 |Extract [this part]|Why so serious?!   |Let's move on.  |Connect to 10.0.0.1  |01:23:45:67:89:ab|Use pin: 0987                        |bob.smith@yahoo.com|6ba7b810-9dad-11d1-80b4-00c04fd430c8|/files/uploads/image.png  |naïve approach|https://example.com?page=1&limit=10       |<div>Sample <i>text</i></div>|lowerCamelCase  |+44 20 7946 0958|4000 1234 5678 9010|umbrella island    |#aB12CD  |\n",
      "|3  |#Travel #Adventure is life!           |800.555.4321   |DEVELOPER|Between [brackets] |Wait! What?!!      |No suffix here  |Found: 172.16.254.1  |AB-CD-EF-12-34-56|My pass: 5678                        |contact@domain.com |9f8d0db1-d8b0-4b53-991e-99300dbdb5c1|/user/photos/pic.jpg      |façade        |https://shop.com/products?id=1001         |<h1>Welcome</h1>             |camelCaseExample|555-222-3333    |5105 1051 0510 0510|orange apple       |#BADA55  |\n",
      "+---+--------------------------------------+---------------+---------+-------------------+-------------------+----------------+---------------------+-----------------+-------------------------------------+-------------------+------------------------------------+--------------------------+--------------+------------------------------------------+-----------------------------+----------------+----------------+-------------------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"AdvancedRegexRealTimeExamples\").getOrCreate()\n",
    "\n",
    "# Sample Data: Various strings for complex regex processing\n",
    "data = [\n",
    "    (1, \"Loving the new #iPhone13! #Apple #Tech\", \"+1-800-555-1234\", \"USER001\", \"Hello [world]!\", \"Let's eat, Grandma!\", \"This is the end.\", \"IP found: 192.168.1.1\", \"00:1A:2B:3C:4D:5E\", \"My credit card is 1234 5678 9012 3456\", \"alice@example.com\", \"550e8400-e29b-41d4-a716-446655440000\", \"/path/to/file/document.txt\", \"Café au lait\", \"https://www.example.com?user=alice&lang=en\", \"<p>Hello <b>world</b>!</p>\", \"userEmail\", \"+1-123-456-7890\", \"4111 1111 1111 1111\", \"apple orange banana\", \"#FF5733\"),\n",
    "    (2, \"Just released #PS5! It's amazing!\", \"(800)555-9876\", \"ADMIN100\", \"Extract [this part]\", \"Why so serious?!\", \"Let's move on.\", \"Connect to 10.0.0.1\", \"01:23:45:67:89:ab\", \"Use pin: 0987\", \"bob.smith@yahoo.com\", \"6ba7b810-9dad-11d1-80b4-00c04fd430c8\", \"/files/uploads/image.png\", \"naïve approach\", \"https://example.com?page=1&limit=10\", \"<div>Sample <i>text</i></div>\", \"lowerCamelCase\", \"+44 20 7946 0958\", \"4000 1234 5678 9010\", \"umbrella island\", \"#aB12CD\"),\n",
    "    (3, \"#Travel #Adventure is life!\", \"800.555.4321\", \"DEVELOPER\", \"Between [brackets]\", \"Wait! What?!!\", \"No suffix here\", \"Found: 172.16.254.1\", \"AB-CD-EF-12-34-56\", \"My pass: 5678\", \"contact@domain.com\", \"9f8d0db1-d8b0-4b53-991e-99300dbdb5c1\", \"/user/photos/pic.jpg\", \"façade\", \"https://shop.com/products?id=1001\", \"<h1>Welcome</h1>\", \"camelCaseExample\", \"555-222-3333\", \"5105 1051 0510 0510\", \"orange apple\", \"#BADA55\"),\n",
    "    # More records can be added similarly\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"id\", \"post\", \"phone\", \"prefix\", \"between_chars\", \"punctuated\", \"suffix_check\", \"text_with_ip\", \"mac_address\", \"text_with_digits\", \"email\", \"uuid\", \"file_path\", \"non_ascii\", \"url_with_params\", \"html_content\", \"camel_case\", \"phone_area\", \"credit_card\", \"vowel_words\", \"hex_color\"])\n",
    "\n",
    "# Create a Temporary View for Spark SQL\n",
    "df.createOrReplaceTempView(\"regex_advanced_examples\")\n",
    "df.cache()\n",
    "# Show the Original DataFrame\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Hashtags from a Social Media Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|                post|            hashtags|\n",
      "+---+--------------------+--------------------+\n",
      "|  1|Loving the new #i...|[#iPhone13, #Appl...|\n",
      "|  2|Just released #PS...|              [#PS5]|\n",
      "|  3|#Travel #Adventur...|[#Travel, #Advent...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = spark.sql (\"\"\" \n",
    "  \n",
    "SELECT id, post, regexp_extract_all(post, '#[A-Za-z0-9]+', 0) AS hashtags FROM regex_advanced_examples;\n",
    "              \n",
    "                   \n",
    "                   \"\"\")\n",
    "\n",
    "res.show()\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "df.withColumn(\"hashtags\", regexp_extract(\"post\", \"#[A-Za-z0-9]+\", 0)).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate Phone Numbers in Various Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------+---------------+---------+-------------------+-------------------+----------------+---------------------+-----------------+-------------------------------------+-------------------+------------------------------------+--------------------------+--------------+------------------------------------------+-----------------------------+----------------+----------------+-------------------+-------------------+---------+------------+\n",
      "|id |post                                  |phone          |prefix   |between_chars      |punctuated         |suffix_check    |text_with_ip         |mac_address      |text_with_digits                     |email              |uuid                                |file_path                 |non_ascii     |url_with_params                           |html_content                 |camel_case      |phone_area      |credit_card        |vowel_words        |hex_color|phone_status|\n",
      "+---+--------------------------------------+---------------+---------+-------------------+-------------------+----------------+---------------------+-----------------+-------------------------------------+-------------------+------------------------------------+--------------------------+--------------+------------------------------------------+-----------------------------+----------------+----------------+-------------------+-------------------+---------+------------+\n",
      "|1  |Loving the new #iPhone13! #Apple #Tech|+1-800-555-1234|USER001  |Hello [world]!     |Let's eat, Grandma!|This is the end.|IP found: 192.168.1.1|00:1A:2B:3C:4D:5E|My credit card is 1234 5678 9012 3456|alice@example.com  |550e8400-e29b-41d4-a716-446655440000|/path/to/file/document.txt|Café au lait  |https://www.example.com?user=alice&lang=en|<p>Hello <b>world</b>!</p>   |userEmail       |+1-123-456-7890 |4111 1111 1111 1111|apple orange banana|#FF5733  |false       |\n",
      "|2  |Just released #PS5! It's amazing!     |(800)555-9876  |ADMIN100 |Extract [this part]|Why so serious?!   |Let's move on.  |Connect to 10.0.0.1  |01:23:45:67:89:ab|Use pin: 0987                        |bob.smith@yahoo.com|6ba7b810-9dad-11d1-80b4-00c04fd430c8|/files/uploads/image.png  |naïve approach|https://example.com?page=1&limit=10       |<div>Sample <i>text</i></div>|lowerCamelCase  |+44 20 7946 0958|4000 1234 5678 9010|umbrella island    |#aB12CD  |false       |\n",
      "|3  |#Travel #Adventure is life!           |800.555.4321   |DEVELOPER|Between [brackets] |Wait! What?!!      |No suffix here  |Found: 172.16.254.1  |AB-CD-EF-12-34-56|My pass: 5678                        |contact@domain.com |9f8d0db1-d8b0-4b53-991e-99300dbdb5c1|/user/photos/pic.jpg      |façade        |https://shop.com/products?id=1001         |<h1>Welcome</h1>             |camelCaseExample|555-222-3333    |5105 1051 0510 0510|orange apple       |#BADA55  |true        |\n",
      "+---+--------------------------------------+---------------+---------+-------------------+-------------------+----------------+---------------------+-----------------+-------------------------------------+-------------------+------------------------------------+--------------------------+--------------+------------------------------------------+-----------------------------+----------------+----------------+-------------------+-------------------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"phone_status\", df[\"phone\"].rlike(\"^(\\\\+?\\\\d{1,3}[- .]?\\\\(\\\\d{3}\\\\)[- .]?|\\\\d{3}[- .])\\\\d{3}[- .]?\\\\d{4}$\")).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o47.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 40) (localhost executor driver): java.util.regex.PatternSyntaxException: Dangling meta character '+' near index 2\r\n^(+?d{1,3}[- .]?(d{3})[- .]?|d{3}[- .])d{3}[- .]?d{4}$\r\n  ^\r\n\tat java.base/java.util.regex.Pattern.error(Pattern.java:2015)\r\n\tat java.base/java.util.regex.Pattern.sequence(Pattern.java:2190)\r\n\tat java.base/java.util.regex.Pattern.expr(Pattern.java:2056)\r\n\tat java.base/java.util.regex.Pattern.group0(Pattern.java:3045)\r\n\tat java.base/java.util.regex.Pattern.sequence(Pattern.java:2111)\r\n\tat java.base/java.util.regex.Pattern.expr(Pattern.java:2056)\r\n\tat java.base/java.util.regex.Pattern.compile(Pattern.java:1778)\r\n\tat java.base/java.util.regex.Pattern.<init>(Pattern.java:1427)\r\n\tat java.base/java.util.regex.Pattern.compile(Pattern.java:1068)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.init(Unknown Source)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.$anonfun$doExecute$4(WholeStageCodegenExec.scala:757)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.$anonfun$doExecute$4$adapted(WholeStageCodegenExec.scala:754)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:908)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:908)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2790)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2726)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2725)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2725)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1211)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1211)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1211)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2989)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2928)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2917)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:976)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4218)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3202)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4208)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4206)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4206)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3202)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3423)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:283)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:322)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.util.regex.PatternSyntaxException: Dangling meta character '+' near index 2\r\n^(+?d{1,3}[- .]?(d{3})[- .]?|d{3}[- .])d{3}[- .]?d{4}$\r\n  ^\r\n\tat java.base/java.util.regex.Pattern.error(Pattern.java:2015)\r\n\tat java.base/java.util.regex.Pattern.sequence(Pattern.java:2190)\r\n\tat java.base/java.util.regex.Pattern.expr(Pattern.java:2056)\r\n\tat java.base/java.util.regex.Pattern.group0(Pattern.java:3045)\r\n\tat java.base/java.util.regex.Pattern.sequence(Pattern.java:2111)\r\n\tat java.base/java.util.regex.Pattern.expr(Pattern.java:2056)\r\n\tat java.base/java.util.regex.Pattern.compile(Pattern.java:1778)\r\n\tat java.base/java.util.regex.Pattern.<init>(Pattern.java:1427)\r\n\tat java.base/java.util.regex.Pattern.compile(Pattern.java:1068)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.init(Unknown Source)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.$anonfun$doExecute$4(WholeStageCodegenExec.scala:757)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.$anonfun$doExecute$4$adapted(WholeStageCodegenExec.scala:754)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:908)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:908)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m res1 \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql (\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m \u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124m  \u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124mSELECT id, phone, CASE WHEN phone RLIKE \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^(\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m+?\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m1,3}[- .]?\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124md\u001b[39m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m)[- .]?|\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124md\u001b[39m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;124m[- .])\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124md\u001b[39m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;124m[- .]?\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124md\u001b[39m\u001b[38;5;132;01m{4}\u001b[39;00m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m THEN \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValid\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m ELSE \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m END AS phone_status FROM regex_advanced_examples;\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124m  \u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m                   \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mres1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\dataframe.py:901\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    896\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    897\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    898\u001b[0m     )\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\lpdda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\lpdda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o47.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 40) (localhost executor driver): java.util.regex.PatternSyntaxException: Dangling meta character '+' near index 2\r\n^(+?d{1,3}[- .]?(d{3})[- .]?|d{3}[- .])d{3}[- .]?d{4}$\r\n  ^\r\n\tat java.base/java.util.regex.Pattern.error(Pattern.java:2015)\r\n\tat java.base/java.util.regex.Pattern.sequence(Pattern.java:2190)\r\n\tat java.base/java.util.regex.Pattern.expr(Pattern.java:2056)\r\n\tat java.base/java.util.regex.Pattern.group0(Pattern.java:3045)\r\n\tat java.base/java.util.regex.Pattern.sequence(Pattern.java:2111)\r\n\tat java.base/java.util.regex.Pattern.expr(Pattern.java:2056)\r\n\tat java.base/java.util.regex.Pattern.compile(Pattern.java:1778)\r\n\tat java.base/java.util.regex.Pattern.<init>(Pattern.java:1427)\r\n\tat java.base/java.util.regex.Pattern.compile(Pattern.java:1068)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.init(Unknown Source)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.$anonfun$doExecute$4(WholeStageCodegenExec.scala:757)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.$anonfun$doExecute$4$adapted(WholeStageCodegenExec.scala:754)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:908)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:908)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2790)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2726)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2725)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2725)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1211)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1211)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1211)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2989)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2928)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2917)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:976)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4218)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3202)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4208)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4206)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4206)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3202)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3423)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:283)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:322)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.util.regex.PatternSyntaxException: Dangling meta character '+' near index 2\r\n^(+?d{1,3}[- .]?(d{3})[- .]?|d{3}[- .])d{3}[- .]?d{4}$\r\n  ^\r\n\tat java.base/java.util.regex.Pattern.error(Pattern.java:2015)\r\n\tat java.base/java.util.regex.Pattern.sequence(Pattern.java:2190)\r\n\tat java.base/java.util.regex.Pattern.expr(Pattern.java:2056)\r\n\tat java.base/java.util.regex.Pattern.group0(Pattern.java:3045)\r\n\tat java.base/java.util.regex.Pattern.sequence(Pattern.java:2111)\r\n\tat java.base/java.util.regex.Pattern.expr(Pattern.java:2056)\r\n\tat java.base/java.util.regex.Pattern.compile(Pattern.java:1778)\r\n\tat java.base/java.util.regex.Pattern.<init>(Pattern.java:1427)\r\n\tat java.base/java.util.regex.Pattern.compile(Pattern.java:1068)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.init(Unknown Source)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.$anonfun$doExecute$4(WholeStageCodegenExec.scala:757)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.$anonfun$doExecute$4$adapted(WholeStageCodegenExec.scala:754)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:908)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:908)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "res1 = spark.sql (\"\"\" \n",
    "  \n",
    "SELECT id, phone, CASE WHEN phone RLIKE '^(\\\\+?\\\\d{1,3}[- .]?\\\\(\\\\d{3}\\\\)[- .]?|\\\\d{3}[- .])\\\\d{3}[- .]?\\\\d{4}$' THEN 'Valid' ELSE 'Invalid' END AS phone_status FROM regex_advanced_examples;\n",
    "  \n",
    "                   \"\"\")\n",
    "\n",
    "res1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if a String Starts with a Specific Prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------------+\n",
      "| id|   prefix|    prefix_check|\n",
      "+---+---------+----------------+\n",
      "|  1|  USER001|Starts with USER|\n",
      "|  2| ADMIN100|Different prefix|\n",
      "|  3|DEVELOPER|Different prefix|\n",
      "+---+---------+----------------+\n",
      "\n",
      "+---+--------------------------------------+---------------+---------+-------------------+-------------------+----------------+---------------------+-----------------+-------------------------------------+-------------------+------------------------------------+--------------------------+--------------+------------------------------------------+-----------------------------+----------------+----------------+-------------------+-------------------+---------+------------+\n",
      "|id |post                                  |phone          |prefix   |between_chars      |punctuated         |suffix_check    |text_with_ip         |mac_address      |text_with_digits                     |email              |uuid                                |file_path                 |non_ascii     |url_with_params                           |html_content                 |camel_case      |phone_area      |credit_card        |vowel_words        |hex_color|prefix_check|\n",
      "+---+--------------------------------------+---------------+---------+-------------------+-------------------+----------------+---------------------+-----------------+-------------------------------------+-------------------+------------------------------------+--------------------------+--------------+------------------------------------------+-----------------------------+----------------+----------------+-------------------+-------------------+---------+------------+\n",
      "|1  |Loving the new #iPhone13! #Apple #Tech|+1-800-555-1234|USER001  |Hello [world]!     |Let's eat, Grandma!|This is the end.|IP found: 192.168.1.1|00:1A:2B:3C:4D:5E|My credit card is 1234 5678 9012 3456|alice@example.com  |550e8400-e29b-41d4-a716-446655440000|/path/to/file/document.txt|Café au lait  |https://www.example.com?user=alice&lang=en|<p>Hello <b>world</b>!</p>   |userEmail       |+1-123-456-7890 |4111 1111 1111 1111|apple orange banana|#FF5733  |true        |\n",
      "|2  |Just released #PS5! It's amazing!     |(800)555-9876  |ADMIN100 |Extract [this part]|Why so serious?!   |Let's move on.  |Connect to 10.0.0.1  |01:23:45:67:89:ab|Use pin: 0987                        |bob.smith@yahoo.com|6ba7b810-9dad-11d1-80b4-00c04fd430c8|/files/uploads/image.png  |naïve approach|https://example.com?page=1&limit=10       |<div>Sample <i>text</i></div>|lowerCamelCase  |+44 20 7946 0958|4000 1234 5678 9010|umbrella island    |#aB12CD  |false       |\n",
      "|3  |#Travel #Adventure is life!           |800.555.4321   |DEVELOPER|Between [brackets] |Wait! What?!!      |No suffix here  |Found: 172.16.254.1  |AB-CD-EF-12-34-56|My pass: 5678                        |contact@domain.com |9f8d0db1-d8b0-4b53-991e-99300dbdb5c1|/user/photos/pic.jpg      |façade        |https://shop.com/products?id=1001         |<h1>Welcome</h1>             |camelCaseExample|555-222-3333    |5105 1051 0510 0510|orange apple       |#BADA55  |false       |\n",
      "+---+--------------------------------------+---------------+---------+-------------------+-------------------+----------------+---------------------+-----------------+-------------------------------------+-------------------+------------------------------------+--------------------------+--------------+------------------------------------------+-----------------------------+----------------+----------------+-------------------+-------------------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res2 = spark.sql (\"\"\" \n",
    "  \n",
    "SELECT id, prefix, CASE WHEN prefix RLIKE '^USER' THEN 'Starts with USER' ELSE 'Different prefix' END AS prefix_check FROM regex_advanced_examples;\n",
    "\n",
    "  \n",
    "                   \"\"\")\n",
    "\n",
    "res2.show()\n",
    "\n",
    "\n",
    "df.withColumn(\"prefix_check\", df[\"prefix\"].rlike(\"^USER\")).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Everything Between Two Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+--------------+\n",
      "| id|      between_chars|extracted_part|\n",
      "+---+-------------------+--------------+\n",
      "|  1|     Hello [world]!|              |\n",
      "|  2|Extract [this part]|              |\n",
      "|  3| Between [brackets]|              |\n",
      "+---+-------------------+--------------+\n",
      "\n",
      "+---+--------------------------------------+---------------+---------+-------------------+-------------------+----------------+---------------------+-----------------+-------------------------------------+-------------------+------------------------------------+--------------------------+--------------+------------------------------------------+-----------------------------+----------------+----------------+-------------------+-------------------+---------+--------------+\n",
      "|id |post                                  |phone          |prefix   |between_chars      |punctuated         |suffix_check    |text_with_ip         |mac_address      |text_with_digits                     |email              |uuid                                |file_path                 |non_ascii     |url_with_params                           |html_content                 |camel_case      |phone_area      |credit_card        |vowel_words        |hex_color|extracted_part|\n",
      "+---+--------------------------------------+---------------+---------+-------------------+-------------------+----------------+---------------------+-----------------+-------------------------------------+-------------------+------------------------------------+--------------------------+--------------+------------------------------------------+-----------------------------+----------------+----------------+-------------------+-------------------+---------+--------------+\n",
      "|1  |Loving the new #iPhone13! #Apple #Tech|+1-800-555-1234|USER001  |Hello [world]!     |Let's eat, Grandma!|This is the end.|IP found: 192.168.1.1|00:1A:2B:3C:4D:5E|My credit card is 1234 5678 9012 3456|alice@example.com  |550e8400-e29b-41d4-a716-446655440000|/path/to/file/document.txt|Café au lait  |https://www.example.com?user=alice&lang=en|<p>Hello <b>world</b>!</p>   |userEmail       |+1-123-456-7890 |4111 1111 1111 1111|apple orange banana|#FF5733  |world         |\n",
      "|2  |Just released #PS5! It's amazing!     |(800)555-9876  |ADMIN100 |Extract [this part]|Why so serious?!   |Let's move on.  |Connect to 10.0.0.1  |01:23:45:67:89:ab|Use pin: 0987                        |bob.smith@yahoo.com|6ba7b810-9dad-11d1-80b4-00c04fd430c8|/files/uploads/image.png  |naïve approach|https://example.com?page=1&limit=10       |<div>Sample <i>text</i></div>|lowerCamelCase  |+44 20 7946 0958|4000 1234 5678 9010|umbrella island    |#aB12CD  |this part     |\n",
      "|3  |#Travel #Adventure is life!           |800.555.4321   |DEVELOPER|Between [brackets] |Wait! What?!!      |No suffix here  |Found: 172.16.254.1  |AB-CD-EF-12-34-56|My pass: 5678                        |contact@domain.com |9f8d0db1-d8b0-4b53-991e-99300dbdb5c1|/user/photos/pic.jpg      |façade        |https://shop.com/products?id=1001         |<h1>Welcome</h1>             |camelCaseExample|555-222-3333    |5105 1051 0510 0510|orange apple       |#BADA55  |brackets      |\n",
      "+---+--------------------------------------+---------------+---------+-------------------+-------------------+----------------+---------------------+-----------------+-------------------------------------+-------------------+------------------------------------+--------------------------+--------------+------------------------------------------+-----------------------------+----------------+----------------+-------------------+-------------------+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "\n",
    "res3 = spark.sql (\"\"\" \n",
    "  \n",
    "SELECT id, between_chars, regexp_extract(between_chars, '\\\\[(.*?)\\\\]', 1) AS extracted_part FROM regex_advanced_examples;\n",
    "\n",
    "\n",
    "  \n",
    "                   \"\"\")\n",
    "\n",
    "res3.show()\n",
    "\n",
    "df.withColumn(\"extracted_part\", regexp_extract(\"between_chars\", \"\\\\[(.*?)\\\\]\", 1)).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Punctuation from a String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+----------------+\n",
      "| id|         punctuated|  no_punctuation|\n",
      "+---+-------------------+----------------+\n",
      "|  1|Let's eat, Grandma!|Le's ea, Gradma!|\n",
      "|  2|   Why so serious?!| Why so serios?!|\n",
      "|  3|      Wait! What?!!|     Wai! Wha?!!|\n",
      "+---+-------------------+----------------+\n",
      "\n",
      "+-------------------+----------------+\n",
      "|punctuated         |no_punctuation  |\n",
      "+-------------------+----------------+\n",
      "|Let's eat, Grandma!|Lets eat Grandma|\n",
      "|Why so serious?!   |Why so serious  |\n",
      "|Wait! What?!!      |Wait What       |\n",
      "+-------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import regexp_extract, regexp_replace\n",
    "\n",
    "\n",
    "res5 = spark.sql (\"\"\" \n",
    "  \n",
    "SELECT id, punctuated, regexp_replace(punctuated, '[\\\\p{Punct}]', '') AS no_punctuation FROM regex_advanced_examples;\n",
    "\n",
    "\n",
    "  \n",
    "                   \"\"\")\n",
    "\n",
    "res5.show()\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "df.select(\"punctuated\").withColumn(\"no_punctuation\", regexp_replace(\"punctuated\", \"[\\\\p{Punct}]\", \"\")).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if a String Ends with a Specific Suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----------------+\n",
      "| id|    suffix_check|suffix_validation|\n",
      "+---+----------------+-----------------+\n",
      "|  1|This is the end.|   Ends with end.|\n",
      "|  2|  Let's move on.| Different suffix|\n",
      "|  3|  No suffix here| Different suffix|\n",
      "+---+----------------+-----------------+\n",
      "\n",
      "+----------------+-----------------+\n",
      "|suffix_check    |suffix_validation|\n",
      "+----------------+-----------------+\n",
      "|This is the end.|true             |\n",
      "|Let's move on.  |false            |\n",
      "|No suffix here  |false            |\n",
      "+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res6 = spark.sql (\"\"\" \n",
    "  \n",
    "SELECT id, suffix_check, \n",
    "CASE WHEN suffix_check RLIKE 'end\\\\.$' THEN 'Ends with end.' ELSE 'Different suffix' END AS suffix_validation \n",
    "FROM regex_advanced_examples;\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "                   \"\"\")\n",
    "\n",
    "res6.show()\n",
    "\n",
    "\n",
    "df.select(\"suffix_check\").withColumn(\"suffix_validation\", df[\"suffix_check\"].rlike(\"end\\\\.$\")).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract IPs from a Text Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------+\n",
      "| id|        text_with_ip|extracted_ip|\n",
      "+---+--------------------+------------+\n",
      "|  1|IP found: 192.168...|            |\n",
      "|  2| Connect to 10.0.0.1|            |\n",
      "|  3| Found: 172.16.254.1|            |\n",
      "+---+--------------------+------------+\n",
      "\n",
      "+---------------------+------------+\n",
      "|text_with_ip         |extracted_ip|\n",
      "+---------------------+------------+\n",
      "|IP found: 192.168.1.1|192.168.1.1 |\n",
      "|Connect to 10.0.0.1  |10.0.0.1    |\n",
      "|Found: 172.16.254.1  |172.16.254.1|\n",
      "+---------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res7 = spark.sql (\"\"\" \n",
    "  \n",
    "SELECT id, text_with_ip, regexp_extract(text_with_ip, '\\\\b\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\b', 0) AS extracted_ip \n",
    "\n",
    "FROM regex_advanced_examples;\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "                   \"\"\")\n",
    "\n",
    "res7.show()\n",
    "\n",
    "\n",
    "df.select(\"text_with_ip\").withColumn(\"extracted_ip\", regexp_extract(\"text_with_ip\", \"\\\\b\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\b\", 0)).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate MAC Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------+\n",
      "| id|      mac_address|mac_status|\n",
      "+---+-----------------+----------+\n",
      "|  1|00:1A:2B:3C:4D:5E|     Valid|\n",
      "|  2|01:23:45:67:89:ab|     Valid|\n",
      "|  3|AB-CD-EF-12-34-56|     Valid|\n",
      "+---+-----------------+----------+\n",
      "\n",
      "+-----------------+----------+\n",
      "|mac_address      |mac_status|\n",
      "+-----------------+----------+\n",
      "|00:1A:2B:3C:4D:5E|true      |\n",
      "|01:23:45:67:89:ab|true      |\n",
      "|AB-CD-EF-12-34-56|true      |\n",
      "+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res8 = spark.sql (\"\"\" \n",
    "  \n",
    "SELECT id, mac_address, CASE WHEN mac_address RLIKE '^([0-9A-Fa-f]{2}[:-]){5}([0-9A-Fa-f]{2})$' THEN 'Valid' ELSE 'Invalid' END AS mac_status FROM regex_advanced_examples;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "                   \"\"\")\n",
    "\n",
    "res8.show()\n",
    "\n",
    "\n",
    "df.select(\"mac_address\").withColumn(\"mac_status\", df[\"mac_address\"].rlike(\"^([0-9A-Fa-f]{2}[:-]){5}([0-9A-Fa-f]{2})$\")).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Replace All Digits with Asterisks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|    text_with_digits|     obfuscated_text|\n",
      "+---+--------------------+--------------------+\n",
      "|  1|My credit card is...|My cre*it car* is...|\n",
      "|  2|       Use pin: 0987|       Use pin: 0987|\n",
      "|  3|       My pass: 5678|       My pass: 5678|\n",
      "+---+--------------------+--------------------+\n",
      "\n",
      "+-------------------------------------+-------------------------------------+\n",
      "|text_with_digits                     |obfuscated_text                      |\n",
      "+-------------------------------------+-------------------------------------+\n",
      "|My credit card is 1234 5678 9012 3456|My credit card is **** **** **** ****|\n",
      "|Use pin: 0987                        |Use pin: ****                        |\n",
      "|My pass: 5678                        |My pass: ****                        |\n",
      "+-------------------------------------+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res9 = spark.sql (\"\"\" \n",
    "  \n",
    "SELECT id, text_with_digits, regexp_replace(text_with_digits, '\\\\d', '*') AS obfuscated_text FROM regex_advanced_examples;\n",
    "\n",
    "                   \"\"\")\n",
    "\n",
    "res9.show()\n",
    "\n",
    "\n",
    "df.select(\"text_with_digits\").withColumn(\"obfuscated_text\", regexp_replace(\"text_with_digits\", \"\\\\d\", \"*\")).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the Username from an Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------------+---------+-------------------+-------------------+----------------+--------------------+-----------------+--------------------+-------------------+--------------------+--------------------+--------------+--------------------+--------------------+----------------+----------------+-------------------+-------------------+---------+\n",
      "| id|                post|          phone|   prefix|      between_chars|         punctuated|    suffix_check|        text_with_ip|      mac_address|    text_with_digits|              email|                uuid|           file_path|     non_ascii|     url_with_params|        html_content|      camel_case|      phone_area|        credit_card|        vowel_words|hex_color|\n",
      "+---+--------------------+---------------+---------+-------------------+-------------------+----------------+--------------------+-----------------+--------------------+-------------------+--------------------+--------------------+--------------+--------------------+--------------------+----------------+----------------+-------------------+-------------------+---------+\n",
      "|  1|Loving the new #i...|+1-800-555-1234|  USER001|     Hello [world]!|Let's eat, Grandma!|This is the end.|IP found: 192.168...|00:1A:2B:3C:4D:5E|My credit card is...|  alice@example.com|550e8400-e29b-41d...|/path/to/file/doc...|  Café au lait|https://www.examp...|<p>Hello <b>world...|       userEmail| +1-123-456-7890|4111 1111 1111 1111|apple orange banana|  #FF5733|\n",
      "|  2|Just released #PS...|  (800)555-9876| ADMIN100|Extract [this part]|   Why so serious?!|  Let's move on.| Connect to 10.0.0.1|01:23:45:67:89:ab|       Use pin: 0987|bob.smith@yahoo.com|6ba7b810-9dad-11d...|/files/uploads/im...|naïve approach|https://example.c...|<div>Sample <i>te...|  lowerCamelCase|+44 20 7946 0958|4000 1234 5678 9010|    umbrella island|  #aB12CD|\n",
      "|  3|#Travel #Adventur...|   800.555.4321|DEVELOPER| Between [brackets]|      Wait! What?!!|  No suffix here| Found: 172.16.254.1|AB-CD-EF-12-34-56|       My pass: 5678| contact@domain.com|9f8d0db1-d8b0-4b5...|/user/photos/pic.jpg|        façade|https://shop.com/...|    <h1>Welcome</h1>|camelCaseExample|    555-222-3333|5105 1051 0510 0510|       orange apple|  #BADA55|\n",
      "+---+--------------------+---------------+---------+-------------------+-------------------+----------------+--------------------+-----------------+--------------------+-------------------+--------------------+--------------------+--------------+--------------------+--------------------+----------------+----------------+-------------------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from regex_advanced_examples\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+---------+\n",
      "| id|              email| username|\n",
      "+---+-------------------+---------+\n",
      "|  1|  alice@example.com|    alice|\n",
      "|  2|bob.smith@yahoo.com|bob.smith|\n",
      "|  3| contact@domain.com|  contact|\n",
      "+---+-------------------+---------+\n",
      "\n",
      "+-------------------+---------+\n",
      "|email              |username |\n",
      "+-------------------+---------+\n",
      "|alice@example.com  |alice    |\n",
      "|bob.smith@yahoo.com|bob.smith|\n",
      "|contact@domain.com |contact  |\n",
      "+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res10 = spark.sql (\"\"\" \n",
    "  \n",
    "SELECT id, email, regexp_extract(email, '^[A-Za-z0-9._%+-]+', 0) AS username FROM regex_advanced_examples;\n",
    "\n",
    "                   \"\"\")\n",
    "\n",
    "res10.show()\n",
    "\n",
    "df.select(\"email\").withColumn(\"username\", regexp_extract(\"email\", \"^[A-Za-z0-9._%+-]+\", 0)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if String Matches a UUID Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+\n",
      "| id|                uuid|uuid_check|\n",
      "+---+--------------------+----------+\n",
      "|  1|550e8400-e29b-41d...|     Valid|\n",
      "|  2|6ba7b810-9dad-11d...|     Valid|\n",
      "|  3|9f8d0db1-d8b0-4b5...|     Valid|\n",
      "+---+--------------------+----------+\n",
      "\n",
      "+------------------------------------+----------+\n",
      "|uuid                                |uuid_check|\n",
      "+------------------------------------+----------+\n",
      "|550e8400-e29b-41d4-a716-446655440000|true      |\n",
      "|6ba7b810-9dad-11d1-80b4-00c04fd430c8|true      |\n",
      "|9f8d0db1-d8b0-4b53-991e-99300dbdb5c1|true      |\n",
      "+------------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res11 = spark.sql (\"\"\" \n",
    "  \n",
    "SELECT id, uuid, CASE WHEN uuid RLIKE '^[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}$' THEN 'Valid' ELSE 'Invalid' END AS uuid_check FROM regex_advanced_examples;\n",
    "\n",
    "\n",
    "                   \"\"\")\n",
    "\n",
    "res11.show()\n",
    "\n",
    "df.select(\"uuid\").withColumn(\"uuid_check\", df[\"uuid\"].rlike(\"^[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}$\")).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the Filename from a File Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------+\n",
      "| id|           file_path|    filename|\n",
      "+---+--------------------+------------+\n",
      "|  1|/path/to/file/doc...|document.txt|\n",
      "|  2|/files/uploads/im...|   image.png|\n",
      "|  3|/user/photos/pic.jpg|     pic.jpg|\n",
      "+---+--------------------+------------+\n",
      "\n",
      "+--------------------------+------------+\n",
      "|file_path                 |filename    |\n",
      "+--------------------------+------------+\n",
      "|/path/to/file/document.txt|document.txt|\n",
      "|/files/uploads/image.png  |image.png   |\n",
      "|/user/photos/pic.jpg      |pic.jpg     |\n",
      "+--------------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res12 = spark.sql (\"\"\" \n",
    "  \n",
    "SELECT id, file_path, regexp_extract(file_path, '[^/]+$', 0) AS filename FROM regex_advanced_examples;\n",
    "\n",
    "                   \"\"\")\n",
    "\n",
    "res12.show()\n",
    "\n",
    "df.select(\"file_path\").withColumn(\"filename\", regexp_extract(\"file_path\", \"[^/]+$\", 0)).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace Non-ASCII Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------+\n",
      "| id|     non_ascii|  ascii_only|\n",
      "+---+--------------+------------+\n",
      "|  1|  Café au lait|   Cafaulait|\n",
      "|  2|naïve approach|naveapproach|\n",
      "|  3|        façade|       faade|\n",
      "+---+--------------+------------+\n",
      "\n",
      "+--------------+-------------+\n",
      "|non_ascii     |ascii_only   |\n",
      "+--------------+-------------+\n",
      "|Café au lait  |Caf au lait  |\n",
      "|naïve approach|nave approach|\n",
      "|façade        |faade        |\n",
      "+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res13 = spark.sql (\"\"\" \n",
    "  \n",
    "SELECT id, non_ascii, regexp_replace(non_ascii, '[^\\\\x00-\\\\x7F]', '') AS ascii_only FROM regex_advanced_examples;\n",
    "\n",
    "                   \"\"\")\n",
    "\n",
    "res13.show()\n",
    "\n",
    "df.select(\"non_ascii\").withColumn(\"ascii_only\", regexp_replace(\"non_ascii\", \"[^\\\\x00-\\\\x7F]\", \"\")).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract URL Parameters from a Query String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------------+---------+-------------------+-------------------+----------------+--------------------+-----------------+--------------------+-------------------+--------------------+--------------------+--------------+--------------------+--------------------+----------------+----------------+-------------------+-------------------+---------+\n",
      "| id|                post|          phone|   prefix|      between_chars|         punctuated|    suffix_check|        text_with_ip|      mac_address|    text_with_digits|              email|                uuid|           file_path|     non_ascii|     url_with_params|        html_content|      camel_case|      phone_area|        credit_card|        vowel_words|hex_color|\n",
      "+---+--------------------+---------------+---------+-------------------+-------------------+----------------+--------------------+-----------------+--------------------+-------------------+--------------------+--------------------+--------------+--------------------+--------------------+----------------+----------------+-------------------+-------------------+---------+\n",
      "|  1|Loving the new #i...|+1-800-555-1234|  USER001|     Hello [world]!|Let's eat, Grandma!|This is the end.|IP found: 192.168...|00:1A:2B:3C:4D:5E|My credit card is...|  alice@example.com|550e8400-e29b-41d...|/path/to/file/doc...|  Café au lait|https://www.examp...|<p>Hello <b>world...|       userEmail| +1-123-456-7890|4111 1111 1111 1111|apple orange banana|  #FF5733|\n",
      "|  2|Just released #PS...|  (800)555-9876| ADMIN100|Extract [this part]|   Why so serious?!|  Let's move on.| Connect to 10.0.0.1|01:23:45:67:89:ab|       Use pin: 0987|bob.smith@yahoo.com|6ba7b810-9dad-11d...|/files/uploads/im...|naïve approach|https://example.c...|<div>Sample <i>te...|  lowerCamelCase|+44 20 7946 0958|4000 1234 5678 9010|    umbrella island|  #aB12CD|\n",
      "|  3|#Travel #Adventur...|   800.555.4321|DEVELOPER| Between [brackets]|      Wait! What?!!|  No suffix here| Found: 172.16.254.1|AB-CD-EF-12-34-56|       My pass: 5678| contact@domain.com|9f8d0db1-d8b0-4b5...|/user/photos/pic.jpg|        façade|https://shop.com/...|    <h1>Welcome</h1>|camelCaseExample|    555-222-3333|5105 1051 0510 0510|       orange apple|  #BADA55|\n",
      "+---+--------------------+---------------+---------+-------------------+-------------------+----------------+--------------------+-----------------+--------------------+-------------------+--------------------+--------------------+--------------+--------------------+--------------------+----------------+----------------+-------------------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from regex_advanced_examples\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+-------------------+\n",
      "|url_with_params                           |query_params       |\n",
      "+------------------------------------------+-------------------+\n",
      "|https://www.example.com?user=alice&lang=en|?user=alice&lang=en|\n",
      "|https://example.com?page=1&limit=10       |?page=1&limit=10   |\n",
      "|https://shop.com/products?id=1001         |?id=1001           |\n",
      "+------------------------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# res14 = spark.sql (\"\"\" \n",
    "  \n",
    "# SELECT id, url_with_params, regexp_extract(url_with_params, '\\\\?.*', 0) AS query_params FROM regex_advanced_examples;\n",
    "\n",
    "\n",
    "#                    \"\"\")\n",
    "\n",
    "# res14.show()\n",
    "\n",
    "\n",
    "df.select(\"url_with_params\").withColumn(\"query_params\", regexp_extract(\"url_with_params\", \"\\\\?.*\", 0)).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove All HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------+\n",
      "| id|        html_content|text_content|\n",
      "+---+--------------------+------------+\n",
      "|  1|<p>Hello <b>world...|Hello world!|\n",
      "|  2|<div>Sample <i>te...| Sample text|\n",
      "|  3|    <h1>Welcome</h1>|     Welcome|\n",
      "+---+--------------------+------------+\n",
      "\n",
      "+-----------------------------+------------+\n",
      "|html_content                 |text_content|\n",
      "+-----------------------------+------------+\n",
      "|<p>Hello <b>world</b>!</p>   |Hello world!|\n",
      "|<div>Sample <i>text</i></div>|Sample text |\n",
      "|<h1>Welcome</h1>             |Welcome     |\n",
      "+-----------------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res15 = spark.sql (\"\"\" \n",
    "  \n",
    "SELECT id, html_content, regexp_replace(html_content, '<[^>]+>', '') AS text_content FROM regex_advanced_examples;\n",
    "\n",
    "\n",
    "                   \"\"\")\n",
    "\n",
    "res15.show()\n",
    "\n",
    "df.select(\"html_content\").withColumn(\"text_content\", regexp_replace(\"html_content\", \"<[^>]+>\", \"\")).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert camelCase to snake_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+\n",
      "|camel_case      |snake_case        |\n",
      "+----------------+------------------+\n",
      "|userEmail       |user_Email        |\n",
      "|lowerCamelCase  |lower_Camel_Case  |\n",
      "|camelCaseExample|camel_Case_Example|\n",
      "+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"camel_case\").withColumn(\"snake_case\", regexp_replace(\"camel_case\", \"(?<=[a-z])([A-Z])\", \"_$1\")).show(truncate=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Area Code from a Phone Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+\n",
      "|phone_area      |area_code|\n",
      "+----------------+---------+\n",
      "|+1-123-456-7890 |123      |\n",
      "|+44 20 7946 0958|794      |\n",
      "|555-222-3333    |555      |\n",
      "+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# res16 = spark.sql (\"\"\" \n",
    "# SELECT id, phone_area, regexp_extract(phone_area, '\\\\(?\\\\d{3}\\\\)?', 0) AS area_code FROM regex_advanced_examples;\n",
    "\n",
    "#                    \"\"\")\n",
    "\n",
    "# res16.show()\n",
    "\n",
    "df.select(\"phone_area\").withColumn(\"area_code\", regexp_extract(\"phone_area\", \"\\\\(?\\\\d{3}\\\\)?\", 0)).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate Credit Card Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----------+\n",
      "| id|        credit_card|card_status|\n",
      "+---+-------------------+-----------+\n",
      "|  1|4111 1111 1111 1111|    Invalid|\n",
      "|  2|4000 1234 5678 9010|    Invalid|\n",
      "|  3|5105 1051 0510 0510|    Invalid|\n",
      "+---+-------------------+-----------+\n",
      "\n",
      "+-------------------+-----------+\n",
      "|credit_card        |card_status|\n",
      "+-------------------+-----------+\n",
      "|4111 1111 1111 1111|true       |\n",
      "|4000 1234 5678 9010|true       |\n",
      "|5105 1051 0510 0510|true       |\n",
      "+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res17 = spark.sql (\"\"\" \n",
    "SELECT id, credit_card, CASE WHEN credit_card RLIKE '^\\\\d{4}[- ]?\\\\d{4}[- ]?\\\\d{4}[- ]?\\\\d{4}$' THEN 'Valid' ELSE 'Invalid' END AS card_status FROM regex_advanced_examples;\n",
    "\n",
    "\n",
    "                   \"\"\")\n",
    "\n",
    "res17.show()\n",
    "\n",
    "df.select(\"credit_card\").withColumn(\"card_status\", df[\"credit_card\"].rlike(\"^\\\\d{4}[- ]?\\\\d{4}[- ]?\\\\d{4}[- ]?\\\\d{4}$\")).show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract All Words Starting with a Vowel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----------------+\n",
      "| id|        vowel_words|vowel_start_words|\n",
      "+---+-------------------+-----------------+\n",
      "|  1|apple orange banana|               []|\n",
      "|  2|    umbrella island|               []|\n",
      "|  3|       orange apple|               []|\n",
      "+---+-------------------+-----------------+\n",
      "\n",
      "+-------------------+-----------------+\n",
      "|vowel_words        |vowel_start_words|\n",
      "+-------------------+-----------------+\n",
      "|apple orange banana|apple            |\n",
      "|umbrella island    |umbrella         |\n",
      "|orange apple       |orange           |\n",
      "+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res18 = spark.sql (\"\"\" \n",
    "SELECT id, vowel_words, regexp_extract_all(vowel_words, '\\\\b[AEIOUaeiou][a-zA-Z]*\\\\b', 0) AS vowel_start_words FROM regex_advanced_examples;\n",
    "\n",
    "\n",
    "                   \"\"\")\n",
    "\n",
    "res18.show()\n",
    "\n",
    "df.select(\"vowel_words\").withColumn(\"vowel_start_words\", regexp_extract(\"vowel_words\", \"\\\\b[AEIOUaeiou][a-zA-Z]*\\\\b\", 0)).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
